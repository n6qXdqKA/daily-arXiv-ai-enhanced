{"id": "2504.20052", "pdf": "https://arxiv.org/pdf/2504.20052", "abs": "https://arxiv.org/abs/2504.20052", "authors": ["Floriane Magera", "Thomas Hoyoux", "Martin Castin", "Olivier Barnich", "Anthony Cioppa", "Marc Van Droogenbroeck"], "title": "Can Geometry Save Central Views for Sports Field Registration?", "categories": ["cs.CV"], "comment": "10 pages, 10 figures, 1 table, 40 references", "summary": "Single-frame sports field registration often serves as the foundation for\nextracting 3D information from broadcast videos, enabling applications related\nto sports analytics, refereeing, or fan engagement. As sports fields have\nrigorous specifications in terms of shape and dimensions of their line, circle\nand point components, sports field markings are commonly used as calibration\ntargets for this task. However, because of the sparse and uneven distribution\nof field markings, close-up camera views around central areas of the field\noften depict only line and circle markings. On these views, sports field\nregistration is challenging for the vast majority of existing methods, as they\nfocus on leveraging line field markings and their intersections. It is indeed a\nchallenge to include circle correspondences in a set of linear equations. In\nthis work, we propose a novel method to derive a set of points and lines from\ncircle correspondences, enabling the exploitation of circle correspondences for\nboth sports field registration and image annotation. In our experiments, we\nillustrate the benefits of our bottom-up geometric method against\ntop-performing detectors and show that our method successfully complements\nthem, enabling sports field registration in difficult scenarios.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u5706\u5f62\u6807\u8bb0\u8fdb\u884c\u8fd0\u52a8\u573a\u6ce8\u518c\u7684\u65b0\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u7279\u5199\u955c\u5934\u4e2d\u96be\u4ee5\u5229\u7528\u5706\u5f62\u6807\u8bb0\u7684\u95ee\u9898\u3002", "motivation": "\u8fd0\u52a8\u573a\u6ce8\u518c\u901a\u5e38\u4f9d\u8d56\u7a00\u758f\u4e14\u5206\u5e03\u4e0d\u5747\u7684\u7ebf\u6807\u8bb0\uff0c\u800c\u7279\u5199\u955c\u5934\u4e2d\u5706\u5f62\u6807\u8bb0\u66f4\u5e38\u89c1\uff0c\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u5229\u7528\u5706\u5f62\u6807\u8bb0\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4ece\u5706\u5f62\u5bf9\u5e94\u5173\u7cfb\u4e2d\u63a8\u5bfc\u70b9\u548c\u7ebf\u7684\u65b9\u6cd5\uff0c\u5c06\u5706\u5f62\u6807\u8bb0\u7eb3\u5165\u7ebf\u6027\u65b9\u7a0b\u7ec4\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u8865\u5145\u73b0\u6709\u9ad8\u6027\u80fd\u68c0\u6d4b\u5668\uff0c\u5728\u56f0\u96be\u573a\u666f\u4e2d\u5b9e\u73b0\u8fd0\u52a8\u573a\u6ce8\u518c\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6210\u529f\u5229\u7528\u5706\u5f62\u6807\u8bb0\uff0c\u6269\u5c55\u4e86\u8fd0\u52a8\u573a\u6ce8\u518c\u7684\u5e94\u7528\u8303\u56f4\u3002"}}
{"id": "2504.20054", "pdf": "https://arxiv.org/pdf/2504.20054", "abs": "https://arxiv.org/abs/2504.20054", "authors": ["Jiayang Sun", "Hongbo Wang", "Jie Cao", "Huaibo Huang", "Ran He"], "title": "Marmot: Multi-Agent Reasoning for Multi-Object Self-Correcting in Improving Image-Text Alignment", "categories": ["cs.CV"], "comment": null, "summary": "While diffusion models excel at generating high-quality images, they often\nstruggle with accurate counting, attributes, and spatial relationships in\ncomplex multi-object scenes. To address these challenges, we propose Marmot, a\nnovel and generalizable framework that employs Multi-Agent Reasoning for\nMulti-Object Self-Correcting, enhancing image-text alignment and facilitating\nmore coherent multi-object image editing. Our framework adopts a\ndivide-and-conquer strategy that decomposes the self-correction task into three\ncritical dimensions (counting, attributes, and spatial relationships), and\nfurther divided into object-level subtasks. We construct a multi-agent editing\nsystem featuring a decision-execution-verification mechanism, effectively\nmitigating inter-object interference and enhancing editing reliability. To\nresolve the problem of subtask integration, we propose a Pixel-Domain Stitching\nSmoother that employs mask-guided two-stage latent space optimization. This\ninnovation enables parallel processing of subtask results, thereby enhancing\nruntime efficiency while eliminating multi-stage distortion accumulation.\nExtensive experiments demonstrate that Marmot significantly improves accuracy\nin object counting, attribute assignment, and spatial relationships for image\ngeneration tasks.", "AI": {"tldr": "Marmot\u6846\u67b6\u901a\u8fc7\u591a\u667a\u80fd\u4f53\u63a8\u7406\u89e3\u51b3\u6269\u6563\u6a21\u578b\u5728\u590d\u6742\u591a\u7269\u4f53\u573a\u666f\u4e2d\u7684\u8ba1\u6570\u3001\u5c5e\u6027\u548c\u7a7a\u95f4\u5173\u7cfb\u95ee\u9898\uff0c\u63d0\u5347\u56fe\u50cf\u751f\u6210\u8d28\u91cf\u3002", "motivation": "\u6269\u6563\u6a21\u578b\u5728\u590d\u6742\u591a\u7269\u4f53\u573a\u666f\u4e2d\u96be\u4ee5\u51c6\u786e\u5904\u7406\u8ba1\u6570\u3001\u5c5e\u6027\u548c\u7a7a\u95f4\u5173\u7cfb\uff0c\u9700\u8981\u4e00\u79cd\u901a\u7528\u6846\u67b6\u6765\u589e\u5f3a\u56fe\u50cf-\u6587\u672c\u5bf9\u9f50\u3002", "method": "\u91c7\u7528\u5206\u6cbb\u7b56\u7565\uff0c\u5c06\u81ea\u6821\u6b63\u4efb\u52a1\u5206\u89e3\u4e3a\u8ba1\u6570\u3001\u5c5e\u6027\u548c\u7a7a\u95f4\u5173\u7cfb\u4e09\u4e2a\u7ef4\u5ea6\uff0c\u5e76\u8fdb\u4e00\u6b65\u7ec6\u5316\u4e3a\u5bf9\u8c61\u7ea7\u5b50\u4efb\u52a1\u3002\u6784\u5efa\u591a\u667a\u80fd\u4f53\u7f16\u8f91\u7cfb\u7edf\uff0c\u7ed3\u5408\u51b3\u7b56-\u6267\u884c-\u9a8c\u8bc1\u673a\u5236\u548c\u50cf\u7d20\u57df\u62fc\u63a5\u5e73\u6ed1\u5668\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cMarmot\u663e\u8457\u63d0\u5347\u4e86\u56fe\u50cf\u751f\u6210\u4efb\u52a1\u4e2d\u5bf9\u8c61\u8ba1\u6570\u3001\u5c5e\u6027\u5206\u914d\u548c\u7a7a\u95f4\u5173\u7cfb\u7684\u51c6\u786e\u6027\u3002", "conclusion": "Marmot\u901a\u8fc7\u591a\u667a\u80fd\u4f53\u63a8\u7406\u548c\u4f18\u5316\u6280\u672f\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u6269\u6563\u6a21\u578b\u5728\u590d\u6742\u573a\u666f\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u63d0\u5347\u4e86\u56fe\u50cf\u7f16\u8f91\u7684\u8fde\u8d2f\u6027\u548c\u6548\u7387\u3002"}}
{"id": "2504.20077", "pdf": "https://arxiv.org/pdf/2504.20077", "abs": "https://arxiv.org/abs/2504.20077", "authors": ["Manish Kansana", "Keyan Alexander Rahimi", "Elias Hossain", "Iman Dehzangi", "Noorbakhsh Amiri Golilarz"], "title": "Edge-Based Learning for Improved Classification Under Adversarial Noise", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Adversarial noise introduces small perturbations in images, misleading deep\nlearning models into misclassification and significantly impacting recognition\naccuracy. In this study, we analyzed the effects of Fast Gradient Sign Method\n(FGSM) adversarial noise on image classification and investigated whether\ntraining on specific image features can improve robustness. We hypothesize that\nwhile adversarial noise perturbs various regions of an image, edges may remain\nrelatively stable and provide essential structural information for\nclassification. To test this, we conducted a series of experiments using brain\ntumor and COVID datasets. Initially, we trained the models on clean images and\nthen introduced subtle adversarial perturbations, which caused deep learning\nmodels to significantly misclassify the images. Retraining on a combination of\nclean and noisy images led to improved performance. To evaluate the robustness\nof the edge features, we extracted edges from the original/clean images and\ntrained the models exclusively on edge-based representations. When noise was\nintroduced to the images, the edge-based models demonstrated greater resilience\nto adversarial attacks compared to those trained on the original or clean\nimages. These results suggest that while adversarial noise is able to exploit\ncomplex non-edge regions significantly more than edges, the improvement in the\naccuracy after retraining is marginally more in the original data as compared\nto the edges. Thus, leveraging edge-based learning can improve the resilience\nof deep learning models against adversarial perturbations.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u5bf9\u6297\u6027\u566a\u58f0\u5bf9\u56fe\u50cf\u5206\u7c7b\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u57fa\u4e8e\u8fb9\u7f18\u7279\u5f81\u7684\u8bad\u7ec3\u80fd\u63d0\u5347\u6a21\u578b\u5bf9\u6297\u653b\u51fb\u7684\u9c81\u68d2\u6027\u3002", "motivation": "\u5bf9\u6297\u6027\u566a\u58f0\u4f1a\u8bef\u5bfc\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u901a\u8fc7\u7279\u5b9a\u56fe\u50cf\u7279\u5f81\uff08\u5982\u8fb9\u7f18\uff09\u63d0\u5347\u6a21\u578b\u9c81\u68d2\u6027\u3002", "method": "\u4f7f\u7528FGSM\u751f\u6210\u5bf9\u6297\u566a\u58f0\uff0c\u5206\u522b\u5728\u539f\u59cb\u56fe\u50cf\u548c\u8fb9\u7f18\u56fe\u50cf\u4e0a\u8bad\u7ec3\u6a21\u578b\uff0c\u5e76\u6d4b\u8bd5\u5176\u5bf9\u6297\u653b\u51fb\u7684\u9c81\u68d2\u6027\u3002", "result": "\u8fb9\u7f18\u7279\u5f81\u6a21\u578b\u5bf9\u5bf9\u6297\u653b\u51fb\u66f4\u5177\u9c81\u68d2\u6027\uff0c\u4f46\u539f\u59cb\u6570\u636e\u91cd\u65b0\u8bad\u7ec3\u540e\u7684\u51c6\u786e\u7387\u63d0\u5347\u7565\u9ad8\u4e8e\u8fb9\u7f18\u6570\u636e\u3002", "conclusion": "\u57fa\u4e8e\u8fb9\u7f18\u7684\u5b66\u4e60\u53ef\u63d0\u5347\u6a21\u578b\u5bf9\u6297\u6270\u52a8\u7684\u9c81\u68d2\u6027\uff0c\u4f46\u9700\u7ed3\u5408\u539f\u59cb\u6570\u636e\u8bad\u7ec3\u4ee5\u4f18\u5316\u6027\u80fd\u3002"}}
{"id": "2504.20091", "pdf": "https://arxiv.org/pdf/2504.20091", "abs": "https://arxiv.org/abs/2504.20091", "authors": ["Noriyuki Kugo", "Xiang Li", "Zixin Li", "Ashish Gupta", "Arpandeep Khatua", "Nidhish Jain", "Chaitanya Patel", "Yuta Kyuragi", "Masamoto Tanabiki", "Kazuki Kozuka", "Ehsan Adeli"], "title": "VideoMultiAgents: A Multi-Agent Framework for Video Question Answering", "categories": ["cs.CV", "cs.MA"], "comment": null, "summary": "Video Question Answering (VQA) inherently relies on multimodal reasoning,\nintegrating visual, temporal, and linguistic cues to achieve a deeper\nunderstanding of video content. However, many existing methods rely on feeding\nframe-level captions into a single model, making it difficult to adequately\ncapture temporal and interactive contexts. To address this limitation, we\nintroduce VideoMultiAgents, a framework that integrates specialized agents for\nvision, scene graph analysis, and text processing. It enhances video\nunderstanding leveraging complementary multimodal reasoning from independently\noperating agents. Our approach is also supplemented with a question-guided\ncaption generation, which produces captions that highlight objects, actions,\nand temporal transitions directly relevant to a given query, thus improving the\nanswer accuracy. Experimental results demonstrate that our method achieves\nstate-of-the-art performance on Intent-QA (79.0%, +6.2% over previous SOTA),\nEgoSchema subset (75.4%, +3.4%), and NExT-QA (79.6%, +0.4%).", "AI": {"tldr": "VideoMultiAgents\u6846\u67b6\u901a\u8fc7\u591a\u6a21\u6001\u63a8\u7406\u63d0\u5347\u89c6\u9891\u95ee\u7b54\u6027\u80fd\uff0c\u7ed3\u5408\u89c6\u89c9\u3001\u573a\u666f\u56fe\u548c\u6587\u672c\u5904\u7406\u4ee3\u7406\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u89c6\u9891\u95ee\u7b54\u65b9\u6cd5\u4f9d\u8d56\u5355\u4e00\u6a21\u578b\u5904\u7406\u5e27\u7ea7\u63cf\u8ff0\uff0c\u96be\u4ee5\u6355\u6349\u65f6\u95f4\u548c\u4ea4\u4e92\u4e0a\u4e0b\u6587\u3002", "method": "\u63d0\u51faVideoMultiAgents\u6846\u67b6\uff0c\u6574\u5408\u89c6\u89c9\u3001\u573a\u666f\u56fe\u5206\u6790\u548c\u6587\u672c\u5904\u7406\u4ee3\u7406\uff0c\u5e76\u5f15\u5165\u95ee\u9898\u5f15\u5bfc\u7684\u6807\u9898\u751f\u6210\u3002", "result": "\u5728Intent-QA\u3001EgoSchema\u5b50\u96c6\u548cNExT-QA\u4e0a\u5206\u522b\u63d0\u53476.2%\u30013.4%\u548c0.4%\uff0c\u8fbe\u5230SOTA\u3002", "conclusion": "VideoMultiAgents\u901a\u8fc7\u591a\u4ee3\u7406\u534f\u4f5c\u548c\u95ee\u9898\u5f15\u5bfc\u6807\u9898\u751f\u6210\uff0c\u663e\u8457\u63d0\u5347\u89c6\u9891\u95ee\u7b54\u6027\u80fd\u3002"}}
{"id": "2504.20082", "pdf": "https://arxiv.org/pdf/2504.20082", "abs": "https://arxiv.org/abs/2504.20082", "authors": ["Firuz Kamalov", "David Santandreu Calonge", "Linda Smail", "Dilshod Azizov", "Dimple R. Thadani", "Theresa Kwong", "Amara Atif"], "title": "Evolution of AI in Education: Agentic Workflows", "categories": ["cs.AI", "cs.CY"], "comment": null, "summary": "Artificial intelligence (AI) has transformed various aspects of education,\nwith large language models (LLMs) driving advancements in automated tutoring,\nassessment, and content generation. However, conventional LLMs are constrained\nby their reliance on static training data, limited adaptability, and lack of\nreasoning. To address these limitations and foster more sustainable\ntechnological practices, AI agents have emerged as a promising new avenue for\neducational innovation. In this review, we examine agentic workflows in\neducation according to four major paradigms: reflection, planning, tool use,\nand multi-agent collaboration. We critically analyze the role of AI agents in\neducation through these key design paradigms, exploring their advantages,\napplications, and challenges. To illustrate the practical potential of agentic\nsystems, we present a proof-of-concept application: a multi-agent framework for\nautomated essay scoring. Preliminary results suggest this agentic approach may\noffer improved consistency compared to stand-alone LLMs. Our findings highlight\nthe transformative potential of AI agents in educational settings while\nunderscoring the need for further research into their interpretability,\ntrustworthiness, and sustainable impact on pedagogical impact.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86AI\u4ee3\u7406\u5728\u6559\u80b2\u4e2d\u7684\u6f5c\u529b\uff0c\u5206\u6790\u4e86\u56db\u79cd\u8bbe\u8ba1\u8303\u5f0f\uff08\u53cd\u601d\u3001\u89c4\u5212\u3001\u5de5\u5177\u4f7f\u7528\u548c\u591a\u4ee3\u7406\u534f\u4f5c\uff09\uff0c\u5e76\u901a\u8fc7\u4e00\u4e2a\u81ea\u52a8\u8bc4\u5206\u6846\u67b6\u5c55\u793a\u4e86\u5176\u4f18\u52bf\u3002", "motivation": "\u4f20\u7edf\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u6559\u80b2\u4e2d\u5b58\u5728\u9759\u6001\u6570\u636e\u4f9d\u8d56\u3001\u9002\u5e94\u6027\u548c\u63a8\u7406\u80fd\u529b\u4e0d\u8db3\u7684\u95ee\u9898\uff0cAI\u4ee3\u7406\u88ab\u89c6\u4e3a\u89e3\u51b3\u8fd9\u4e9b\u9650\u5236\u7684\u65b0\u9014\u5f84\u3002", "method": "\u901a\u8fc7\u56db\u79cd\u8bbe\u8ba1\u8303\u5f0f\uff08\u53cd\u601d\u3001\u89c4\u5212\u3001\u5de5\u5177\u4f7f\u7528\u548c\u591a\u4ee3\u7406\u534f\u4f5c\uff09\u5206\u6790AI\u4ee3\u7406\u5728\u6559\u80b2\u4e2d\u7684\u4f5c\u7528\uff0c\u5e76\u5f00\u53d1\u4e86\u4e00\u4e2a\u591a\u4ee3\u7406\u6846\u67b6\u7528\u4e8e\u81ea\u52a8\u4f5c\u6587\u8bc4\u5206\u3002", "result": "\u521d\u6b65\u7ed3\u679c\u8868\u660e\uff0c\u4ee3\u7406\u65b9\u6cd5\u6bd4\u72ec\u7acbLLMs\u5177\u6709\u66f4\u9ad8\u7684\u4e00\u81f4\u6027\u3002", "conclusion": "AI\u4ee3\u7406\u5728\u6559\u80b2\u4e2d\u5177\u6709\u53d8\u9769\u6f5c\u529b\uff0c\u4f46\u9700\u8fdb\u4e00\u6b65\u7814\u7a76\u5176\u53ef\u89e3\u91ca\u6027\u3001\u53ef\u4fe1\u5ea6\u548c\u5bf9\u6559\u5b66\u7684\u53ef\u6301\u7eed\u5f71\u54cd\u3002"}}
{"id": "2504.20097", "pdf": "https://arxiv.org/pdf/2504.20097", "abs": "https://arxiv.org/abs/2504.20097", "authors": ["Junran Guo", "Tonglin Mu", "Keyuan Li", "Jianing Li", "Ziyang Luo", "Ye Chen", "Xiaodong Fan", "Jinquan Huang", "Minjie Liu", "Jinbei Zhang", "Ruoyang Qi", "Naiting Gu", "Shihai Sun"], "title": "Long-Distance Field Demonstration of Imaging-Free Drone Identification in Intracity Environments", "categories": ["cs.CV", "quant-ph"], "comment": "15 pages, 9 figures", "summary": "Detecting small objects, such as drones, over long distances presents a\nsignificant challenge with broad implications for security, surveillance,\nenvironmental monitoring, and autonomous systems. Traditional imaging-based\nmethods rely on high-resolution image acquisition, but are often constrained by\nrange, power consumption, and cost. In contrast, data-driven\nsingle-photon-single-pixel light detection and ranging\n(\\text{D\\textsuperscript{2}SP\\textsuperscript{2}-LiDAR}) provides an\nimaging-free alternative, directly enabling target identification while\nreducing system complexity and cost. However, its detection range has been\nlimited to a few hundred meters. Here, we introduce a novel integration of\nresidual neural networks (ResNet) with\n\\text{D\\textsuperscript{2}SP\\textsuperscript{2}-LiDAR}, incorporating a refined\nobservation model to extend the detection range to 5~\\si{\\kilo\\meter} in an\nintracity environment while enabling high-accuracy identification of drone\nposes and types. Experimental results demonstrate that our approach not only\noutperforms conventional imaging-based recognition systems, but also achieves\n94.93\\% pose identification accuracy and 97.99\\% type classification accuracy,\neven under weak signal conditions with long distances and low signal-to-noise\nratios (SNRs). These findings highlight the potential of imaging-free methods\nfor robust long-range detection of small targets in real-world scenarios.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u6b8b\u5dee\u795e\u7ecf\u7f51\u7edc\uff08ResNet\uff09\u4e0eD2SP2-LiDAR\u7684\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5c0f\u578b\u76ee\u6807\uff08\u5982\u65e0\u4eba\u673a\uff09\u7684\u8fdc\u8ddd\u79bb\u68c0\u6d4b\u80fd\u529b\uff0c\u5e76\u5b9e\u73b0\u4e86\u9ad8\u7cbe\u5ea6\u7684\u59ff\u6001\u548c\u7c7b\u578b\u8bc6\u522b\u3002", "motivation": "\u4f20\u7edf\u57fa\u4e8e\u9ad8\u5206\u8fa8\u7387\u6210\u50cf\u7684\u65b9\u6cd5\u5728\u8fdc\u8ddd\u79bb\u68c0\u6d4b\u5c0f\u578b\u76ee\u6807\u65f6\u5b58\u5728\u8303\u56f4\u3001\u529f\u8017\u548c\u6210\u672c\u9650\u5236\uff0c\u800c\u73b0\u6709\u7684D2SP2-LiDAR\u6280\u672f\u68c0\u6d4b\u8303\u56f4\u6709\u9650\u3002", "method": "\u901a\u8fc7\u5c06ResNet\u4e0eD2SP2-LiDAR\u7ed3\u5408\uff0c\u5e76\u4f18\u5316\u89c2\u6d4b\u6a21\u578b\uff0c\u5c06\u68c0\u6d4b\u8303\u56f4\u6269\u5c55\u52305\u516c\u91cc\uff0c\u540c\u65f6\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u76ee\u6807\u8bc6\u522b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u57285\u516c\u91cc\u8303\u56f4\u5185\u5b9e\u73b0\u4e8694.93%\u7684\u59ff\u6001\u8bc6\u522b\u51c6\u786e\u7387\u548c97.99%\u7684\u7c7b\u578b\u5206\u7c7b\u51c6\u786e\u7387\uff0c\u4f18\u4e8e\u4f20\u7edf\u6210\u50cf\u65b9\u6cd5\u3002", "conclusion": "\u6210\u50cf\u65e0\u5173\u7684\u65b9\u6cd5\u5728\u8fdc\u8ddd\u79bb\u5c0f\u578b\u76ee\u6807\u68c0\u6d4b\u4e2d\u5177\u6709\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2504.20084", "pdf": "https://arxiv.org/pdf/2504.20084", "abs": "https://arxiv.org/abs/2504.20084", "authors": ["Xiaojian Li", "Haoyuan Shi", "Rongwu Xu", "Wei Xu"], "title": "AI Awareness", "categories": ["cs.AI", "cs.CL", "cs.CY"], "comment": null, "summary": "Recent breakthroughs in artificial intelligence (AI) have brought about\nincreasingly capable systems that demonstrate remarkable abilities in\nreasoning, language understanding, and problem-solving. These advancements have\nprompted a renewed examination of AI awareness, not as a philosophical question\nof consciousness, but as a measurable, functional capacity. In this review, we\nexplore the emerging landscape of AI awareness, which includes meta-cognition\n(the ability to represent and reason about its own state), self-awareness\n(recognizing its own identity, knowledge, limitations, inter alia), social\nawareness (modeling the knowledge, intentions, and behaviors of other agents),\nand situational awareness (assessing and responding to the context in which it\noperates).\n  First, we draw on insights from cognitive science, psychology, and\ncomputational theory to trace the theoretical foundations of awareness and\nexamine how the four distinct forms of AI awareness manifest in\nstate-of-the-art AI. Next, we systematically analyze current evaluation methods\nand empirical findings to better understand these manifestations. Building on\nthis, we explore how AI awareness is closely linked to AI capabilities,\ndemonstrating that more aware AI agents tend to exhibit higher levels of\nintelligent behaviors. Finally, we discuss the risks associated with AI\nawareness, including key topics in AI safety, alignment, and broader ethical\nconcerns.\n  AI awareness is a double-edged sword: it improves general capabilities, i.e.,\nreasoning, safety, while also raises concerns around misalignment and societal\nrisks, demanding careful oversight as AI capabilities grow. On the whole, our\ninterdisciplinary review provides a roadmap for future research and aims to\nclarify the role of AI awareness in the ongoing development of intelligent\nmachines.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86AI\u610f\u8bc6\u7684\u6982\u5ff5\uff0c\u5305\u62ec\u5143\u8ba4\u77e5\u3001\u81ea\u6211\u610f\u8bc6\u3001\u793e\u4f1a\u610f\u8bc6\u548c\u60c5\u5883\u610f\u8bc6\uff0c\u63a2\u8ba8\u5176\u7406\u8bba\u57fa\u7840\u3001\u8bc4\u4f30\u65b9\u6cd5\u53ca\u4e0eAI\u80fd\u529b\u7684\u5173\u7cfb\uff0c\u540c\u65f6\u6307\u51fa\u5176\u6f5c\u5728\u98ce\u9669\u3002", "motivation": "\u968f\u7740AI\u80fd\u529b\u7684\u63d0\u5347\uff0c\u7814\u7a76AI\u610f\u8bc6\u7684\u529f\u80fd\u6027\u8868\u73b0\u53ca\u5176\u5f71\u54cd\u53d8\u5f97\u91cd\u8981\u3002", "method": "\u7ed3\u5408\u8ba4\u77e5\u79d1\u5b66\u3001\u5fc3\u7406\u5b66\u548c\u8ba1\u7b97\u7406\u8bba\uff0c\u5206\u6790AI\u610f\u8bc6\u7684\u56db\u79cd\u5f62\u5f0f\u53ca\u5176\u5728\u5148\u8fdbAI\u4e2d\u7684\u8868\u73b0\uff0c\u5e76\u8bc4\u4f30\u76f8\u5173\u5b9e\u8bc1\u7814\u7a76\u3002", "result": "\u7814\u7a76\u53d1\u73b0AI\u610f\u8bc6\u4e0e\u667a\u80fd\u884c\u4e3a\u6c34\u5e73\u6b63\u76f8\u5173\uff0c\u4f46\u4e5f\u5e26\u6765\u5b89\u5168\u548c\u5bf9\u9f50\u98ce\u9669\u3002", "conclusion": "AI\u610f\u8bc6\u662f\u4e00\u628a\u53cc\u5203\u5251\uff0c\u9700\u5728\u63d0\u5347\u80fd\u529b\u7684\u540c\u65f6\u8c28\u614e\u5e94\u5bf9\u5176\u98ce\u9669\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u65b9\u5411\u3002"}}
{"id": "2504.20104", "pdf": "https://arxiv.org/pdf/2504.20104", "abs": "https://arxiv.org/abs/2504.20104", "authors": ["Luiz F. P. Southier", "Marcelo Filipak", "Luiz A. Zanlorensi", "Ildefonso Wasilevski", "Fabio Favarim", "Jefferson T. Oliva", "Marcelo Teixeira", "Dalcimar Casanova"], "title": "An on-production high-resolution longitudinal neonatal fingerprint database in Brazil", "categories": ["cs.CV"], "comment": null, "summary": "The neonatal period is critical for survival, requiring accurate and early\nidentification to enable timely interventions such as vaccinations, HIV\ntreatment, and nutrition programs. Biometric solutions offer potential for\nchild protection by helping to prevent baby swaps, locate missing children, and\nsupport national identity systems. However, developing effective biometric\nidentification systems for newborns remains a major challenge due to the\nphysiological variability caused by finger growth, weight changes, and skin\ntexture alterations during early development. Current literature has attempted\nto address these issues by applying scaling factors to emulate growth-induced\ndistortions in minutiae maps, but such approaches fail to capture the complex\nand non-linear growth patterns of infants. A key barrier to progress in this\ndomain is the lack of comprehensive, longitudinal biometric datasets capturing\nthe evolution of neonatal fingerprints over time. This study addresses this gap\nby focusing on designing and developing a high-quality biometric database of\nneonatal fingerprints, acquired at multiple early life stages. The dataset is\nintended to support the training and evaluation of machine learning models\naimed at emulating the effects of growth on biometric features. We hypothesize\nthat such a dataset will enable the development of more robust and accurate\nDeep Learning-based models, capable of predicting changes in the minutiae map\nwith higher fidelity than conventional scaling-based methods. Ultimately, this\neffort lays the groundwork for more reliable biometric identification systems\ntailored to the unique developmental trajectory of newborns.", "AI": {"tldr": "\u7814\u7a76\u65e8\u5728\u5f00\u53d1\u65b0\u751f\u513f\u6307\u7eb9\u7684\u9ad8\u8d28\u91cf\u751f\u7269\u7279\u5f81\u6570\u636e\u5e93\uff0c\u4ee5\u652f\u6301\u673a\u5668\u5b66\u4e60\u6a21\u578b\u8bad\u7ec3\uff0c\u89e3\u51b3\u65b0\u751f\u513f\u6307\u7eb9\u56e0\u751f\u957f\u53d8\u5316\u5bfc\u81f4\u7684\u8bc6\u522b\u96be\u9898\u3002", "motivation": "\u65b0\u751f\u513f\u671f\u5bf9\u751f\u5b58\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u751f\u7269\u8bc6\u522b\u7cfb\u7edf\u56e0\u751f\u7406\u53d8\u5316\uff08\u5982\u6307\u7eb9\u751f\u957f\uff09\u96be\u4ee5\u51c6\u786e\u8bc6\u522b\uff0c\u7f3a\u4e4f\u76f8\u5173\u6570\u636e\u96c6\u963b\u788d\u4e86\u6280\u672f\u8fdb\u6b65\u3002", "method": "\u8bbe\u8ba1\u5e76\u5f00\u53d1\u591a\u9636\u6bb5\u91c7\u96c6\u7684\u65b0\u751f\u513f\u6307\u7eb9\u6570\u636e\u5e93\uff0c\u7528\u4e8e\u8bad\u7ec3\u548c\u8bc4\u4f30\u673a\u5668\u5b66\u4e60\u6a21\u578b\uff0c\u6a21\u62df\u751f\u957f\u5bf9\u751f\u7269\u7279\u5f81\u7684\u5f71\u54cd\u3002", "result": "\u9884\u671f\u8be5\u6570\u636e\u96c6\u5c06\u652f\u6301\u5f00\u53d1\u66f4\u51c6\u786e\u7684\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u4f18\u4e8e\u4f20\u7edf\u7684\u7f29\u653e\u65b9\u6cd5\u3002", "conclusion": "\u7814\u7a76\u4e3a\u9488\u5bf9\u65b0\u751f\u513f\u72ec\u7279\u751f\u957f\u8f68\u8ff9\u7684\u53ef\u9760\u751f\u7269\u8bc6\u522b\u7cfb\u7edf\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2504.20090", "pdf": "https://arxiv.org/pdf/2504.20090", "abs": "https://arxiv.org/abs/2504.20090", "authors": ["Aishik Sanyal", "Samuel Schapiro", "Sumuk Shashidhar", "Royce Moon", "Lav R. Varshney", "Dilek Hakkani-Tur"], "title": "Spark: A System for Scientifically Creative Idea Generation", "categories": ["cs.AI", "cs.IR", "cs.LG"], "comment": null, "summary": "Recently, large language models (LLMs) have shown promising abilities to\ngenerate novel research ideas in science, a direction which coincides with many\nfoundational principles in computational creativity (CC). In light of these\ndevelopments, we present an idea generation system named Spark that couples\nretrieval-augmented idea generation using LLMs with a reviewer model named\nJudge trained on 600K scientific reviews from OpenReview. Our work is both a\nsystem demonstration and intended to inspire other CC researchers to explore\ngrounding the generation and evaluation of scientific ideas within foundational\nCC principles. To this end, we release the annotated dataset used to train\nJudge, inviting other researchers to explore the use of LLMs for idea\ngeneration and creative evaluations.", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86\u4e00\u4e2a\u540d\u4e3aSpark\u7684\u7cfb\u7edf\uff0c\u7ed3\u5408\u68c0\u7d22\u589e\u5f3a\u7684LLM\u751f\u6210\u79d1\u5b66\u521b\u610f\u4e0e\u57fa\u4e8eOpenReview\u8bad\u7ec3\u7684\u8bc4\u5ba1\u6a21\u578bJudge\uff0c\u65e8\u5728\u63a8\u52a8\u8ba1\u7b97\u521b\u9020\u529b\u7814\u7a76\u3002", "motivation": "\u63a2\u7d22\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u79d1\u5b66\u521b\u610f\u751f\u6210\u4e2d\u7684\u6f5c\u529b\uff0c\u5e76\u4e0e\u8ba1\u7b97\u521b\u9020\u529b\uff08CC\uff09\u7684\u57fa\u7840\u539f\u5219\u7ed3\u5408\u3002", "method": "\u5f00\u53d1Spark\u7cfb\u7edf\uff0c\u7ed3\u5408\u68c0\u7d22\u589e\u5f3a\u7684LLM\u751f\u6210\u521b\u610f\uff0c\u5e76\u8bad\u7ec3\u8bc4\u5ba1\u6a21\u578bJudge\uff08\u57fa\u4e8e60\u4e07\u4efd\u79d1\u5b66\u8bc4\u5ba1\u6570\u636e\uff09\u3002", "result": "\u7cfb\u7edf\u5c55\u793a\u4e86LLMs\u5728\u79d1\u5b66\u521b\u610f\u751f\u6210\u4e2d\u7684\u5e94\u7528\u6f5c\u529b\uff0c\u5e76\u53d1\u5e03\u4e86\u8bad\u7ec3Judge\u7684\u6570\u636e\u96c6\u3002", "conclusion": "\u7814\u7a76\u4e3a\u8ba1\u7b97\u521b\u9020\u529b\u9886\u57df\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\uff0c\u9f13\u52b1\u8fdb\u4e00\u6b65\u63a2\u7d22LLMs\u5728\u521b\u610f\u751f\u6210\u4e0e\u8bc4\u4f30\u4e2d\u7684\u5e94\u7528\u3002"}}
{"id": "2504.20111", "pdf": "https://arxiv.org/pdf/2504.20111", "abs": "https://arxiv.org/abs/2504.20111", "authors": ["Anubhav Jain", "Yuya Kobayashi", "Naoki Murata", "Yuhta Takida", "Takashi Shibuya", "Yuki Mitsufuji", "Niv Cohen", "Nasir Memon", "Julian Togelius"], "title": "Forging and Removing Latent-Noise Diffusion Watermarks Using a Single Image", "categories": ["cs.CV"], "comment": null, "summary": "Watermarking techniques are vital for protecting intellectual property and\npreventing fraudulent use of media. Most previous watermarking schemes designed\nfor diffusion models embed a secret key in the initial noise. The resulting\npattern is often considered hard to remove and forge into unrelated images. In\nthis paper, we propose a black-box adversarial attack without presuming access\nto the diffusion model weights. Our attack uses only a single watermarked\nexample and is based on a simple observation: there is a many-to-one mapping\nbetween images and initial noises. There are regions in the clean image latent\nspace pertaining to each watermark that get mapped to the same initial noise\nwhen inverted. Based on this intuition, we propose an adversarial attack to\nforge the watermark by introducing perturbations to the images such that we can\nenter the region of watermarked images. We show that we can also apply a\nsimilar approach for watermark removal by learning perturbations to exit this\nregion. We report results on multiple watermarking schemes (Tree-Ring, RingID,\nWIND, and Gaussian Shading) across two diffusion models (SDv1.4 and SDv2.0).\nOur results demonstrate the effectiveness of the attack and expose\nvulnerabilities in the watermarking methods, motivating future research on\nimproving them.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u6269\u6563\u6a21\u578b\u6c34\u5370\u7684\u9ed1\u76d2\u5bf9\u6297\u653b\u51fb\u65b9\u6cd5\uff0c\u4ec5\u9700\u4e00\u4e2a\u6c34\u5370\u6837\u672c\u5373\u53ef\u4f2a\u9020\u6216\u79fb\u9664\u6c34\u5370\uff0c\u66b4\u9732\u4e86\u73b0\u6709\u6c34\u5370\u6280\u672f\u7684\u8106\u5f31\u6027\u3002", "motivation": "\u73b0\u6709\u6c34\u5370\u6280\u672f\u901a\u5e38\u5c06\u5bc6\u94a5\u5d4c\u5165\u521d\u59cb\u566a\u58f0\u4e2d\uff0c\u88ab\u8ba4\u4e3a\u96be\u4ee5\u79fb\u9664\u6216\u4f2a\u9020\u3002\u672c\u6587\u65e8\u5728\u63ed\u793a\u5176\u6f5c\u5728\u6f0f\u6d1e\u3002", "method": "\u57fa\u4e8e\u56fe\u50cf\u4e0e\u521d\u59cb\u566a\u58f0\u7684\u591a\u5bf9\u4e00\u6620\u5c04\u5173\u7cfb\uff0c\u901a\u8fc7\u6270\u52a8\u56fe\u50cf\u8fdb\u5165\u6216\u9000\u51fa\u6c34\u5370\u533a\u57df\uff0c\u5b9e\u73b0\u4f2a\u9020\u6216\u79fb\u9664\u6c34\u5370\u3002", "result": "\u5728\u591a\u79cd\u6c34\u5370\u65b9\u6848\uff08Tree-Ring\u3001RingID\u7b49\uff09\u548c\u6269\u6563\u6a21\u578b\uff08SDv1.4\u3001SDv2.0\uff09\u4e0a\u9a8c\u8bc1\u4e86\u653b\u51fb\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u73b0\u6709\u6c34\u5370\u6280\u672f\u5b58\u5728\u6f0f\u6d1e\uff0c\u9700\u8fdb\u4e00\u6b65\u7814\u7a76\u6539\u8fdb\u3002"}}
{"id": "2504.20109", "pdf": "https://arxiv.org/pdf/2504.20109", "abs": "https://arxiv.org/abs/2504.20109", "authors": ["Rajeev Gupta", "Suhani Gupta", "Ronak Parikh", "Divya Gupta", "Amir Javaheri", "Jairaj Singh Shaktawat"], "title": "Personalized Artificial General Intelligence (AGI) via Neuroscience-Inspired Continuous Learning Systems", "categories": ["cs.AI", "cs.LG"], "comment": "39 pages, 16 figures", "summary": "Artificial Intelligence has made remarkable advancements in recent years,\nprimarily driven by increasingly large deep learning models. However, achieving\ntrue Artificial General Intelligence (AGI) demands fundamentally new\narchitectures rather than merely scaling up existing models. Current approaches\nlargely depend on expanding model parameters, which improves task-specific\nperformance but falls short in enabling continuous, adaptable, and generalized\nlearning. Achieving AGI capable of continuous learning and personalization on\nresource-constrained edge devices is an even bigger challenge.\n  This paper reviews the state of continual learning and neuroscience-inspired\nAI, and proposes a novel architecture for Personalized AGI that integrates\nbrain-like learning mechanisms for edge deployment. We review literature on\ncontinuous lifelong learning, catastrophic forgetting, and edge AI, and discuss\nkey neuroscience principles of human learning, including Synaptic Pruning,\nHebbian plasticity, Sparse Coding, and Dual Memory Systems, as inspirations for\nAI systems. Building on these insights, we outline an AI architecture that\nfeatures complementary fast-and-slow learning modules, synaptic\nself-optimization, and memory-efficient model updates to support on-device\nlifelong adaptation.\n  Conceptual diagrams of the proposed architecture and learning processes are\nprovided. We address challenges such as catastrophic forgetting, memory\nefficiency, and system scalability, and present application scenarios for\nmobile AI assistants and embodied AI systems like humanoid robots. We conclude\nwith key takeaways and future research directions toward truly continual,\npersonalized AGI on the edge. While the architecture is theoretical, it\nsynthesizes diverse findings and offers a roadmap for future implementation.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u7684\u4e2a\u6027\u5316\u901a\u7528\u4eba\u5de5\u667a\u80fd\uff08AGI\uff09\u67b6\u6784\uff0c\u7ed3\u5408\u795e\u7ecf\u79d1\u5b66\u542f\u53d1\u673a\u5236\uff0c\u652f\u6301\u8fb9\u7f18\u8bbe\u5907\u7684\u6301\u7eed\u5b66\u4e60\u548c\u9002\u5e94\u3002", "motivation": "\u5f53\u524d\u5927\u578b\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u867d\u5728\u4efb\u52a1\u6027\u80fd\u4e0a\u6709\u6240\u63d0\u5347\uff0c\u4f46\u65e0\u6cd5\u5b9e\u73b0\u771f\u6b63\u7684\u901a\u7528\u4eba\u5de5\u667a\u80fd\uff08AGI\uff09\uff0c\u5c24\u5176\u662f\u5728\u8d44\u6e90\u53d7\u9650\u7684\u8fb9\u7f18\u8bbe\u5907\u4e0a\u3002", "method": "\u6574\u5408\u795e\u7ecf\u79d1\u5b66\u539f\u7406\uff08\u5982\u7a81\u89e6\u4fee\u526a\u3001Hebbian\u53ef\u5851\u6027\u7b49\uff09\uff0c\u8bbe\u8ba1\u4e92\u8865\u7684\u5feb\u6162\u5b66\u4e60\u6a21\u5757\u548c\u5185\u5b58\u9ad8\u6548\u6a21\u578b\u66f4\u65b0\u673a\u5236\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7406\u8bba\u67b6\u6784\uff0c\u652f\u6301\u8fb9\u7f18\u8bbe\u5907\u7684\u7ec8\u8eab\u5b66\u4e60\u548c\u4e2a\u6027\u5316\u9002\u5e94\uff0c\u5e76\u8ba8\u8bba\u4e86\u5e94\u7528\u573a\u666f\uff08\u5982\u79fb\u52a8AI\u52a9\u624b\uff09\u3002", "conclusion": "\u8be5\u67b6\u6784\u4e3a\u672a\u6765\u5b9e\u73b0\u771f\u6b63\u6301\u7eed\u3001\u4e2a\u6027\u5316\u7684\u8fb9\u7f18AGI\u63d0\u4f9b\u4e86\u8def\u7ebf\u56fe\uff0c\u4f46\u4ecd\u9700\u8fdb\u4e00\u6b65\u5b9e\u73b0\u3002"}}
{"id": "2504.20178", "pdf": "https://arxiv.org/pdf/2504.20178", "abs": "https://arxiv.org/abs/2504.20178", "authors": ["Zhe Cui", "Yuli Li", "Le-Nam Tran"], "title": "A Transformer-based Multimodal Fusion Model for Efficient Crowd Counting Using Visual and Wireless Signals", "categories": ["cs.CV", "cs.LG"], "comment": "This paper was accepted at IEEE WCNC 2025", "summary": "Current crowd-counting models often rely on single-modal inputs, such as\nvisual images or wireless signal data, which can result in significant\ninformation loss and suboptimal recognition performance. To address these\nshortcomings, we propose TransFusion, a novel multimodal fusion-based\ncrowd-counting model that integrates Channel State Information (CSI) with image\ndata. By leveraging the powerful capabilities of Transformer networks,\nTransFusion effectively combines these two distinct data modalities, enabling\nthe capture of comprehensive global contextual information that is critical for\naccurate crowd estimation. However, while transformers are well capable of\ncapturing global features, they potentially fail to identify finer-grained,\nlocal details essential for precise crowd counting. To mitigate this, we\nincorporate Convolutional Neural Networks (CNNs) into the model architecture,\nenhancing its ability to extract detailed local features that complement the\nglobal context provided by the Transformer. Extensive experimental evaluations\ndemonstrate that TransFusion achieves high accuracy with minimal counting\nerrors while maintaining superior efficiency.", "AI": {"tldr": "TransFusion\u662f\u4e00\u79cd\u57fa\u4e8e\u591a\u6a21\u6001\u878d\u5408\u7684\u4eba\u7fa4\u8ba1\u6570\u6a21\u578b\uff0c\u7ed3\u5408\u4e86CSI\u548c\u56fe\u50cf\u6570\u636e\uff0c\u5229\u7528Transformer\u548cCNN\u7684\u4f18\u52bf\uff0c\u5b9e\u73b0\u4e86\u9ad8\u7cbe\u5ea6\u548c\u9ad8\u6548\u7684\u4eba\u7fa4\u8ba1\u6570\u3002", "motivation": "\u5f53\u524d\u4eba\u7fa4\u8ba1\u6570\u6a21\u578b\u4f9d\u8d56\u5355\u6a21\u6001\u8f93\u5165\uff08\u5982\u56fe\u50cf\u6216\u65e0\u7ebf\u4fe1\u53f7\u6570\u636e\uff09\uff0c\u5bfc\u81f4\u4fe1\u606f\u4e22\u5931\u548c\u6027\u80fd\u4e0d\u4f73\u3002", "method": "\u63d0\u51faTransFusion\u6a21\u578b\uff0c\u7ed3\u5408Transformer\uff08\u5168\u5c40\u7279\u5f81\uff09\u548cCNN\uff08\u5c40\u90e8\u7279\u5f81\uff09\uff0c\u878d\u5408CSI\u4e0e\u56fe\u50cf\u6570\u636e\u3002", "result": "\u5b9e\u9a8c\u8868\u660eTransFusion\u5728\u8ba1\u6570\u8bef\u5dee\u6700\u5c0f\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u4e86\u9ad8\u7cbe\u5ea6\u548c\u9ad8\u6548\u7387\u3002", "conclusion": "TransFusion\u901a\u8fc7\u591a\u6a21\u6001\u878d\u5408\u548c\u6df7\u5408\u7f51\u7edc\u67b6\u6784\u663e\u8457\u63d0\u5347\u4e86\u4eba\u7fa4\u8ba1\u6570\u7684\u6027\u80fd\u3002"}}
{"id": "2504.20113", "pdf": "https://arxiv.org/pdf/2504.20113", "abs": "https://arxiv.org/abs/2504.20113", "authors": ["Lingbo Li", "Anuradha Mathrani", "Teo Susnjak"], "title": "Transforming Evidence Synthesis: A Systematic Review of the Evolution of Automated Meta-Analysis in the Age of AI", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "Exponential growth in scientific literature has heightened the demand for\nefficient evidence-based synthesis, driving the rise of the field of Automated\nMeta-analysis (AMA) powered by natural language processing and machine\nlearning. This PRISMA systematic review introduces a structured framework for\nassessing the current state of AMA, based on screening 978 papers from 2006 to\n2024, and analyzing 54 studies across diverse domains. Findings reveal a\npredominant focus on automating data processing (57%), such as extraction and\nstatistical modeling, while only 17% address advanced synthesis stages. Just\none study (2%) explored preliminary full-process automation, highlighting a\ncritical gap that limits AMA's capacity for comprehensive synthesis. Despite\nrecent breakthroughs in large language models (LLMs) and advanced AI, their\nintegration into statistical modeling and higher-order synthesis, such as\nheterogeneity assessment and bias evaluation, remains underdeveloped. This has\nconstrained AMA's potential for fully autonomous meta-analysis. From our\ndataset spanning medical (67%) and non-medical (33%) applications, we found\nthat AMA has exhibited distinct implementation patterns and varying degrees of\neffectiveness in actually improving efficiency, scalability, and\nreproducibility. While automation has enhanced specific meta-analytic tasks,\nachieving seamless, end-to-end automation remains an open challenge. As AI\nsystems advance in reasoning and contextual understanding, addressing these\ngaps is now imperative. Future efforts must focus on bridging automation across\nall meta-analysis stages, refining interpretability, and ensuring\nmethodological robustness to fully realize AMA's potential for scalable,\ndomain-agnostic synthesis.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u7cfb\u7edf\u7efc\u8ff0\u8bc4\u4f30\u4e86\u81ea\u52a8\u5316\u5143\u5206\u6790\uff08AMA\uff09\u7684\u73b0\u72b6\uff0c\u53d1\u73b0\u5176\u4e3b\u8981\u96c6\u4e2d\u5728\u6570\u636e\u5904\u7406\u9636\u6bb5\uff0c\u800c\u9ad8\u7ea7\u5408\u6210\u9636\u6bb5\u7684\u7814\u7a76\u8f83\u5c11\uff0c\u9650\u5236\u4e86\u5168\u9762\u81ea\u52a8\u5316\u7684\u6f5c\u529b\u3002", "motivation": "\u79d1\u5b66\u6587\u732e\u7684\u6307\u6570\u589e\u957f\u63a8\u52a8\u4e86\u5bf9\u9ad8\u6548\u8bc1\u636e\u5408\u6210\u7684\u9700\u6c42\uff0c\u4fc3\u4f7fAMA\u9886\u57df\u7684\u53d1\u5c55\u3002\u672c\u6587\u65e8\u5728\u8bc4\u4f30AMA\u7684\u5f53\u524d\u72b6\u6001\u548c\u672a\u6765\u65b9\u5411\u3002", "method": "\u91c7\u7528PRISMA\u7cfb\u7edf\u7efc\u8ff0\u65b9\u6cd5\uff0c\u7b5b\u9009\u4e86978\u7bc7\u8bba\u6587\uff082006-2024\u5e74\uff09\uff0c\u5e76\u5206\u6790\u4e8654\u9879\u7814\u7a76\uff0c\u6db5\u76d6\u533b\u5b66\u548c\u975e\u533b\u5b66\u9886\u57df\u3002", "result": "\u7814\u7a76\u53d1\u73b0AMA\u4e3b\u8981\u5173\u6ce8\u6570\u636e\u5904\u7406\uff0857%\uff09\uff0c\u800c\u9ad8\u7ea7\u5408\u6210\u9636\u6bb5\uff0817%\uff09\u548c\u5168\u6d41\u7a0b\u81ea\u52a8\u5316\uff082%\uff09\u7814\u7a76\u8f83\u5c11\u3002AI\u5728\u7edf\u8ba1\u5efa\u6a21\u548c\u9ad8\u7ea7\u5408\u6210\u4e2d\u7684\u5e94\u7528\u4ecd\u4e0d\u8db3\u3002", "conclusion": "\u672a\u6765\u9700\u586b\u8865\u81ea\u52a8\u5316\u5728\u5404\u9636\u6bb5\u7684\u7a7a\u767d\uff0c\u63d0\u5347\u89e3\u91ca\u6027\u548c\u65b9\u6cd5\u7a33\u5065\u6027\uff0c\u4ee5\u5b9e\u73b0AMA\u5728\u8de8\u9886\u57df\u5408\u6210\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2504.20179", "pdf": "https://arxiv.org/pdf/2504.20179", "abs": "https://arxiv.org/abs/2504.20179", "authors": ["Jingjing Wang", "Dan Zhang", "Joshua Luo", "Yin Yang", "Feng Luo"], "title": "Integration Flow Models", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Ordinary differential equation (ODE) based generative models have emerged as\na powerful approach for producing high-quality samples in many applications.\nHowever, the ODE-based methods either suffer the discretization error of\nnumerical solvers of ODE, which restricts the quality of samples when only a\nfew NFEs are used, or struggle with training instability. In this paper, we\nproposed Integration Flow, which directly learns the integral of ODE-based\ntrajectory paths without solving the ODE functions. Moreover, Integration Flow\nexplicitly incorporates the target state $\\mathbf{x}_0$ as the anchor state in\nguiding the reverse-time dynamics. We have theoretically proven this can\ncontribute to both stability and accuracy. To the best of our knowledge,\nIntegration Flow is the first model with a unified structure to estimate\nODE-based generative models and the first to show the exact straightness of\n1-Rectified Flow without reflow. Through theoretical analysis and empirical\nevaluations, we show that Integration Flows achieve improved performance when\nit is applied to existing ODE-based models, such as diffusion models, Rectified\nFlows, and PFGM++. Specifically, Integration Flow achieves one-step generation\non CIFAR10 with FIDs of 2.86 for the Variance Exploding (VE) diffusion model,\n3.36 for rectified flow without reflow, and 2.91 for PFGM++; and on ImageNet\nwith FIDs of 4.09 for VE diffusion model, 4.35 for rectified flow without\nreflow and 4.15 for PFGM++.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faIntegration Flow\uff0c\u4e00\u79cd\u76f4\u63a5\u5b66\u4e60ODE\u8f68\u8ff9\u8def\u5f84\u79ef\u5206\u7684\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u4f20\u7edfODE\u751f\u6210\u6a21\u578b\u7684\u79bb\u6563\u5316\u8bef\u5dee\u548c\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u6027\u95ee\u9898\u3002", "motivation": "\u4f20\u7edfODE\u751f\u6210\u6a21\u578b\u5b58\u5728\u6570\u503c\u6c42\u89e3\u5668\u7684\u79bb\u6563\u5316\u8bef\u5dee\u548c\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u6027\uff0c\u9650\u5236\u4e86\u6837\u672c\u8d28\u91cf\u3002", "method": "\u63d0\u51faIntegration Flow\uff0c\u76f4\u63a5\u5b66\u4e60ODE\u8f68\u8ff9\u7684\u79ef\u5206\uff0c\u5e76\u5f15\u5165\u76ee\u6807\u72b6\u6001\u4f5c\u4e3a\u53cd\u5411\u52a8\u529b\u5b66\u7684\u951a\u70b9\u3002", "result": "\u5728CIFAR10\u548cImageNet\u4e0a\uff0cIntegration Flow\u663e\u8457\u63d0\u5347\u4e86\u73b0\u6709ODE\u6a21\u578b\u7684\u6027\u80fd\uff08\u5982\u6269\u6563\u6a21\u578b\u3001Rectified Flows\u548cPFGM++\uff09\u3002", "conclusion": "Integration Flow\u901a\u8fc7\u7edf\u4e00\u7ed3\u6784\u548c\u7406\u8bba\u8bc1\u660e\uff0c\u63d0\u9ad8\u4e86ODE\u751f\u6210\u6a21\u578b\u7684\u7a33\u5b9a\u6027\u548c\u51c6\u786e\u6027\u3002"}}
{"id": "2504.20278", "pdf": "https://arxiv.org/pdf/2504.20278", "abs": "https://arxiv.org/abs/2504.20278", "authors": ["Haoyu Yang", "Kamyar Azizzadenesheli", "Haoxing Ren"], "title": "Deep Physics Prior for First Order Inverse Optimization", "categories": ["cs.AI"], "comment": "10 pages, 5 figure. Under Review", "summary": "Inverse design optimization aims to infer system parameters from observed\nsolutions, posing critical challenges across domains such as semiconductor\nmanufacturing, structural engineering, materials science, and fluid dynamics.\nThe lack of explicit mathematical representations in many systems complicates\nthis process and makes the first order optimization impossible. Mainstream\napproaches, including generative AI and Bayesian optimization, address these\nchallenges but have limitations. Generative AI is computationally expensive,\nwhile Bayesian optimization, relying on surrogate models, suffers from\nscalability, sensitivity to priors, and noise issues, often leading to\nsuboptimal solutions. This paper introduces Deep Physics Prior (DPP), a novel\nmethod enabling first-order gradient-based inverse optimization with surrogate\nmachine learning models. By leveraging pretrained auxiliary Neural Operators,\nDPP enforces prior distribution constraints to ensure robust and meaningful\nsolutions. This approach is particularly effective when prior data and\nobservation distributions are unknown.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aDeep Physics Prior\uff08DPP\uff09\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u9006\u8bbe\u8ba1\u4f18\u5316\u95ee\u9898\uff0c\u901a\u8fc7\u9884\u8bad\u7ec3\u7684\u8f85\u52a9\u795e\u7ecf\u7b97\u5b50\u5b9e\u73b0\u68af\u5ea6\u4f18\u5316\u3002", "motivation": "\u9006\u8bbe\u8ba1\u4f18\u5316\u5728\u591a\u4e2a\u9886\u57df\uff08\u5982\u534a\u5bfc\u4f53\u5236\u9020\u3001\u7ed3\u6784\u5de5\u7a0b\u7b49\uff09\u9762\u4e34\u6311\u6218\uff0c\u4f20\u7edf\u65b9\u6cd5\uff08\u5982\u751f\u6210\u5f0fAI\u548c\u8d1d\u53f6\u65af\u4f18\u5316\uff09\u5b58\u5728\u8ba1\u7b97\u6210\u672c\u9ad8\u6216\u6a21\u578b\u4f9d\u8d56\u6027\u5f3a\u7684\u95ee\u9898\u3002", "method": "DPP\u5229\u7528\u9884\u8bad\u7ec3\u7684\u8f85\u52a9\u795e\u7ecf\u7b97\u5b50\uff0c\u901a\u8fc7\u5148\u9a8c\u5206\u5e03\u7ea6\u675f\u5b9e\u73b0\u68af\u5ea6\u4f18\u5316\uff0c\u9002\u7528\u4e8e\u5148\u9a8c\u6570\u636e\u548c\u89c2\u6d4b\u5206\u5e03\u672a\u77e5\u7684\u60c5\u51b5\u3002", "result": "DPP\u80fd\u591f\u63d0\u4f9b\u7a33\u5065\u4e14\u6709\u610f\u4e49\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u514b\u670d\u4e86\u4f20\u7edf\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002", "conclusion": "DPP\u4e3a\u9006\u8bbe\u8ba1\u4f18\u5316\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u53ef\u9760\u7684\u65b0\u65b9\u6cd5\u3002"}}
{"id": "2504.20199", "pdf": "https://arxiv.org/pdf/2504.20199", "abs": "https://arxiv.org/abs/2504.20199", "authors": ["Juntian Zhang", "Chuanqi cheng", "Yuhan Liu", "Wei Liu", "Jian Luan", "Rui Yan"], "title": "Weaving Context Across Images: Improving Vision-Language Models through Focus-Centric Visual Chains", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": null, "summary": "Vision-language models (VLMs) achieve remarkable success in single-image\ntasks. However, real-world scenarios often involve intricate multi-image\ninputs, leading to a notable performance decline as models struggle to\ndisentangle critical information scattered across complex visual features. In\nthis work, we propose Focus-Centric Visual Chain, a novel paradigm that\nenhances VLMs'perception, comprehension, and reasoning abilities in multi-image\nscenarios. To facilitate this paradigm, we propose Focus-Centric Data\nSynthesis, a scalable bottom-up approach for synthesizing high-quality data\nwith elaborate reasoning paths. Through this approach, We construct VISC-150K,\na large-scale dataset with reasoning data in the form of Focus-Centric Visual\nChain, specifically designed for multi-image tasks. Experimental results on\nseven multi-image benchmarks demonstrate that our method achieves average\nperformance gains of 3.16% and 2.24% across two distinct model architectures,\nwithout compromising the general vision-language capabilities. our study\nrepresents a significant step toward more robust and capable vision-language\nsystems that can handle complex visual scenarios.", "AI": {"tldr": "\u63d0\u51faFocus-Centric Visual Chain\uff0c\u63d0\u5347\u591a\u56fe\u50cf\u573a\u666f\u4e0b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u6027\u80fd\uff0c\u5e76\u901a\u8fc7\u5408\u6210\u6570\u636e\u96c6VISC-150K\u9a8c\u8bc1\u5176\u6709\u6548\u6027\u3002", "motivation": "\u73b0\u5b9e\u573a\u666f\u4e2d\u591a\u56fe\u50cf\u8f93\u5165\u590d\u6742\uff0c\u73b0\u6709\u6a21\u578b\u6027\u80fd\u4e0b\u964d\uff0c\u9700\u63d0\u5347\u6a21\u578b\u5728\u591a\u56fe\u50cf\u4efb\u52a1\u4e2d\u7684\u80fd\u529b\u3002", "method": "\u63d0\u51faFocus-Centric Visual Chain\u8303\u5f0f\u53caFocus-Centric Data Synthesis\u65b9\u6cd5\uff0c\u6784\u5efaVISC-150K\u6570\u636e\u96c6\u3002", "result": "\u5728\u4e03\u4e2a\u591a\u56fe\u50cf\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u5e73\u5747\u6027\u80fd\u63d0\u53473.16%\u548c2.24%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u5904\u7406\u590d\u6742\u89c6\u89c9\u573a\u666f\u7684\u89c6\u89c9\u8bed\u8a00\u7cfb\u7edf\u63d0\u4f9b\u4e86\u91cd\u8981\u8fdb\u5c55\u3002"}}
{"id": "2504.20294", "pdf": "https://arxiv.org/pdf/2504.20294", "abs": "https://arxiv.org/abs/2504.20294", "authors": ["William P. McCarthy", "Saujas Vaduguru", "Karl D. D. Willis", "Justin Matejka", "Judith E. Fan", "Daniel Fried", "Yewen Pu"], "title": "mrCAD: Multimodal Refinement of Computer-aided Designs", "categories": ["cs.AI", "cs.CL", "cs.HC"], "comment": "the first two authors contributed equally", "summary": "A key feature of human collaboration is the ability to iteratively refine the\nconcepts we have communicated. In contrast, while generative AI excels at the\n\\textit{generation} of content, it often struggles to make specific\nlanguage-guided \\textit{modifications} of its prior outputs. To bridge the gap\nbetween how humans and machines perform edits, we present mrCAD, a dataset of\nmultimodal instructions in a communication game. In each game, players created\ncomputer aided designs (CADs) and refined them over several rounds to match\nspecific target designs. Only one player, the Designer, could see the target,\nand they must instruct the other player, the Maker, using text, drawing, or a\ncombination of modalities. mrCAD consists of 6,082 communication games, 15,163\ninstruction-execution rounds, played between 1,092 pairs of human players. We\nanalyze the dataset and find that generation and refinement instructions differ\nin their composition of drawing and text. Using the mrCAD task as a benchmark,\nwe find that state-of-the-art VLMs are better at following generation\ninstructions than refinement instructions. These results lay a foundation for\nanalyzing and modeling a multimodal language of refinement that is not\nrepresented in previous datasets.", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86mrCAD\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u7814\u7a76\u4eba\u7c7b\u5982\u4f55\u901a\u8fc7\u591a\u6a21\u6001\u6307\u4ee4\uff08\u6587\u672c\u548c\u7ed8\u56fe\uff09\u534f\u4f5c\u7ec6\u5316\u8bbe\u8ba1\uff0c\u5e76\u53d1\u73b0\u751f\u6210\u5f0fAI\u5728\u7ec6\u5316\u6307\u4ee4\u4e0a\u7684\u8868\u73b0\u8f83\u5dee\u3002", "motivation": "\u4eba\u7c7b\u534f\u4f5c\u4e2d\u80fd\u591f\u8fed\u4ee3\u7ec6\u5316\u6982\u5ff5\uff0c\u800c\u751f\u6210\u5f0fAI\u5728\u751f\u6210\u5185\u5bb9\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5728\u7ec6\u5316\u4fee\u6539\u4e0a\u5b58\u5728\u4e0d\u8db3\uff0c\u56e0\u6b64\u9700\u8981\u7814\u7a76\u591a\u6a21\u6001\u6307\u4ee4\u7684\u7ec6\u5316\u8fc7\u7a0b\u3002", "method": "\u901a\u8fc7mrCAD\u6570\u636e\u96c6\uff0c\u8bb0\u5f55\u4e861,092\u5bf9\u4eba\u7c7b\u73a9\u5bb6\u57286,082\u573a\u901a\u4fe1\u6e38\u620f\u4e2d\u768415,163\u8f6e\u6307\u4ee4\u6267\u884c\uff0c\u5206\u6790\u4e86\u6587\u672c\u548c\u7ed8\u56fe\u5728\u591a\u6a21\u6001\u6307\u4ee4\u4e2d\u7684\u5dee\u5f02\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u751f\u6210\u5f0fAI\u5728\u9075\u5faa\u751f\u6210\u6307\u4ee4\u4e0a\u4f18\u4e8e\u7ec6\u5316\u6307\u4ee4\uff0c\u4e14\u7ec6\u5316\u6307\u4ee4\u7684\u7ec4\u6210\u4e0e\u751f\u6210\u6307\u4ee4\u4e0d\u540c\u3002", "conclusion": "mrCAD\u4e3a\u5206\u6790\u548c\u5efa\u6a21\u591a\u6a21\u6001\u7ec6\u5316\u8bed\u8a00\u63d0\u4f9b\u4e86\u57fa\u7840\uff0c\u586b\u8865\u4e86\u73b0\u6709\u6570\u636e\u96c6\u7684\u7a7a\u767d\u3002"}}
{"id": "2504.20203", "pdf": "https://arxiv.org/pdf/2504.20203", "abs": "https://arxiv.org/abs/2504.20203", "authors": ["Vladyslav Polushko", "Damjan Hatic", "Ronald R\u00f6sch", "Thomas M\u00e4rz", "Markus Rauhut", "Andreas Weinmann"], "title": "Remote Sensing Imagery for Flood Detection: Exploration of Augmentation Strategies", "categories": ["cs.CV", "eess.IV"], "comment": null, "summary": "Floods cause serious problems around the world. Responding quickly and\neffectively requires accurate and timely information about the affected areas.\nThe effective use of Remote Sensing images for accurate flood detection\nrequires specific detection methods. Typically, Deep Neural Networks are\nemployed, which are trained on specific datasets. For the purpose of river\nflood detection in RGB imagery, we use the BlessemFlood21 dataset. We here\nexplore the use of different augmentation strategies, ranging from basic\napproaches to more complex techniques, including optical distortion. By\nidentifying effective strategies, we aim to refine the training process of\nstate-of-the-art Deep Learning segmentation networks.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u5229\u7528\u4e0d\u540c\u6570\u636e\u589e\u5f3a\u7b56\u7565\u4f18\u5316\u6df1\u5ea6\u5b66\u4e60\u7f51\u7edc\u5728RGB\u56fe\u50cf\u4e2d\u68c0\u6d4b\u6cb3\u6d41\u6d2a\u6c34\u7684\u6548\u679c\u3002", "motivation": "\u6d2a\u6c34\u662f\u5168\u7403\u6027\u95ee\u9898\uff0c\u5feb\u901f\u6709\u6548\u54cd\u5e94\u9700\u8981\u51c6\u786e\u53ca\u65f6\u7684\u53d7\u707e\u533a\u57df\u4fe1\u606f\u3002\u9065\u611f\u56fe\u50cf\u7684\u6d2a\u6c34\u68c0\u6d4b\u9700\u8981\u7279\u5b9a\u65b9\u6cd5\uff0c\u800c\u6df1\u5ea6\u5b66\u4e60\u7f51\u7edc\u901a\u5e38\u4f9d\u8d56\u7279\u5b9a\u6570\u636e\u96c6\u8bad\u7ec3\u3002", "method": "\u4f7f\u7528BlessemFlood21\u6570\u636e\u96c6\uff0c\u7814\u7a76\u4ece\u57fa\u7840\u5230\u590d\u6742\uff08\u5982\u5149\u5b66\u7578\u53d8\uff09\u7684\u4e0d\u540c\u6570\u636e\u589e\u5f3a\u7b56\u7565\u3002", "result": "\u65e8\u5728\u901a\u8fc7\u8bc6\u522b\u6709\u6548\u7b56\u7565\u4f18\u5316\u6700\u5148\u8fdb\u6df1\u5ea6\u5b66\u4e60\u5206\u5272\u7f51\u7edc\u7684\u8bad\u7ec3\u8fc7\u7a0b\u3002", "conclusion": "\u901a\u8fc7\u6570\u636e\u589e\u5f3a\u7b56\u7565\u4f18\u5316\u8bad\u7ec3\uff0c\u53ef\u63d0\u5347\u6d2a\u6c34\u68c0\u6d4b\u7684\u51c6\u786e\u6027\u548c\u6548\u7387\u3002"}}
{"id": "2504.20318", "pdf": "https://arxiv.org/pdf/2504.20318", "abs": "https://arxiv.org/abs/2504.20318", "authors": ["Ryan Xiao Wang", "Felipe Trevizan"], "title": "Leveraging Action Relational Structures for Integrated Learning and Planning", "categories": ["cs.AI"], "comment": "Extended version of ICAPS 2025 paper", "summary": "Recent advances in planning have explored using learning methods to help\nplanning. However, little attention has been given to adapting search\nalgorithms to work better with learning systems. In this paper, we introduce\npartial-space search, a new search space for classical planning that leverages\nthe relational structure of actions given by PDDL action schemas -- a structure\noverlooked by traditional planning approaches. Partial-space search provides a\nmore granular view of the search space and allows earlier pruning of poor\nactions compared to state-space search. To guide partial-space search, we\nintroduce action set heuristics that evaluate sets of actions in a state. We\ndescribe how to automatically convert existing heuristics into action set\nheuristics. We also train action set heuristics from scratch using large\ntraining datasets from partial-space search. Our new planner, LazyLifted,\nexploits our better integrated search and learning heuristics and outperforms\nthe state-of-the-art ML-based heuristic on IPC 2023 learning track (LT)\nbenchmarks. We also show the efficiency of LazyLifted on high-branching factor\ntasks and show that it surpasses LAMA in the combined IPC 2023 LT and\nhigh-branching factor benchmarks.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u641c\u7d22\u7a7a\u95f4\u65b9\u6cd5\u2014\u2014\u90e8\u5206\u7a7a\u95f4\u641c\u7d22\uff0c\u7ed3\u5408\u5b66\u4e60\u7cfb\u7edf\u4f18\u5316\u89c4\u5212\u4efb\u52a1\uff0c\u5e76\u5728\u6027\u80fd\u4e0a\u8d85\u8d8a\u4e86\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edf\u89c4\u5212\u65b9\u6cd5\u5ffd\u89c6\u4e86PDDL\u52a8\u4f5c\u6a21\u5f0f\u7684\u5173\u7cfb\u7ed3\u6784\uff0c\u800c\u5b66\u4e60\u7cfb\u7edf\u4e0e\u641c\u7d22\u7b97\u6cd5\u7684\u7ed3\u5408\u5c1a\u672a\u5f97\u5230\u5145\u5206\u7814\u7a76\u3002", "method": "\u5f15\u5165\u90e8\u5206\u7a7a\u95f4\u641c\u7d22\u548c\u52a8\u4f5c\u96c6\u542f\u53d1\u5f0f\uff0c\u5229\u7528PDDL\u52a8\u4f5c\u6a21\u5f0f\u7684\u5173\u7cfb\u7ed3\u6784\uff0c\u5e76\u901a\u8fc7\u8bad\u7ec3\u6570\u636e\u96c6\u81ea\u52a8\u751f\u6210\u542f\u53d1\u5f0f\u3002", "result": "\u65b0\u89c4\u5212\u5668LazyLifted\u5728IPC 2023\u5b66\u4e60\u8d5b\u9053\u548c\u9ad8\u5206\u652f\u56e0\u5b50\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u90e8\u5206\u7a7a\u95f4\u641c\u7d22\u4e0e\u5b66\u4e60\u542f\u53d1\u5f0f\u7684\u7ed3\u5408\u663e\u8457\u63d0\u5347\u4e86\u89c4\u5212\u4efb\u52a1\u7684\u6548\u7387\u548c\u6027\u80fd\u3002"}}
{"id": "2504.20222", "pdf": "https://arxiv.org/pdf/2504.20222", "abs": "https://arxiv.org/abs/2504.20222", "authors": ["Naoko Sawada", "Pedro Miraldo", "Suhas Lohit", "Tim K. Marks", "Moitreya Chatterjee"], "title": "FreBIS: Frequency-Based Stratification for Neural Implicit Surface Representations", "categories": ["cs.CV"], "comment": "Accepted to CVPR 2025 CV4Metaverse Workshop", "summary": "Neural implicit surface representation techniques are in high demand for\nadvancing technologies in augmented reality/virtual reality, digital twins,\nautonomous navigation, and many other fields. With their ability to model\nobject surfaces in a scene as a continuous function, such techniques have made\nremarkable strides recently, especially over classical 3D surface\nreconstruction methods, such as those that use voxels or point clouds. However,\nthese methods struggle with scenes that have varied and complex surfaces\nprincipally because they model any given scene with a single encoder network\nthat is tasked to capture all of low through high-surface frequency information\nin the scene simultaneously. In this work, we propose a novel, neural implicit\nsurface representation approach called FreBIS to overcome this challenge.\nFreBIS works by stratifying the scene based on the frequency of surfaces into\nmultiple frequency levels, with each level (or a group of levels) encoded by a\ndedicated encoder. Moreover, FreBIS encourages these encoders to capture\ncomplementary information by promoting mutual dissimilarity of the encoded\nfeatures via a novel, redundancy-aware weighting module. Empirical evaluations\non the challenging BlendedMVS dataset indicate that replacing the standard\nencoder in an off-the-shelf neural surface reconstruction method with our\nfrequency-stratified encoders yields significant improvements. These\nenhancements are evident both in the quality of the reconstructed 3D surfaces\nand in the fidelity of their renderings from any viewpoint.", "AI": {"tldr": "FreBIS\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u795e\u7ecf\u9690\u5f0f\u8868\u9762\u8868\u793a\u65b9\u6cd5\uff0c\u901a\u8fc7\u5206\u5c42\u7f16\u7801\u4e0d\u540c\u9891\u7387\u7684\u8868\u9762\u4fe1\u606f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u590d\u6742\u573a\u666f\u76843D\u91cd\u5efa\u8d28\u91cf\u3002", "motivation": "\u4f20\u7edf\u795e\u7ecf\u9690\u5f0f\u8868\u9762\u8868\u793a\u65b9\u6cd5\u4f7f\u7528\u5355\u4e00\u7f16\u7801\u7f51\u7edc\u5904\u7406\u6240\u6709\u8868\u9762\u9891\u7387\u4fe1\u606f\uff0c\u96be\u4ee5\u5e94\u5bf9\u590d\u6742\u573a\u666f\u3002FreBIS\u65e8\u5728\u901a\u8fc7\u5206\u5c42\u7f16\u7801\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "FreBIS\u5c06\u573a\u666f\u6309\u8868\u9762\u9891\u7387\u5206\u5c42\uff0c\u6bcf\u5c42\u7531\u4e13\u7528\u7f16\u7801\u5668\u5904\u7406\uff0c\u5e76\u901a\u8fc7\u5197\u4f59\u611f\u77e5\u6743\u91cd\u6a21\u5757\u4fc3\u8fdb\u7279\u5f81\u4e92\u8865\u6027\u3002", "result": "\u5728BlendedMVS\u6570\u636e\u96c6\u4e0a\uff0cFreBIS\u663e\u8457\u63d0\u5347\u4e863D\u8868\u9762\u91cd\u5efa\u8d28\u91cf\u548c\u6e32\u67d3\u4fdd\u771f\u5ea6\u3002", "conclusion": "FreBIS\u901a\u8fc7\u5206\u5c42\u7f16\u7801\u548c\u7279\u5f81\u4e92\u8865\u6027\u8bbe\u8ba1\uff0c\u6709\u6548\u63d0\u5347\u4e86\u590d\u6742\u573a\u666f\u7684\u795e\u7ecf\u9690\u5f0f\u8868\u9762\u8868\u793a\u80fd\u529b\u3002"}}
{"id": "2504.20340", "pdf": "https://arxiv.org/pdf/2504.20340", "abs": "https://arxiv.org/abs/2504.20340", "authors": ["Khoi Trinh", "Scott Seidenberger", "Raveen Wijewickrama", "Murtuza Jadliwala", "Anindya Maiti"], "title": "A Picture is Worth a Thousand Prompts? Efficacy of Iterative Human-Driven Prompt Refinement in Image Regeneration Tasks", "categories": ["cs.AI", "cs.CV", "cs.HC"], "comment": null, "summary": "With AI-generated content becoming ubiquitous across the web, social media,\nand other digital platforms, it is vital to examine how such content are\ninspired and generated. The creation of AI-generated images often involves\nrefining the input prompt iteratively to achieve desired visual outcomes. This\nstudy focuses on the relatively underexplored concept of image regeneration\nusing AI, in which a human operator attempts to closely recreate a specific\ntarget image by iteratively refining their prompt. Image regeneration is\ndistinct from normal image generation, which lacks any predefined visual\nreference. A separate challenge lies in determining whether existing image\nsimilarity metrics (ISMs) can provide reliable, objective feedback in iterative\nworkflows, given that we do not fully understand if subjective human judgments\nof similarity align with these metrics. Consequently, we must first validate\ntheir alignment with human perception before assessing their potential as a\nfeedback mechanism in the iterative prompt refinement process. To address these\nresearch gaps, we present a structured user study evaluating how iterative\nprompt refinement affects the similarity of regenerated images relative to\ntheir targets, while also examining whether ISMs capture the same improvements\nperceived by human observers. Our findings suggest that incremental prompt\nadjustments substantially improve alignment, verified through both subjective\nevaluations and quantitative measures, underscoring the broader potential of\niterative workflows to enhance generative AI content creation across various\napplication domains.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86AI\u56fe\u50cf\u518d\u751f\u4e2d\u901a\u8fc7\u8fed\u4ee3\u63d0\u793a\u4f18\u5316\u5b9e\u73b0\u76ee\u6807\u56fe\u50cf\u91cd\u73b0\u7684\u6548\u679c\uff0c\u5e76\u9a8c\u8bc1\u4e86\u56fe\u50cf\u76f8\u4f3c\u6027\u6307\u6807\u4e0e\u4eba\u7c7b\u611f\u77e5\u7684\u4e00\u81f4\u6027\u3002", "motivation": "\u968f\u7740AI\u751f\u6210\u5185\u5bb9\u666e\u53ca\uff0c\u7814\u7a76\u5982\u4f55\u901a\u8fc7\u8fed\u4ee3\u63d0\u793a\u4f18\u5316\u5b9e\u73b0\u7279\u5b9a\u76ee\u6807\u56fe\u50cf\u7684\u518d\u751f\uff0c\u5e76\u9a8c\u8bc1\u73b0\u6709\u56fe\u50cf\u76f8\u4f3c\u6027\u6307\u6807\u662f\u5426\u53ef\u9760\u3002", "method": "\u901a\u8fc7\u7ed3\u6784\u5316\u7528\u6237\u7814\u7a76\uff0c\u8bc4\u4f30\u8fed\u4ee3\u63d0\u793a\u4f18\u5316\u5bf9\u56fe\u50cf\u76f8\u4f3c\u6027\u7684\u5f71\u54cd\uff0c\u5e76\u6bd4\u8f83\u56fe\u50cf\u76f8\u4f3c\u6027\u6307\u6807\u4e0e\u4eba\u7c7b\u611f\u77e5\u7684\u4e00\u81f4\u6027\u3002", "result": "\u8fed\u4ee3\u63d0\u793a\u4f18\u5316\u663e\u8457\u63d0\u5347\u4e86\u56fe\u50cf\u5bf9\u9f50\u6548\u679c\uff0c\u4e3b\u89c2\u8bc4\u4f30\u4e0e\u5b9a\u91cf\u6307\u6807\u5747\u9a8c\u8bc1\u4e86\u8fd9\u4e00\u70b9\u3002", "conclusion": "\u8fed\u4ee3\u5de5\u4f5c\u6d41\u7a0b\u5728\u751f\u6210AI\u5185\u5bb9\u4e2d\u5177\u6709\u5e7f\u6cdb\u6f5c\u529b\uff0c\u56fe\u50cf\u76f8\u4f3c\u6027\u6307\u6807\u53ef\u4f5c\u4e3a\u6709\u6548\u53cd\u9988\u673a\u5236\u3002"}}
{"id": "2504.20234", "pdf": "https://arxiv.org/pdf/2504.20234", "abs": "https://arxiv.org/abs/2504.20234", "authors": ["Bartosz Ptak", "Marek Kraft"], "title": "Improving trajectory continuity in drone-based crowd monitoring using a set of minimal-cost techniques and deep discriminative correlation filters", "categories": ["cs.CV", "cs.RO"], "comment": "Preprint submitted to the Expert Systems with Applications journal", "summary": "Drone-based crowd monitoring is the key technology for applications in\nsurveillance, public safety, and event management. However, maintaining\ntracking continuity and consistency remains a significant challenge.\nTraditional detection-assignment tracking methods struggle with false\npositives, false negatives, and frequent identity switches, leading to degraded\ncounting accuracy and making in-depth analysis impossible. This paper\nintroduces a point-oriented online tracking algorithm that improves trajectory\ncontinuity and counting reliability in drone-based crowd monitoring. Our method\nbuilds on the Simple Online and Real-time Tracking (SORT) framework, replacing\nthe original bounding-box assignment with a point-distance metric. The\nalgorithm is enhanced with three cost-effective techniques: camera motion\ncompensation, altitude-aware assignment, and classification-based trajectory\nvalidation. Further, Deep Discriminative Correlation Filters (DDCF) that re-use\nspatial feature maps from localisation algorithms for increased computational\nefficiency through neural network resource sharing are integrated to refine\nobject tracking by reducing noise and handling missed detections. The proposed\nmethod is evaluated on the DroneCrowd and newly shared UP-COUNT-TRACK datasets,\ndemonstrating substantial improvements in tracking metrics, reducing counting\nerrors to 23% and 15%, respectively. The results also indicate a significant\nreduction of identity switches while maintaining high tracking accuracy,\noutperforming baseline online trackers and even an offline greedy optimisation\nmethod.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u70b9\u8ddd\u79bb\u5ea6\u91cf\u7684\u65e0\u4eba\u673a\u4eba\u7fa4\u76d1\u63a7\u5728\u7ebf\u8ddf\u8e2a\u7b97\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8f68\u8ff9\u8fde\u7eed\u6027\u548c\u8ba1\u6570\u53ef\u9760\u6027\u3002", "motivation": "\u65e0\u4eba\u673a\u4eba\u7fa4\u76d1\u63a7\u5728\u516c\u5171\u5b89\u5168\u7b49\u9886\u57df\u6709\u91cd\u8981\u5e94\u7528\uff0c\u4f46\u4f20\u7edf\u65b9\u6cd5\u5b58\u5728\u8bef\u68c0\u3001\u6f0f\u68c0\u548c\u8eab\u4efd\u5207\u6362\u95ee\u9898\uff0c\u5bfc\u81f4\u8ba1\u6570\u4e0d\u51c6\u548c\u5206\u6790\u56f0\u96be\u3002", "method": "\u5728SORT\u6846\u67b6\u57fa\u7840\u4e0a\uff0c\u7528\u70b9\u8ddd\u79bb\u5ea6\u91cf\u66ff\u4ee3\u8fb9\u754c\u6846\u5206\u914d\uff0c\u7ed3\u5408\u76f8\u673a\u8fd0\u52a8\u8865\u507f\u3001\u9ad8\u5ea6\u611f\u77e5\u5206\u914d\u548c\u5206\u7c7b\u8f68\u8ff9\u9a8c\u8bc1\uff0c\u5e76\u96c6\u6210DDCF\u63d0\u9ad8\u8ba1\u7b97\u6548\u7387\u3002", "result": "\u5728DroneCrowd\u548cUP-COUNT-TRACK\u6570\u636e\u96c6\u4e0a\uff0c\u8ba1\u6570\u8bef\u5dee\u5206\u522b\u964d\u81f323%\u548c15%\uff0c\u8eab\u4efd\u5207\u6362\u663e\u8457\u51cf\u5c11\uff0c\u4f18\u4e8e\u57fa\u7ebf\u5728\u7ebf\u8ddf\u8e2a\u5668\u548c\u79bb\u7ebf\u8d2a\u5a6a\u4f18\u5316\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u65e0\u4eba\u673a\u4eba\u7fa4\u76d1\u63a7\u4e2d\u7684\u8ddf\u8e2a\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u8ba1\u6570\u51c6\u786e\u6027\u548c\u8f68\u8ff9\u8fde\u7eed\u6027\u3002"}}
{"id": "2504.20406", "pdf": "https://arxiv.org/pdf/2504.20406", "abs": "https://arxiv.org/abs/2504.20406", "authors": ["Paiheng Xu", "Gang Wu", "Xiang Chen", "Tong Yu", "Chang Xiao", "Franck Dernoncourt", "Tianyi Zhou", "Wei Ai", "Viswanathan Swaminathan"], "title": "Skill Discovery for Software Scripting Automation via Offline Simulations with LLMs", "categories": ["cs.AI", "cs.SE"], "comment": null, "summary": "Scripting interfaces enable users to automate tasks and customize software\nworkflows, but creating scripts traditionally requires programming expertise\nand familiarity with specific APIs, posing barriers for many users. While Large\nLanguage Models (LLMs) can generate code from natural language queries, runtime\ncode generation is severely limited due to unverified code, security risks,\nlonger response times, and higher computational costs. To bridge the gap, we\npropose an offline simulation framework to curate a software-specific skillset,\na collection of verified scripts, by exploiting LLMs and publicly available\nscripting guides. Our framework comprises two components: (1) task creation,\nusing top-down functionality guidance and bottom-up API synergy exploration to\ngenerate helpful tasks; and (2) skill generation with trials, refining and\nvalidating scripts based on execution feedback. To efficiently navigate the\nextensive API landscape, we introduce a Graph Neural Network (GNN)-based link\nprediction model to capture API synergy, enabling the generation of skills\ninvolving underutilized APIs and expanding the skillset's diversity.\nExperiments with Adobe Illustrator demonstrate that our framework significantly\nimproves automation success rates, reduces response time, and saves runtime\ntoken costs compared to traditional runtime code generation. This is the first\nattempt to use software scripting interfaces as a testbed for LLM-based\nsystems, highlighting the advantages of leveraging execution feedback in a\ncontrolled environment and offering valuable insights into aligning AI\ncapabilities with user needs in specialized software domains.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u79bb\u7ebf\u6a21\u62df\u6846\u67b6\uff0c\u901a\u8fc7\u5229\u7528LLMs\u548c\u516c\u5f00\u811a\u672c\u6307\u5357\uff0c\u751f\u6210\u5df2\u9a8c\u8bc1\u7684\u811a\u672c\u96c6\u5408\uff0c\u4ee5\u63d0\u9ad8\u81ea\u52a8\u5316\u6210\u529f\u7387\u548c\u6548\u7387\u3002", "motivation": "\u4f20\u7edf\u811a\u672c\u7f16\u5199\u9700\u8981\u7f16\u7a0b\u77e5\u8bc6\uff0c\u800c\u8fd0\u884c\u65f6\u4ee3\u7801\u751f\u6210\u5b58\u5728\u5b89\u5168\u98ce\u9669\u548c\u6548\u7387\u95ee\u9898\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u5b89\u5168\u9ad8\u6548\u7684\u65b9\u6cd5\u3002", "method": "\u6846\u67b6\u5305\u62ec\u4efb\u52a1\u521b\u5efa\uff08\u529f\u80fd\u6307\u5bfc\u548cAPI\u534f\u540c\u63a2\u7d22\uff09\u548c\u6280\u80fd\u751f\u6210\uff08\u901a\u8fc7\u6267\u884c\u53cd\u9988\u9a8c\u8bc1\u811a\u672c\uff09\uff0c\u5e76\u4f7f\u7528GNN\u6a21\u578b\u9884\u6d4bAPI\u534f\u540c\u3002", "result": "\u5728Adobe Illustrator\u5b9e\u9a8c\u4e2d\uff0c\u6846\u67b6\u663e\u8457\u63d0\u9ad8\u4e86\u81ea\u52a8\u5316\u6210\u529f\u7387\uff0c\u51cf\u5c11\u4e86\u54cd\u5e94\u65f6\u95f4\u548c\u8fd0\u884c\u65f6\u6210\u672c\u3002", "conclusion": "\u8be5\u6846\u67b6\u9996\u6b21\u5c06\u8f6f\u4ef6\u811a\u672c\u63a5\u53e3\u4f5c\u4e3aLLM\u7cfb\u7edf\u7684\u6d4b\u8bd5\u5e73\u53f0\uff0c\u5c55\u793a\u4e86\u5728\u53d7\u63a7\u73af\u5883\u4e2d\u5229\u7528\u6267\u884c\u53cd\u9988\u7684\u4f18\u52bf\u3002"}}
{"id": "2504.20241", "pdf": "https://arxiv.org/pdf/2504.20241", "abs": "https://arxiv.org/abs/2504.20241", "authors": ["Kamirul Kamirul", "Odysseas Pappas", "Alin Achim"], "title": "Physics-Informed Diffusion Models for SAR Ship Wake Generation from Text Prompts", "categories": ["cs.CV"], "comment": "4 pages; Submitted Machine Intelligence for GeoAnalytics and Remote\n  Sensing (MIGARS) - 2025", "summary": "Detecting ship presence via wake signatures in SAR imagery is attracting\nconsiderable research interest, but limited annotated data availability poses\nsignificant challenges for supervised learning. Physics-based simulations are\ncommonly used to address this data scarcity, although they are slow and\nconstrain end-to-end learning. In this work, we explore a new direction for\nmore efficient and end-to-end SAR ship wake simulation using a diffusion model\ntrained on data generated by a physics-based simulator. The training dataset is\nbuilt by pairing images produced by the simulator with text prompts derived\nfrom simulation parameters. Experimental result show that the model generates\nrealistic Kelvin wake patterns and achieves significantly faster inference than\nthe physics-based simulator. These results highlight the potential of diffusion\nmodels for fast and controllable wake image generation, opening new\npossibilities for end-to-end downstream tasks in maritime SAR analysis.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u9ad8\u6548\u751f\u6210SAR\u56fe\u50cf\u4e2d\u7684\u8239\u8236\u5c3e\u8ff9\uff0c\u89e3\u51b3\u4e86\u7269\u7406\u6a21\u62df\u901f\u5ea6\u6162\u7684\u95ee\u9898\u3002", "motivation": "\u7531\u4e8e\u6807\u6ce8\u6570\u636e\u7a00\u7f3a\uff0c\u76d1\u7763\u5b66\u4e60\u5728SAR\u56fe\u50cf\u4e2d\u68c0\u6d4b\u8239\u8236\u5c3e\u8ff9\u9762\u4e34\u6311\u6218\uff0c\u800c\u7269\u7406\u6a21\u62df\u901f\u5ea6\u6162\u4e14\u9650\u5236\u4e86\u7aef\u5230\u7aef\u5b66\u4e60\u3002", "method": "\u4f7f\u7528\u6269\u6563\u6a21\u578b\uff0c\u901a\u8fc7\u7269\u7406\u6a21\u62df\u751f\u6210\u7684\u6570\u636e\u8bad\u7ec3\uff0c\u5c06\u6a21\u62df\u56fe\u50cf\u4e0e\u57fa\u4e8e\u53c2\u6570\u7684\u6587\u672c\u63d0\u793a\u914d\u5bf9\u3002", "result": "\u6a21\u578b\u80fd\u751f\u6210\u903c\u771f\u7684\u5f00\u5c14\u6587\u5c3e\u8ff9\uff0c\u4e14\u63a8\u7406\u901f\u5ea6\u663e\u8457\u5feb\u4e8e\u7269\u7406\u6a21\u62df\u5668\u3002", "conclusion": "\u6269\u6563\u6a21\u578b\u5728\u5feb\u901f\u53ef\u63a7\u7684\u5c3e\u8ff9\u56fe\u50cf\u751f\u6210\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u4e3a\u6d77\u4e0aSAR\u5206\u6790\u7684\u7aef\u5230\u7aef\u4efb\u52a1\u63d0\u4f9b\u4e86\u65b0\u53ef\u80fd\u3002"}}
{"id": "2504.20426", "pdf": "https://arxiv.org/pdf/2504.20426", "abs": "https://arxiv.org/abs/2504.20426", "authors": ["Jiapeng Wang", "Jinhao Jiang", "Zhiqiang Zhang", "Jun Zhou", "Wayne Xin Zhao"], "title": "RV-Syn: Rational and Verifiable Mathematical Reasoning Data Synthesis based on Structured Function Library", "categories": ["cs.AI"], "comment": null, "summary": "The advancement of reasoning capabilities in Large Language Models (LLMs)\nrequires substantial amounts of high-quality reasoning data, particularly in\nmathematics. Existing data synthesis methods, such as data augmentation from\nannotated training sets or direct question generation based on relevant\nknowledge points and documents, have expanded datasets but face challenges in\nmastering the inner logic of the problem during generation and ensuring the\nverifiability of the solutions. To address these issues, we propose RV-Syn, a\nnovel Rational and Verifiable mathematical Synthesis approach. RV-Syn\nconstructs a structured mathematical operation function library based on\ninitial seed problems and generates computational graphs as solutions by\ncombining Python-formatted functions from this library. These graphs are then\nback-translated into complex problems. Based on the constructed computation\ngraph, we achieve solution-guided logic-aware problem generation. Furthermore,\nthe executability of the computational graph ensures the verifiability of the\nsolving process. Experimental results show that RV-Syn surpasses existing\nsynthesis methods, including those involving human-generated problems,\nachieving greater efficient data scaling. This approach provides a scalable\nframework for generating high-quality reasoning datasets.", "AI": {"tldr": "RV-Syn\u662f\u4e00\u79cd\u65b0\u9896\u7684\u6570\u5b66\u6570\u636e\u5408\u6210\u65b9\u6cd5\uff0c\u901a\u8fc7\u6784\u5efa\u7ed3\u6784\u5316\u6570\u5b66\u64cd\u4f5c\u51fd\u6570\u5e93\u548c\u8ba1\u7b97\u56fe\uff0c\u751f\u6210\u53ef\u9a8c\u8bc1\u7684\u9ad8\u8d28\u91cf\u63a8\u7406\u6570\u636e\u3002", "motivation": "\u73b0\u6709\u6570\u636e\u5408\u6210\u65b9\u6cd5\u5728\u638c\u63e1\u95ee\u9898\u5185\u5728\u903b\u8f91\u548c\u786e\u4fdd\u89e3\u51b3\u65b9\u6848\u53ef\u9a8c\u8bc1\u6027\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u65b9\u6cd5\u751f\u6210\u6570\u5b66\u63a8\u7406\u6570\u636e\u3002", "method": "RV-Syn\u57fa\u4e8e\u521d\u59cb\u79cd\u5b50\u95ee\u9898\u6784\u5efa\u6570\u5b66\u64cd\u4f5c\u51fd\u6570\u5e93\uff0c\u751f\u6210Python\u683c\u5f0f\u7684\u8ba1\u7b97\u56fe\u4f5c\u4e3a\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u5c06\u5176\u53cd\u5411\u7ffb\u8bd1\u4e3a\u590d\u6742\u95ee\u9898\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cRV-Syn\u5728\u6570\u636e\u6269\u5c55\u6548\u7387\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5305\u62ec\u4eba\u5de5\u751f\u6210\u95ee\u9898\u7684\u65b9\u6cd5\u3002", "conclusion": "RV-Syn\u4e3a\u751f\u6210\u9ad8\u8d28\u91cf\u63a8\u7406\u6570\u636e\u96c6\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u6846\u67b6\u3002"}}
{"id": "2504.20288", "pdf": "https://arxiv.org/pdf/2504.20288", "abs": "https://arxiv.org/abs/2504.20288", "authors": ["Shinnosuke Saito", "Takashi Matsubara"], "title": "Image Interpolation with Score-based Riemannian Metrics of Diffusion Models", "categories": ["cs.CV"], "comment": null, "summary": "Diffusion models excel in content generation by implicitly learning the data\nmanifold, yet they lack a practical method to leverage this manifold - unlike\nother deep generative models equipped with latent spaces. This paper introduces\na novel framework that treats the data space of pre-trained diffusion models as\na Riemannian manifold, with a metric derived from the score function.\nExperiments with MNIST and Stable Diffusion show that this geometry-aware\napproach yields image interpolations that are more realistic, less noisy, and\nmore faithful to prompts than existing methods, demonstrating its potential for\nimproved content generation and editing.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u6846\u67b6\uff0c\u5c06\u9884\u8bad\u7ec3\u6269\u6563\u6a21\u578b\u7684\u6570\u636e\u7a7a\u95f4\u89c6\u4e3a\u9ece\u66fc\u6d41\u5f62\uff0c\u5229\u7528\u8bc4\u5206\u51fd\u6570\u5bfc\u51fa\u7684\u5ea6\u91cf\uff0c\u751f\u6210\u66f4\u771f\u5b9e\u3001\u566a\u58f0\u66f4\u5c11\u4e14\u66f4\u7b26\u5408\u63d0\u793a\u7684\u56fe\u50cf\u63d2\u503c\u3002", "motivation": "\u6269\u6563\u6a21\u578b\u5728\u5185\u5bb9\u751f\u6210\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u7f3a\u4e4f\u5229\u7528\u6570\u636e\u6d41\u5f62\u7684\u5b9e\u7528\u65b9\u6cd5\uff0c\u800c\u5176\u4ed6\u6df1\u5ea6\u751f\u6210\u6a21\u578b\u5177\u6709\u6f5c\u5728\u7a7a\u95f4\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u5c06\u9884\u8bad\u7ec3\u6269\u6563\u6a21\u578b\u7684\u6570\u636e\u7a7a\u95f4\u89c6\u4e3a\u9ece\u66fc\u6d41\u5f62\uff0c\u5e76\u57fa\u4e8e\u8bc4\u5206\u51fd\u6570\u5b9a\u4e49\u5ea6\u91cf\uff0c\u7528\u4e8e\u56fe\u50cf\u63d2\u503c\u3002", "result": "\u5728MNIST\u548cStable Diffusion\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u751f\u6210\u7684\u56fe\u50cf\u63d2\u503c\u66f4\u771f\u5b9e\u3001\u566a\u58f0\u66f4\u5c11\u4e14\u66f4\u7b26\u5408\u63d0\u793a\u3002", "conclusion": "\u8be5\u6846\u67b6\u5c55\u793a\u4e86\u5728\u5185\u5bb9\u751f\u6210\u548c\u7f16\u8f91\u4e2d\u7684\u6f5c\u529b\uff0c\u4e3a\u6269\u6563\u6a21\u578b\u7684\u6570\u636e\u6d41\u5f62\u5229\u7528\u63d0\u4f9b\u4e86\u5b9e\u7528\u65b9\u6cd5\u3002"}}
{"id": "2504.20445", "pdf": "https://arxiv.org/pdf/2504.20445", "abs": "https://arxiv.org/abs/2504.20445", "authors": ["Tianqing Zhang", "Zixin Zhu", "Kairong Yu", "Hongwei Wang"], "title": "Head-Tail-Aware KL Divergence in Knowledge Distillation for Spiking Neural Networks", "categories": ["cs.AI"], "comment": "Accepted by IJCNN2025", "summary": "Spiking Neural Networks (SNNs) have emerged as a promising approach for\nenergy-efficient and biologically plausible computation. However, due to\nlimitations in existing training methods and inherent model constraints, SNNs\noften exhibit a performance gap when compared to Artificial Neural Networks\n(ANNs). Knowledge distillation (KD) has been explored as a technique to\ntransfer knowledge from ANN teacher models to SNN student models to mitigate\nthis gap. Traditional KD methods typically use Kullback-Leibler (KL) divergence\nto align output distributions. However, conventional KL-based approaches fail\nto fully exploit the unique characteristics of SNNs, as they tend to\noveremphasize high-probability predictions while neglecting low-probability\nones, leading to suboptimal generalization. To address this, we propose\nHead-Tail Aware Kullback-Leibler (HTA-KL) divergence, a novel KD method for\nSNNs. HTA-KL introduces a cumulative probability-based mask to dynamically\ndistinguish between high- and low-probability regions. It assigns adaptive\nweights to ensure balanced knowledge transfer, enhancing the overall\nperformance. By integrating forward KL (FKL) and reverse KL (RKL) divergence,\nour method effectively align both head and tail regions of the distribution. We\nevaluate our methods on CIFAR-10, CIFAR-100 and Tiny ImageNet datasets. Our\nmethod outperforms existing methods on most datasets with fewer timesteps.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aHTA-KL\u7684\u65b0\u77e5\u8bc6\u84b8\u998f\u65b9\u6cd5\uff0c\u7528\u4e8e\u63d0\u5347\u8109\u51b2\u795e\u7ecf\u7f51\u7edc\uff08SNNs\uff09\u7684\u6027\u80fd\uff0c\u901a\u8fc7\u52a8\u6001\u533a\u5206\u9ad8\u4f4e\u6982\u7387\u533a\u57df\u5e76\u5e73\u8861\u77e5\u8bc6\u8f6c\u79fb\u3002", "motivation": "\u7531\u4e8e\u73b0\u6709\u8bad\u7ec3\u65b9\u6cd5\u548c\u6a21\u578b\u9650\u5236\uff0cSNNs\u7684\u6027\u80fd\u901a\u5e38\u4e0d\u5982\u4eba\u5de5\u795e\u7ecf\u7f51\u7edc\uff08ANNs\uff09\uff0c\u4f20\u7edfKL\u6563\u5ea6\u65b9\u6cd5\u672a\u80fd\u5145\u5206\u5229\u7528SNNs\u7279\u6027\uff0c\u5bfc\u81f4\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\u3002", "method": "\u63d0\u51faHTA-KL\u6563\u5ea6\uff0c\u5f15\u5165\u7d2f\u79ef\u6982\u7387\u63a9\u7801\u52a8\u6001\u533a\u5206\u9ad8\u4f4e\u6982\u7387\u533a\u57df\uff0c\u7ed3\u5408\u524d\u5411\u548c\u53cd\u5411KL\u6563\u5ea6\u4ee5\u5e73\u8861\u77e5\u8bc6\u8f6c\u79fb\u3002", "result": "\u5728CIFAR-10\u3001CIFAR-100\u548cTiny ImageNet\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u4e14\u6240\u9700\u65f6\u95f4\u6b65\u66f4\u5c11\u3002", "conclusion": "HTA-KL\u65b9\u6cd5\u6709\u6548\u63d0\u5347\u4e86SNNs\u7684\u6027\u80fd\uff0c\u901a\u8fc7\u66f4\u5e73\u8861\u7684\u77e5\u8bc6\u8f6c\u79fb\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u7684\u4e0d\u8db3\u3002"}}
{"id": "2504.20303", "pdf": "https://arxiv.org/pdf/2504.20303", "abs": "https://arxiv.org/abs/2504.20303", "authors": ["Junlin Guo", "James R. Zimmer-Dauphinee", "Jordan M. Nieusma", "Siqi Lu", "Quan Liu", "Ruining Deng", "Can Cui", "Jialin Yue", "Yizhe Lin", "Tianyuan Yao", "Juming Xiong", "Junchao Zhu", "Chongyu Qu", "Yuechen Yang", "Mitchell Wilkes", "Xiao Wang", "Parker VanValkenburgh", "Steven A. Wernke", "Yuankai Huo"], "title": "DeepAndes: A Self-Supervised Vision Foundation Model for Multi-Spectral Remote Sensing Imagery of the Andes", "categories": ["cs.CV"], "comment": null, "summary": "By mapping sites at large scales using remotely sensed data, archaeologists\ncan generate unique insights into long-term demographic trends, inter-regional\nsocial networks, and past adaptations to climate change. Remote sensing surveys\ncomplement field-based approaches, and their reach can be especially great when\ncombined with deep learning and computer vision techniques. However,\nconventional supervised deep learning methods face challenges in annotating\nfine-grained archaeological features at scale. While recent vision foundation\nmodels have shown remarkable success in learning large-scale remote sensing\ndata with minimal annotations, most off-the-shelf solutions are designed for\nRGB images rather than multi-spectral satellite imagery, such as the 8-band\ndata used in our study. In this paper, we introduce DeepAndes, a\ntransformer-based vision foundation model trained on three million\nmulti-spectral satellite images, specifically tailored for Andean archaeology.\nDeepAndes incorporates a customized DINOv2 self-supervised learning algorithm\noptimized for 8-band multi-spectral imagery, marking the first foundation model\ndesigned explicitly for the Andes region. We evaluate its image understanding\nperformance through imbalanced image classification, image instance retrieval,\nand pixel-level semantic segmentation tasks. Our experiments show that\nDeepAndes achieves superior F1 scores, mean average precision, and Dice scores\nin few-shot learning scenarios, significantly outperforming models trained from\nscratch or pre-trained on smaller datasets. This underscores the effectiveness\nof large-scale self-supervised pre-training in archaeological remote sensing.\nCodes will be available on https://github.com/geopacha/DeepAndes.", "AI": {"tldr": "DeepAndes\u662f\u4e00\u4e2a\u57fa\u4e8eTransformer\u7684\u89c6\u89c9\u57fa\u7840\u6a21\u578b\uff0c\u4e13\u4e3a\u5b89\u7b2c\u65af\u8003\u53e4\u8bbe\u8ba1\uff0c\u901a\u8fc7\u81ea\u76d1\u7763\u5b66\u4e60\u4f18\u5316\u591a\u5149\u8c31\u536b\u661f\u56fe\u50cf\u5206\u6790\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8003\u53e4\u9065\u611f\u4efb\u52a1\u7684\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u76d1\u7763\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u5728\u6807\u6ce8\u7ec6\u7c92\u5ea6\u8003\u53e4\u7279\u5f81\u65f6\u9762\u4e34\u6311\u6218\uff0c\u4e14\u73b0\u6709\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u591a\u9488\u5bf9RGB\u56fe\u50cf\u800c\u975e\u591a\u5149\u8c31\u6570\u636e\u3002DeepAndes\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u91c7\u7528\u5b9a\u5236\u7684DINOv2\u81ea\u76d1\u7763\u5b66\u4e60\u7b97\u6cd5\uff0c\u8bad\u7ec3\u4e8e300\u4e07\u5f20\u591a\u5149\u8c31\u536b\u661f\u56fe\u50cf\uff0c\u9488\u5bf98\u6ce2\u6bb5\u6570\u636e\u4f18\u5316\u3002", "result": "\u5728\u5c11\u6837\u672c\u5b66\u4e60\u573a\u666f\u4e0b\uff0cDeepAndes\u5728\u5206\u7c7b\u3001\u68c0\u7d22\u548c\u8bed\u4e49\u5206\u5272\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u663e\u8457\u4f18\u4e8e\u4ece\u5934\u8bad\u7ec3\u6216\u5c0f\u6570\u636e\u96c6\u9884\u8bad\u7ec3\u7684\u6a21\u578b\u3002", "conclusion": "\u5927\u89c4\u6a21\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\u5728\u8003\u53e4\u9065\u611f\u4e2d\u5177\u6709\u663e\u8457\u6548\u679c\uff0cDeepAndes\u4e3a\u5b89\u7b2c\u65af\u8003\u53e4\u63d0\u4f9b\u4e86\u9ad8\u6548\u5de5\u5177\u3002"}}
{"id": "2504.20462", "pdf": "https://arxiv.org/pdf/2504.20462", "abs": "https://arxiv.org/abs/2504.20462", "authors": ["Qi Wang", "Xiao Zhang", "Mingyi Li", "Yuan Yuan", "Mengbai Xiao", "Fuzhen Zhuang", "Dongxiao Yu"], "title": "TAMO:Fine-Grained Root Cause Analysis via Tool-Assisted LLM Agent with Multi-Modality Observation Data", "categories": ["cs.AI"], "comment": null, "summary": "With the development of distributed systems, microservices and cloud native\ntechnologies have become central to modern enterprise software development.\nDespite bringing significant advantages, these technologies also increase\nsystem complexity and operational challenges. Traditional root cause analysis\n(RCA) struggles to achieve automated fault response, heavily relying on manual\nintervention. In recent years, large language models (LLMs) have made\nbreakthroughs in contextual inference and domain knowledge integration,\nproviding new solutions for Artificial Intelligence for Operations (AIOps).\nHowever, Existing LLM-based approaches face three key challenges: text input\nconstraints, dynamic service dependency hallucinations, and context window\nlimitations. To address these issues, we propose a tool-assisted LLM agent with\nmulti-modality observation data, namely TAMO, for fine-grained RCA. It unifies\nmulti-modal observational data into time-aligned representations to extract\nconsistent features and employs specialized root cause localization and fault\nclassification tools for perceiving the contextual environment. This approach\novercomes the limitations of LLM in handling real-time changing service\ndependencies and raw observational data and guides LLM to generate repair\nstrategies aligned with system contexts by structuring key information into a\nprompt. Experimental results show that TAMO performs well in root cause\nanalysis when dealing with public datasets characterized by heterogeneity and\ncommon fault types, demonstrating its effectiveness.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aTAMO\u7684\u5de5\u5177\u8f85\u52a9LLM\u4ee3\u7406\uff0c\u7528\u4e8e\u89e3\u51b3\u5fae\u670d\u52a1\u548c\u4e91\u539f\u751f\u6280\u672f\u4e2d\u7684\u6545\u969c\u6839\u56e0\u5206\u6790\u95ee\u9898\u3002\u901a\u8fc7\u591a\u6a21\u6001\u89c2\u6d4b\u6570\u636e\u548c\u4e13\u7528\u5de5\u5177\uff0c\u514b\u670d\u4e86\u73b0\u6709LLM\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u968f\u7740\u5206\u5e03\u5f0f\u7cfb\u7edf\u7684\u53d1\u5c55\uff0c\u5fae\u670d\u52a1\u548c\u4e91\u539f\u751f\u6280\u672f\u5e26\u6765\u4e86\u7cfb\u7edf\u590d\u6742\u6027\u548c\u64cd\u4f5c\u6311\u6218\u7684\u589e\u52a0\uff0c\u4f20\u7edf\u6839\u56e0\u5206\u6790\u4f9d\u8d56\u4eba\u5de5\u5e72\u9884\uff0c\u96be\u4ee5\u5b9e\u73b0\u81ea\u52a8\u5316\u3002LLM\u7684\u7a81\u7834\u4e3aAIOps\u63d0\u4f9b\u4e86\u65b0\u89e3\u51b3\u65b9\u6848\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u9762\u4e34\u6587\u672c\u8f93\u5165\u9650\u5236\u3001\u52a8\u6001\u670d\u52a1\u4f9d\u8d56\u5e7b\u89c9\u548c\u4e0a\u4e0b\u6587\u7a97\u53e3\u9650\u5236\u7b49\u6311\u6218\u3002", "method": "TAMO\u901a\u8fc7\u7edf\u4e00\u591a\u6a21\u6001\u89c2\u6d4b\u6570\u636e\u4e3a\u65f6\u95f4\u5bf9\u9f50\u8868\u793a\uff0c\u63d0\u53d6\u4e00\u81f4\u7279\u5f81\uff0c\u5e76\u5229\u7528\u4e13\u7528\u5de5\u5177\u8fdb\u884c\u6839\u56e0\u5b9a\u4f4d\u548c\u6545\u969c\u5206\u7c7b\uff0c\u4ee5\u611f\u77e5\u4e0a\u4e0b\u6587\u73af\u5883\u3002\u901a\u8fc7\u7ed3\u6784\u5316\u5173\u952e\u4fe1\u606f\u751f\u6210\u63d0\u793a\uff0c\u6307\u5bfcLLM\u751f\u6210\u4e0e\u7cfb\u7edf\u4e0a\u4e0b\u6587\u4e00\u81f4\u7684\u4fee\u590d\u7b56\u7565\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cTAMO\u5728\u5904\u7406\u5f02\u6784\u6027\u548c\u5e38\u89c1\u6545\u969c\u7c7b\u578b\u7684\u516c\u5171\u6570\u636e\u96c6\u65f6\u8868\u73b0\u826f\u597d\uff0c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "conclusion": "TAMO\u901a\u8fc7\u7ed3\u5408\u591a\u6a21\u6001\u6570\u636e\u548c\u4e13\u7528\u5de5\u5177\uff0c\u663e\u8457\u63d0\u5347\u4e86LLM\u5728\u6839\u56e0\u5206\u6790\u4e2d\u7684\u80fd\u529b\uff0c\u4e3aAIOps\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2504.20306", "pdf": "https://arxiv.org/pdf/2504.20306", "abs": "https://arxiv.org/abs/2504.20306", "authors": ["Teja Krishna Cherukuri", "Nagur Shareef Shaik", "Sribhuvan Reddy Yellu", "Jun-Won Chung", "Dong Hye Ye"], "title": "Dynamic Contextual Attention Network: Transforming Spatial Representations into Adaptive Insights for Endoscopic Polyp Diagnosis", "categories": ["cs.CV"], "comment": "Accepted at 47th Annual International Conference of the IEEE\n  Engineering in Medicine and Biology Society (EMBC) 2025", "summary": "Colorectal polyps are key indicators for early detection of colorectal\ncancer. However, traditional endoscopic imaging often struggles with accurate\npolyp localization and lacks comprehensive contextual awareness, which can\nlimit the explainability of diagnoses. To address these issues, we propose the\nDynamic Contextual Attention Network (DCAN). This novel approach transforms\nspatial representations into adaptive contextual insights, using an attention\nmechanism that enhances focus on critical polyp regions without explicit\nlocalization modules. By integrating contextual awareness into the\nclassification process, DCAN improves decision interpretability and overall\ndiagnostic performance. This advancement in imaging could lead to more reliable\ncolorectal cancer detection, enabling better patient outcomes.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u52a8\u6001\u4e0a\u4e0b\u6587\u6ce8\u610f\u529b\u7f51\u7edc\uff08DCAN\uff09\uff0c\u7528\u4e8e\u6539\u8fdb\u7ed3\u80a0\u606f\u8089\u7684\u5b9a\u4f4d\u548c\u8bca\u65ad\u89e3\u91ca\u6027\u3002", "motivation": "\u4f20\u7edf\u5185\u7aa5\u955c\u6210\u50cf\u5728\u606f\u8089\u5b9a\u4f4d\u548c\u4e0a\u4e0b\u6587\u611f\u77e5\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u9650\u5236\u4e86\u8bca\u65ad\u7684\u53ef\u89e3\u91ca\u6027\u3002", "method": "\u4f7f\u7528\u6ce8\u610f\u529b\u673a\u5236\u5c06\u7a7a\u95f4\u8868\u5f81\u8f6c\u5316\u4e3a\u81ea\u9002\u5e94\u4e0a\u4e0b\u6587\u4fe1\u606f\uff0c\u65e0\u9700\u663e\u5f0f\u5b9a\u4f4d\u6a21\u5757\u3002", "result": "DCAN\u63d0\u9ad8\u4e86\u5206\u7c7b\u8fc7\u7a0b\u7684\u89e3\u91ca\u6027\u548c\u8bca\u65ad\u6027\u80fd\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u671b\u63d0\u5347\u7ed3\u80a0\u764c\u68c0\u6d4b\u7684\u53ef\u9760\u6027\uff0c\u6539\u5584\u60a3\u8005\u9884\u540e\u3002"}}
{"id": "2504.20464", "pdf": "https://arxiv.org/pdf/2504.20464", "abs": "https://arxiv.org/abs/2504.20464", "authors": ["Jiahao Li", "Kaer Huang"], "title": "A Summary on GUI Agents with Foundation Models Enhanced by Reinforcement Learning", "categories": ["cs.AI"], "comment": null, "summary": "Graphical User Interface (GUI) agents, driven by Multi-modal Large Language\nModels (MLLMs), have emerged as a promising paradigm for enabling intelligent\ninteraction with digital systems. This paper provides a structured summary of\nrecent advances in GUI agents, focusing on architectures enhanced by\nReinforcement Learning (RL). We first formalize GUI agent tasks as Markov\nDecision Processes and discuss typical execution environments and evaluation\nmetrics. We then review the modular architecture of (M)LLM-based GUI agents,\ncovering Perception, Planning, and Acting modules, and trace their evolution\nthrough representative works. Furthermore, we categorize GUI agent training\nmethodologies into Prompt-based, Supervised Fine-Tuning (SFT)-based, and\nRL-based approaches, highlighting the progression from simple prompt\nengineering to dynamic policy learning via RL. Our summary illustrates how\nrecent innovations in multimodal perception, decision reasoning, and adaptive\naction generation have significantly improved the generalization and robustness\nof GUI agents in complex real-world environments. We conclude by identifying\nkey challenges and future directions for building more capable and reliable GUI\nagents.", "AI": {"tldr": "\u672c\u6587\u603b\u7ed3\u4e86\u57fa\u4e8e\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u7684GUI\u4ee3\u7406\u7684\u6700\u65b0\u8fdb\u5c55\uff0c\u91cd\u70b9\u4ecb\u7ecd\u4e86\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u589e\u5f3a\u7684\u67b6\u6784\uff0c\u5e76\u63a2\u8ba8\u4e86\u5176\u4efb\u52a1\u5f62\u5f0f\u5316\u3001\u6a21\u5757\u5316\u8bbe\u8ba1\u53ca\u8bad\u7ec3\u65b9\u6cd5\u3002", "motivation": "GUI\u4ee3\u7406\u4f5c\u4e3a\u667a\u80fd\u4ea4\u4e92\u7684\u8303\u5f0f\uff0c\u5176\u6027\u80fd\u63d0\u5347\u4f9d\u8d56\u4e8e\u591a\u6a21\u6001\u611f\u77e5\u548c\u52a8\u6001\u7b56\u7565\u5b66\u4e60\uff0c\u672c\u6587\u65e8\u5728\u7cfb\u7edf\u68b3\u7406\u76f8\u5173\u6280\u672f\u8fdb\u5c55\u3002", "method": "\u5c06GUI\u4ee3\u7406\u4efb\u52a1\u5f62\u5f0f\u5316\u4e3a\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\uff0c\u5206\u6790\u5176\u6a21\u5757\u5316\u67b6\u6784\uff08\u611f\u77e5\u3001\u89c4\u5212\u3001\u6267\u884c\uff09\uff0c\u5e76\u5206\u7c7b\u8bad\u7ec3\u65b9\u6cd5\uff08\u63d0\u793a\u5de5\u7a0b\u3001\u76d1\u7763\u5fae\u8c03\u3001\u5f3a\u5316\u5b66\u4e60\uff09\u3002", "result": "\u591a\u6a21\u6001\u611f\u77e5\u3001\u51b3\u7b56\u63a8\u7406\u548c\u81ea\u9002\u5e94\u52a8\u4f5c\u751f\u6210\u7684\u6280\u672f\u521b\u65b0\u663e\u8457\u63d0\u5347\u4e86GUI\u4ee3\u7406\u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u6cdb\u5316\u80fd\u529b\u548c\u9c81\u68d2\u6027\u3002", "conclusion": "\u672a\u6765\u9700\u89e3\u51b3\u5173\u952e\u6311\u6218\u4ee5\u6784\u5efa\u66f4\u5f3a\u5927\u53ef\u9760\u7684GUI\u4ee3\u7406\uff0c\u5982\u8fdb\u4e00\u6b65\u4f18\u5316\u591a\u6a21\u6001\u878d\u5408\u548c\u52a8\u6001\u7b56\u7565\u5b66\u4e60\u3002"}}
{"id": "2504.20322", "pdf": "https://arxiv.org/pdf/2504.20322", "abs": "https://arxiv.org/abs/2504.20322", "authors": ["Sumit Mamtani", "Yash Thesia"], "title": "Fine Grain Classification: Connecting Meta using Cross-Contrastive pre-training", "categories": ["cs.CV", "cs.LG"], "comment": "9 pages, 4 figures. Submitted to arXiv", "summary": "Fine-grained visual classification aims to recognize objects belonging to\nmultiple subordinate categories within a super-category. However, this remains\na challenging problem, as appearance information alone is often insufficient to\naccurately differentiate between fine-grained visual categories. To address\nthis, we propose a novel and unified framework that leverages meta-information\nto assist fine-grained identification. We tackle the joint learning of visual\nand meta-information through cross-contrastive pre-training. In the first\nstage, we employ three encoders for images, text, and meta-information,\naligning their projected embeddings to achieve better representations. We then\nfine-tune the image and meta-information encoders for the classification task.\nExperiments on the NABirds dataset demonstrate that our framework effectively\nutilizes meta-information to enhance fine-grained recognition performance. With\nthe addition of meta-information, our framework surpasses the current baseline\non NABirds by 7.83%. Furthermore, it achieves an accuracy of 84.44% on the\nNABirds dataset, outperforming many existing state-of-the-art approaches that\nutilize meta-information.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u5143\u4fe1\u606f\u8f85\u52a9\u7ec6\u7c92\u5ea6\u89c6\u89c9\u5206\u7c7b\u7684\u7edf\u4e00\u6846\u67b6\uff0c\u901a\u8fc7\u8de8\u5bf9\u6bd4\u9884\u8bad\u7ec3\u8054\u5408\u5b66\u4e60\u89c6\u89c9\u548c\u5143\u4fe1\u606f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5206\u7c7b\u6027\u80fd\u3002", "motivation": "\u7ec6\u7c92\u5ea6\u89c6\u89c9\u5206\u7c7b\u4ec5\u4f9d\u8d56\u5916\u89c2\u4fe1\u606f\u96be\u4ee5\u51c6\u786e\u533a\u5206\u5b50\u7c7b\u522b\uff0c\u56e0\u6b64\u9700\u8981\u5f15\u5165\u5143\u4fe1\u606f\u8f85\u52a9\u8bc6\u522b\u3002", "method": "\u91c7\u7528\u4e09\u4e2a\u7f16\u7801\u5668\u5206\u522b\u5904\u7406\u56fe\u50cf\u3001\u6587\u672c\u548c\u5143\u4fe1\u606f\uff0c\u901a\u8fc7\u8de8\u5bf9\u6bd4\u9884\u8bad\u7ec3\u5bf9\u9f50\u5d4c\u5165\u8868\u793a\uff0c\u968f\u540e\u5fae\u8c03\u56fe\u50cf\u548c\u5143\u4fe1\u606f\u7f16\u7801\u5668\u8fdb\u884c\u5206\u7c7b\u4efb\u52a1\u3002", "result": "\u5728NABirds\u6570\u636e\u96c6\u4e0a\uff0c\u6846\u67b6\u5229\u7528\u5143\u4fe1\u606f\u4f7f\u6027\u80fd\u63d0\u53477.83%\uff0c\u51c6\u786e\u7387\u8fbe\u523084.44%\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u63d0\u51fa\u7684\u6846\u67b6\u901a\u8fc7\u7ed3\u5408\u5143\u4fe1\u606f\u6709\u6548\u63d0\u5347\u4e86\u7ec6\u7c92\u5ea6\u89c6\u89c9\u5206\u7c7b\u6027\u80fd\uff0c\u8bc1\u660e\u4e86\u5143\u4fe1\u606f\u7684\u4ef7\u503c\u3002"}}
{"id": "2504.20505", "pdf": "https://arxiv.org/pdf/2504.20505", "abs": "https://arxiv.org/abs/2504.20505", "authors": ["Xi Chen", "Julien Cumin", "Fano Ramparany", "Dominique Vaufreydaz"], "title": "MuRAL: A Multi-Resident Ambient Sensor Dataset Annotated with Natural Language for Activities of Daily Living", "categories": ["cs.AI"], "comment": null, "summary": "Recent advances in Large Language Models (LLMs) have shown promising\npotential for human activity recognition (HAR) using ambient sensors,\nespecially through natural language reasoning and zero-shot learning. However,\nexisting datasets such as CASAS, ARAS, and MARBLE were not originally designed\nwith LLMs in mind and therefore lack the contextual richness, complexity, and\nannotation granularity required to fully exploit LLM capabilities. In this\npaper, we introduce MuRAL, the first Multi-Resident Ambient sensor dataset with\nnatural Language, comprising over 21 hours of multi-user sensor data collected\nfrom 21 sessions in a smart-home environment. MuRAL is annotated with\nfine-grained natural language descriptions, resident identities, and high-level\nactivity labels, all situated in dynamic, realistic multi-resident settings. We\nbenchmark MuRAL using state-of-the-art LLMs for three core tasks: subject\nassignment, action description, and activity classification. Our results\ndemonstrate that while LLMs can provide rich semantic interpretations of\nambient data, current models still face challenges in handling multi-user\nambiguity and under-specified sensor contexts. We release MuRAL to support\nfuture research on LLM-powered, explainable, and socially aware activity\nunderstanding in smart environments. For access to the dataset, please reach\nout to us via the provided contact information. A direct link for dataset\nretrieval will be made available at this location in due course.", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86MuRAL\u6570\u636e\u96c6\uff0c\u9996\u4e2a\u4e3a\u591a\u5c45\u6c11\u73af\u5883\u8bbe\u8ba1\u7684\u81ea\u7136\u8bed\u8a00\u6807\u6ce8\u7684\u4f20\u611f\u5668\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u8bc4\u4f30LLM\u5728\u4eba\u7c7b\u6d3b\u52a8\u8bc6\u522b\u4e2d\u7684\u8868\u73b0\u3002", "motivation": "\u73b0\u6709\u6570\u636e\u96c6\uff08\u5982CASAS\u3001ARAS\u3001MARBLE\uff09\u7f3a\u4e4f\u4e0a\u4e0b\u6587\u4e30\u5bcc\u6027\u548c\u6807\u6ce8\u7c92\u5ea6\uff0c\u65e0\u6cd5\u5145\u5206\u5229\u7528LLM\u7684\u6f5c\u529b\u3002", "method": "\u6536\u96c621\u5c0f\u65f6\u591a\u7528\u6237\u4f20\u611f\u5668\u6570\u636e\uff0c\u6807\u6ce8\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0\u3001\u5c45\u6c11\u8eab\u4efd\u548c\u9ad8\u5c42\u6d3b\u52a8\u6807\u7b7e\uff0c\u5e76\u8bc4\u4f30LLM\u5728\u4e09\u4e2a\u6838\u5fc3\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "result": "LLM\u80fd\u63d0\u4f9b\u4e30\u5bcc\u7684\u8bed\u4e49\u89e3\u91ca\uff0c\u4f46\u5728\u5904\u7406\u591a\u7528\u6237\u6a21\u7cca\u6027\u548c\u4f20\u611f\u5668\u4e0a\u4e0b\u6587\u4e0d\u8db3\u65f6\u4ecd\u9762\u4e34\u6311\u6218\u3002", "conclusion": "MuRAL\u652f\u6301\u672a\u6765\u7814\u7a76\uff0c\u63a8\u52a8LLM\u5728\u667a\u80fd\u73af\u5883\u4e2d\u53ef\u89e3\u91ca\u548c\u793e\u4ea4\u611f\u77e5\u7684\u6d3b\u52a8\u7406\u89e3\u3002"}}
{"id": "2504.20343", "pdf": "https://arxiv.org/pdf/2504.20343", "abs": "https://arxiv.org/abs/2504.20343", "authors": ["Amaan Izhar", "Nurul Japar", "Norisma Idris", "Ting Dang"], "title": "MicarVLMoE: A Modern Gated Cross-Aligned Vision-Language Mixture of Experts Model for Medical Image Captioning and Report Generation", "categories": ["cs.CV"], "comment": "Accepted by IJCNN 2025, 8 pages, 8 figures, 3 tables", "summary": "Medical image reporting (MIR) aims to generate structured clinical\ndescriptions from radiological images. Existing methods struggle with\nfine-grained feature extraction, multimodal alignment, and generalization\nacross diverse imaging types, often relying on vanilla transformers and\nfocusing primarily on chest X-rays. We propose MicarVLMoE, a vision-language\nmixture-of-experts model with gated cross-aligned fusion, designed to address\nthese limitations. Our architecture includes: (i) a multiscale vision encoder\n(MSVE) for capturing anatomical details at varying resolutions, (ii) a\nmultihead dual-branch latent attention (MDLA) module for vision-language\nalignment through latent bottleneck representations, and (iii) a modulated\nmixture-of-experts (MoE) decoder for adaptive expert specialization. We extend\nMIR to CT scans, retinal imaging, MRI scans, and gross pathology images,\nreporting state-of-the-art results on COVCTR, MMR, PGROSS, and ROCO datasets.\nExtensive experiments and ablations confirm improved clinical accuracy,\ncross-modal alignment, and model interpretability. Code is available at\nhttps://github.com/AI-14/micar-vl-moe.", "AI": {"tldr": "MicarVLMoE\u6a21\u578b\u901a\u8fc7\u591a\u5c3a\u5ea6\u89c6\u89c9\u7f16\u7801\u5668\u548c\u4e13\u5bb6\u6df7\u5408\u89e3\u7801\u5668\uff0c\u89e3\u51b3\u4e86\u533b\u5b66\u56fe\u50cf\u62a5\u544a\u4e2d\u7ec6\u7c92\u5ea6\u7279\u5f81\u63d0\u53d6\u548c\u591a\u6a21\u6001\u5bf9\u9f50\u7684\u6311\u6218\uff0c\u5e76\u5728\u591a\u79cd\u5f71\u50cf\u7c7b\u578b\u4e0a\u53d6\u5f97\u4e86\u5148\u8fdb\u6210\u679c\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u7ec6\u7c92\u5ea6\u7279\u5f81\u63d0\u53d6\u3001\u591a\u6a21\u6001\u5bf9\u9f50\u548c\u8de8\u5f71\u50cf\u7c7b\u578b\u6cdb\u5316\u65b9\u9762\u8868\u73b0\u4e0d\u4f73\uff0c\u4e14\u4e3b\u8981\u5173\u6ce8\u80f8\u90e8X\u5149\u7247\u3002", "method": "\u63d0\u51faMicarVLMoE\u6a21\u578b\uff0c\u5305\u62ec\u591a\u5c3a\u5ea6\u89c6\u89c9\u7f16\u7801\u5668\uff08MSVE\uff09\u3001\u591a\u5934\u53cc\u5206\u652f\u6f5c\u5728\u6ce8\u610f\u529b\u6a21\u5757\uff08MDLA\uff09\u548c\u8c03\u5236\u7684\u4e13\u5bb6\u6df7\u5408\u89e3\u7801\u5668\uff08MoE\uff09\u3002", "result": "\u5728COVCTR\u3001MMR\u3001PGROSS\u548cROCO\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u7ed3\u679c\uff0c\u63d0\u5347\u4e86\u4e34\u5e8a\u51c6\u786e\u6027\u3001\u8de8\u6a21\u6001\u5bf9\u9f50\u548c\u6a21\u578b\u53ef\u89e3\u91ca\u6027\u3002", "conclusion": "MicarVLMoE\u5728\u591a\u79cd\u533b\u5b66\u5f71\u50cf\u7c7b\u578b\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90\u3002"}}
{"id": "2504.20595", "pdf": "https://arxiv.org/pdf/2504.20595", "abs": "https://arxiv.org/abs/2504.20595", "authors": ["Rulin Shao", "Rui Qiao", "Varsha Kishore", "Niklas Muennighoff", "Xi Victoria Lin", "Daniela Rus", "Bryan Kian Hsiang Low", "Sewon Min", "Wen-tau Yih", "Pang Wei Koh", "Luke Zettlemoyer"], "title": "ReasonIR: Training Retrievers for Reasoning Tasks", "categories": ["cs.AI", "cs.CL", "cs.IR", "cs.LG"], "comment": "Our code is released at\n  \\url{https://github.com/facebookresearch/ReasonIR}", "summary": "We present ReasonIR-8B, the first retriever specifically trained for general\nreasoning tasks. Existing retrievers have shown limited gains on reasoning\ntasks, in part because existing training datasets focus on short factual\nqueries tied to documents that straightforwardly answer them. We develop a\nsynthetic data generation pipeline that, for each document, our pipeline\ncreates a challenging and relevant query, along with a plausibly related but\nultimately unhelpful hard negative. By training on a mixture of our synthetic\ndata and existing public data, ReasonIR-8B achieves a new state-of-the-art of\n29.9 nDCG@10 without reranker and 36.9 nDCG@10 with reranker on BRIGHT, a\nwidely-used reasoning-intensive information retrieval (IR) benchmark. When\napplied to RAG tasks, ReasonIR-8B improves MMLU and GPQA performance by 6.4%\nand 22.6% respectively, relative to the closed-book baseline, outperforming\nother retrievers and search engines. In addition, ReasonIR-8B uses test-time\ncompute more effectively: on BRIGHT, its performance consistently increases\nwith longer and more information-rich rewritten queries; it continues to\noutperform other retrievers when combined with an LLM reranker. Our training\nrecipe is general and can be easily extended to future LLMs; to this end, we\nopen-source our code, data, and model.", "AI": {"tldr": "ReasonIR-8B\u662f\u9996\u4e2a\u4e13\u4e3a\u901a\u7528\u63a8\u7406\u4efb\u52a1\u8bad\u7ec3\u7684\u68c0\u7d22\u6a21\u578b\uff0c\u901a\u8fc7\u5408\u6210\u6570\u636e\u548c\u516c\u5171\u6570\u636e\u6df7\u5408\u8bad\u7ec3\uff0c\u5728\u63a8\u7406\u5bc6\u96c6\u578bIR\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u6700\u4f73\u6027\u80fd\uff0c\u5e76\u5728RAG\u4efb\u52a1\u4e2d\u663e\u8457\u63d0\u5347\u8868\u73b0\u3002", "motivation": "\u73b0\u6709\u68c0\u7d22\u6a21\u578b\u5728\u63a8\u7406\u4efb\u52a1\u4e2d\u8868\u73b0\u6709\u9650\uff0c\u4e3b\u8981\u56e0\u4e3a\u8bad\u7ec3\u6570\u636e\u96c6\u4e2d\u4e8e\u7b80\u5355\u4e8b\u5b9e\u67e5\u8be2\u3002", "method": "\u5f00\u53d1\u5408\u6210\u6570\u636e\u751f\u6210\u6d41\u7a0b\uff0c\u521b\u5efa\u5177\u6709\u6311\u6218\u6027\u7684\u67e5\u8be2\u548c\u786c\u8d1f\u6837\u672c\uff0c\u7ed3\u5408\u516c\u5171\u6570\u636e\u8bad\u7ec3\u6a21\u578b\u3002", "result": "\u5728BRIGHT\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u523029.9 nDCG@10\uff08\u65e0\u91cd\u6392\uff09\u548c36.9 nDCG@10\uff08\u6709\u91cd\u6392\uff09\uff0c\u5728RAG\u4efb\u52a1\u4e2d\u63d0\u5347MMLU\u548cGPQA\u6027\u80fd\u3002", "conclusion": "ReasonIR-8B\u5728\u63a8\u7406\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u8bad\u7ec3\u65b9\u6cd5\u901a\u7528\u4e14\u53ef\u6269\u5c55\uff0c\u4ee3\u7801\u548c\u6570\u636e\u5df2\u5f00\u6e90\u3002"}}
{"id": "2504.20362", "pdf": "https://arxiv.org/pdf/2504.20362", "abs": "https://arxiv.org/abs/2504.20362", "authors": ["Qinhua Xie", "Hao Tang"], "title": "TTTFusion: A Test-Time Training-Based Strategy for Multimodal Medical Image Fusion in Surgical Robots", "categories": ["cs.CV"], "comment": null, "summary": "With the increasing use of surgical robots in clinical practice, enhancing\ntheir ability to process multimodal medical images has become a key research\nchallenge. Although traditional medical image fusion methods have made progress\nin improving fusion accuracy, they still face significant challenges in\nreal-time performance, fine-grained feature extraction, and edge\npreservation.In this paper, we introduce TTTFusion, a Test-Time Training\n(TTT)-based image fusion strategy that dynamically adjusts model parameters\nduring inference to efficiently fuse multimodal medical images. By adapting the\nmodel during the test phase, our method optimizes the parameters based on the\ninput image data, leading to improved accuracy and better detail preservation\nin the fusion results.Experimental results demonstrate that TTTFusion\nsignificantly enhances the fusion quality of multimodal images compared to\ntraditional fusion methods, particularly in fine-grained feature extraction and\nedge preservation. This approach not only improves image fusion accuracy but\nalso offers a novel technical solution for real-time image processing in\nsurgical robots.", "AI": {"tldr": "TTTFusion\u662f\u4e00\u79cd\u57fa\u4e8e\u6d4b\u8bd5\u65f6\u8bad\u7ec3\uff08TTT\uff09\u7684\u56fe\u50cf\u878d\u5408\u7b56\u7565\uff0c\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u6a21\u578b\u53c2\u6570\u63d0\u5347\u591a\u6a21\u6001\u533b\u5b66\u56fe\u50cf\u7684\u878d\u5408\u8d28\u91cf\uff0c\u5c24\u5176\u5728\u7ec6\u8282\u63d0\u53d6\u548c\u8fb9\u7f18\u4fdd\u7559\u65b9\u9762\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u968f\u7740\u624b\u672f\u673a\u5668\u4eba\u4e34\u5e8a\u5e94\u7528\u7684\u589e\u52a0\uff0c\u63d0\u5347\u5176\u5904\u7406\u591a\u6a21\u6001\u533b\u5b66\u56fe\u50cf\u7684\u80fd\u529b\u6210\u4e3a\u5173\u952e\u6311\u6218\u3002\u4f20\u7edf\u878d\u5408\u65b9\u6cd5\u5728\u5b9e\u65f6\u6027\u3001\u7ec6\u8282\u63d0\u53d6\u548c\u8fb9\u7f18\u4fdd\u7559\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\u3002", "method": "\u63d0\u51faTTTFusion\u7b56\u7565\uff0c\u5728\u63a8\u7406\u9636\u6bb5\u52a8\u6001\u8c03\u6574\u6a21\u578b\u53c2\u6570\uff0c\u6839\u636e\u8f93\u5165\u56fe\u50cf\u4f18\u5316\u878d\u5408\u6548\u679c\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cTTTFusion\u5728\u591a\u6a21\u6001\u56fe\u50cf\u878d\u5408\u8d28\u91cf\u4e0a\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\uff0c\u5c24\u5176\u5728\u7ec6\u8282\u548c\u8fb9\u7f18\u4fdd\u7559\u65b9\u9762\u3002", "conclusion": "TTTFusion\u4e0d\u4ec5\u63d0\u5347\u4e86\u878d\u5408\u7cbe\u5ea6\uff0c\u8fd8\u4e3a\u624b\u672f\u673a\u5668\u4eba\u7684\u5b9e\u65f6\u56fe\u50cf\u5904\u7406\u63d0\u4f9b\u4e86\u65b0\u6280\u672f\u65b9\u6848\u3002"}}
{"id": "2504.20624", "pdf": "https://arxiv.org/pdf/2504.20624", "abs": "https://arxiv.org/abs/2504.20624", "authors": ["Zihan Niu", "Zheyong Xie", "Shaosheng Cao", "Chonggang Lu", "Zheyu Ye", "Tong Xu", "Zuozhu Liu", "Yan Gao", "Jia Chen", "Zhe Xu", "Yi Wu", "Yao Hu"], "title": "PaRT: Enhancing Proactive Social Chatbots with Personalized Real-Time Retrieval", "categories": ["cs.AI"], "comment": null, "summary": "Social chatbots have become essential intelligent companions in daily\nscenarios ranging from emotional support to personal interaction. However,\nconventional chatbots with passive response mechanisms usually rely on users to\ninitiate or sustain dialogues by bringing up new topics, resulting in\ndiminished engagement and shortened dialogue duration. In this paper, we\npresent PaRT, a novel framework enabling context-aware proactive dialogues for\nsocial chatbots through personalized real-time retrieval and generation.\nSpecifically, PaRT first integrates user profiles and dialogue context into a\nlarge language model (LLM), which is initially prompted to refine user queries\nand recognize their underlying intents for the upcoming conversation. Guided by\nrefined intents, the LLM generates personalized dialogue topics, which then\nserve as targeted queries to retrieve relevant passages from RedNote. Finally,\nwe prompt LLMs with summarized passages to generate knowledge-grounded and\nengagement-optimized responses. Our approach has been running stably in a\nreal-world production environment for more than 30 days, achieving a 21.77\\%\nimprovement in the average duration of dialogues.", "AI": {"tldr": "PaRT\u6846\u67b6\u901a\u8fc7\u4e2a\u6027\u5316\u5b9e\u65f6\u68c0\u7d22\u548c\u751f\u6210\uff0c\u5b9e\u73b0\u793e\u4ea4\u804a\u5929\u673a\u5668\u4eba\u7684\u4e0a\u4e0b\u6587\u611f\u77e5\u4e3b\u52a8\u5bf9\u8bdd\uff0c\u663e\u8457\u63d0\u5347\u5bf9\u8bdd\u65f6\u957f\u3002", "motivation": "\u4f20\u7edf\u804a\u5929\u673a\u5668\u4eba\u4f9d\u8d56\u7528\u6237\u4e3b\u52a8\u53d1\u8d77\u6216\u7ef4\u6301\u5bf9\u8bdd\uff0c\u5bfc\u81f4\u53c2\u4e0e\u5ea6\u4f4e\u548c\u5bf9\u8bdd\u65f6\u957f\u7f29\u77ed\u3002", "method": "PaRT\u6574\u5408\u7528\u6237\u753b\u50cf\u548c\u5bf9\u8bdd\u4e0a\u4e0b\u6587\u5230LLM\u4e2d\uff0c\u751f\u6210\u4e2a\u6027\u5316\u8bdd\u9898\u5e76\u68c0\u7d22\u76f8\u5173\u77e5\u8bc6\uff0c\u6700\u7ec8\u751f\u6210\u77e5\u8bc6\u9a71\u52a8\u7684\u4f18\u5316\u54cd\u5e94\u3002", "result": "\u5728\u5b9e\u9645\u751f\u4ea7\u73af\u5883\u4e2d\u8fd0\u884c30\u5929\uff0c\u5e73\u5747\u5bf9\u8bdd\u65f6\u957f\u63d0\u534721.77%\u3002", "conclusion": "PaRT\u6846\u67b6\u6709\u6548\u63d0\u5347\u4e86\u793e\u4ea4\u804a\u5929\u673a\u5668\u4eba\u7684\u4e3b\u52a8\u6027\u548c\u7528\u6237\u53c2\u4e0e\u5ea6\u3002"}}
{"id": "2504.20376", "pdf": "https://arxiv.org/pdf/2504.20376", "abs": "https://arxiv.org/abs/2504.20376", "authors": ["Shiqian Zhao", "Jiayang Liu", "Yiming Li", "Runyi Hu", "Xiaojun Jia", "Wenshu Fan", "Xinfeng Li", "Jie Zhang", "Wei Dong", "Tianwei Zhang", "Luu Anh Tuan"], "title": "Inception: Jailbreak the Memory Mechanism of Text-to-Image Generation Systems", "categories": ["cs.CV", "cs.CR"], "comment": "17 pages, 8 figures", "summary": "Currently, the memory mechanism has been widely and successfully exploited in\nonline text-to-image (T2I) generation systems ($e.g.$, DALL$\\cdot$E 3) for\nalleviating the growing tokenization burden and capturing key information in\nmulti-turn interactions. Despite its practicality, its security analyses have\nfallen far behind. In this paper, we reveal that this mechanism exacerbates the\nrisk of jailbreak attacks. Different from previous attacks that fuse the unsafe\ntarget prompt into one ultimate adversarial prompt, which can be easily\ndetected or may generate non-unsafe images due to under- or over-optimization,\nwe propose Inception, the first multi-turn jailbreak attack against the memory\nmechanism in real-world text-to-image generation systems. Inception embeds the\nmalice at the inception of the chat session turn by turn, leveraging the\nmechanism that T2I generation systems retrieve key information in their memory.\nSpecifically, Inception mainly consists of two modules. It first segments the\nunsafe prompt into chunks, which are subsequently fed to the system in multiple\nturns, serving as pseudo-gradients for directive optimization. Specifically, we\ndevelop a series of segmentation policies that ensure the images generated are\nsemantically consistent with the target prompt. Secondly, after segmentation,\nto overcome the challenge of the inseparability of minimum unsafe words, we\npropose recursion, a strategy that makes minimum unsafe words subdivisible.\nCollectively, segmentation and recursion ensure that all the request prompts\nare benign but can lead to malicious outcomes. We conduct experiments on the\nreal-world text-to-image generation system ($i.e.$, DALL$\\cdot$E 3) to validate\nthe effectiveness of Inception. The results indicate that Inception surpasses\nthe state-of-the-art by a 14\\% margin in attack success rate.", "AI": {"tldr": "\u8bba\u6587\u63ed\u793a\u4e86\u5728\u7ebf\u6587\u672c\u751f\u6210\u56fe\u50cf\u7cfb\u7edf\u4e2d\u7684\u8bb0\u5fc6\u673a\u5236\u52a0\u5267\u4e86\u8d8a\u72f1\u653b\u51fb\u7684\u98ce\u9669\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aInception\u7684\u591a\u8f6e\u8d8a\u72f1\u653b\u51fb\u65b9\u6cd5\uff0c\u901a\u8fc7\u5206\u5757\u548c\u9012\u5f52\u7b56\u7565\u5b9e\u73b0\u9ad8\u6210\u529f\u7387\u3002", "motivation": "\u5f53\u524d\u6587\u672c\u751f\u6210\u56fe\u50cf\u7cfb\u7edf\u7684\u8bb0\u5fc6\u673a\u5236\u867d\u7136\u5b9e\u7528\uff0c\u4f46\u5176\u5b89\u5168\u6027\u5206\u6790\u6ede\u540e\uff0c\u5b58\u5728\u88ab\u6ee5\u7528\u4e8e\u8d8a\u72f1\u653b\u51fb\u7684\u98ce\u9669\u3002", "method": "\u63d0\u51faInception\u65b9\u6cd5\uff0c\u5c06\u6076\u610f\u63d0\u793a\u5206\u5757\u8f93\u5165\u7cfb\u7edf\uff0c\u5229\u7528\u8bb0\u5fc6\u673a\u5236\u9010\u6b65\u4f18\u5316\uff0c\u5e76\u901a\u8fc7\u9012\u5f52\u7b56\u7565\u5904\u7406\u4e0d\u53ef\u5206\u5272\u7684\u6700\u5c0f\u6076\u610f\u8bcd\u6c47\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cInception\u5728\u771f\u5b9e\u7cfb\u7edf\u4e2d\u7684\u653b\u51fb\u6210\u529f\u7387\u6bd4\u73b0\u6709\u65b9\u6cd5\u9ad8\u51fa14%\u3002", "conclusion": "\u8bb0\u5fc6\u673a\u5236\u7684\u5b89\u5168\u6027\u9700\u5f15\u8d77\u91cd\u89c6\uff0cInception\u65b9\u6cd5\u5c55\u793a\u4e86\u5176\u6f5c\u5728\u5a01\u80c1\uff0c\u4e3a\u672a\u6765\u9632\u5fa1\u63d0\u4f9b\u4e86\u7814\u7a76\u65b9\u5411\u3002"}}
{"id": "2504.20628", "pdf": "https://arxiv.org/pdf/2504.20628", "abs": "https://arxiv.org/abs/2504.20628", "authors": ["Marta Kryven", "Cole Wyeth", "Aidan Curtis", "Kevin Ellis"], "title": "Cognitive maps are generative programs", "categories": ["cs.AI", "cs.ET"], "comment": "9 pages, 4 figures, to be published in Cognitive Sciences Society\n  proceedings", "summary": "Making sense of the world and acting in it relies on building simplified\nmental representations that abstract away aspects of reality. This principle of\ncognitive mapping is universal to agents with limited resources. Living\norganisms, people, and algorithms all face the problem of forming functional\nrepresentations of their world under various computing constraints. In this\nwork, we explore the hypothesis that human resource-efficient planning may\narise from representing the world as predictably structured. Building on the\nmetaphor of concepts as programs, we propose that cognitive maps can take the\nform of generative programs that exploit predictability and redundancy, in\ncontrast to directly encoding spatial layouts. We use a behavioral experiment\nto show that people who navigate in structured spaces rely on modular planning\nstrategies that align with programmatic map representations. We describe a\ncomputational model that predicts human behavior in a variety of structured\nscenarios. This model infers a small distribution over possible programmatic\ncognitive maps conditioned on human prior knowledge of the world, and uses this\ndistribution to generate resource-efficient plans. Our models leverages a Large\nLanguage Model as an embedding of human priors, implicitly learned through\ntraining on a vast corpus of human data. Our model demonstrates improved\ncomputational efficiency, requires drastically less memory, and outperforms\nunstructured planning algorithms with cognitive constraints at predicting human\nbehavior, suggesting that human planning strategies rely on programmatic\ncognitive maps.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4eba\u7c7b\u8d44\u6e90\u9ad8\u6548\u89c4\u5212\u53ef\u80fd\u6e90\u4e8e\u5c06\u4e16\u754c\u8868\u793a\u4e3a\u53ef\u9884\u6d4b\u7ed3\u6784\uff0c\u63d0\u51fa\u8ba4\u77e5\u5730\u56fe\u53ef\u8868\u73b0\u4e3a\u751f\u6210\u7a0b\u5e8f\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u548c\u8ba1\u7b97\u6a21\u578b\u9a8c\u8bc1\u4e86\u7a0b\u5e8f\u5316\u8ba4\u77e5\u5730\u56fe\u7684\u5047\u8bbe\u3002", "motivation": "\u7814\u7a76\u4eba\u7c7b\u5982\u4f55\u5728\u6709\u9650\u8d44\u6e90\u4e0b\u6784\u5efa\u529f\u80fd\u6027\u4e16\u754c\u8868\u5f81\uff0c\u63a2\u7d22\u8ba4\u77e5\u5730\u56fe\u662f\u5426\u4ee5\u7a0b\u5e8f\u5316\u5f62\u5f0f\u5229\u7528\u53ef\u9884\u6d4b\u6027\u548c\u5197\u4f59\u6027\u3002", "method": "\u7ed3\u5408\u884c\u4e3a\u5b9e\u9a8c\u548c\u8ba1\u7b97\u6a21\u578b\uff0c\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5d4c\u5165\u4eba\u7c7b\u5148\u9a8c\u77e5\u8bc6\uff0c\u63a8\u65ad\u7a0b\u5e8f\u5316\u8ba4\u77e5\u5730\u56fe\u5206\u5e03\u3002", "result": "\u6a21\u578b\u5728\u9884\u6d4b\u4eba\u7c7b\u884c\u4e3a\u65f6\u8868\u73b0\u4f18\u4e8e\u975e\u7ed3\u6784\u5316\u89c4\u5212\u7b97\u6cd5\uff0c\u8ba1\u7b97\u6548\u7387\u66f4\u9ad8\u4e14\u5185\u5b58\u9700\u6c42\u66f4\u4f4e\u3002", "conclusion": "\u4eba\u7c7b\u89c4\u5212\u7b56\u7565\u4f9d\u8d56\u4e8e\u7a0b\u5e8f\u5316\u8ba4\u77e5\u5730\u56fe\uff0c\u9a8c\u8bc1\u4e86\u8d44\u6e90\u9ad8\u6548\u89c4\u5212\u7684\u5047\u8bbe\u3002"}}
{"id": "2504.20378", "pdf": "https://arxiv.org/pdf/2504.20378", "abs": "https://arxiv.org/abs/2504.20378", "authors": ["Jiang Wu", "Rui Li", "Yu Zhu", "Rong Guo", "Jinqiu Sun", "Yanning Zhang"], "title": "Sparse2DGS: Geometry-Prioritized Gaussian Splatting for Surface Reconstruction from Sparse Views", "categories": ["cs.CV"], "comment": "CVPR 2025", "summary": "We present a Gaussian Splatting method for surface reconstruction using\nsparse input views. Previous methods relying on dense views struggle with\nextremely sparse Structure-from-Motion points for initialization. While\nlearning-based Multi-view Stereo (MVS) provides dense 3D points, directly\ncombining it with Gaussian Splatting leads to suboptimal results due to the\nill-posed nature of sparse-view geometric optimization. We propose Sparse2DGS,\nan MVS-initialized Gaussian Splatting pipeline for complete and accurate\nreconstruction. Our key insight is to incorporate the geometric-prioritized\nenhancement schemes, allowing for direct and robust geometric learning under\nill-posed conditions. Sparse2DGS outperforms existing methods by notable\nmargins while being ${2}\\times$ faster than the NeRF-based fine-tuning\napproach.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7a00\u758f\u8f93\u5165\u89c6\u56fe\u7684\u9ad8\u65af\u6cfc\u6e85\u8868\u9762\u91cd\u5efa\u65b9\u6cd5Sparse2DGS\uff0c\u7ed3\u5408\u51e0\u4f55\u4f18\u5148\u589e\u5f3a\u65b9\u6848\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u4e14\u901f\u5ea6\u66f4\u5feb\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u5bc6\u96c6\u89c6\u56fe\u6216\u7a00\u758f\u521d\u59cb\u5316\u6548\u679c\u4e0d\u4f73\uff0c\u5b66\u4e60\u578b\u591a\u89c6\u56fe\u7acb\u4f53\u89c6\u89c9\uff08MVS\uff09\u76f4\u63a5\u7ed3\u5408\u9ad8\u65af\u6cfc\u6e85\u6548\u679c\u4e0d\u7406\u60f3\uff0c\u9700\u89e3\u51b3\u7a00\u758f\u89c6\u56fe\u51e0\u4f55\u4f18\u5316\u7684\u4e0d\u9002\u5b9a\u95ee\u9898\u3002", "method": "\u63d0\u51faSparse2DGS\uff0c\u5229\u7528MVS\u521d\u59cb\u5316\u9ad8\u65af\u6cfc\u6e85\u7ba1\u9053\uff0c\u7ed3\u5408\u51e0\u4f55\u4f18\u5148\u589e\u5f3a\u65b9\u6848\uff0c\u5b9e\u73b0\u76f4\u63a5\u4e14\u7a33\u5065\u7684\u51e0\u4f55\u5b66\u4e60\u3002", "result": "Sparse2DGS\u5728\u7a00\u758f\u89c6\u56fe\u4e0b\u8868\u73b0\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u901f\u5ea6\u6bd4\u57fa\u4e8eNeRF\u7684\u5fae\u8c03\u65b9\u6cd5\u5feb2\u500d\u3002", "conclusion": "Sparse2DGS\u901a\u8fc7\u51e0\u4f55\u4f18\u5148\u589e\u5f3a\u65b9\u6848\uff0c\u5728\u7a00\u758f\u89c6\u56fe\u4e0b\u5b9e\u73b0\u4e86\u9ad8\u6548\u4e14\u51c6\u786e\u7684\u8868\u9762\u91cd\u5efa\u3002"}}
{"id": "2504.20676", "pdf": "https://arxiv.org/pdf/2504.20676", "abs": "https://arxiv.org/abs/2504.20676", "authors": ["Shrisha Rao"], "title": "The Limits of AI Explainability: An Algorithmic Information Theory Approach", "categories": ["cs.AI", "cs.CY", "cs.IT", "math.IT", "68Q30, 68T01", "I.2.0; H.1.1; K.4.1"], "comment": null, "summary": "This paper establishes a theoretical foundation for understanding the\nfundamental limits of AI explainability through algorithmic information theory.\nWe formalize explainability as the approximation of complex models by simpler\nones, quantifying both approximation error and explanation complexity using\nKolmogorov complexity. Our key theoretical contributions include: (1) a\ncomplexity gap theorem proving that any explanation significantly simpler than\nthe original model must differ from it on some inputs; (2) precise bounds\nshowing that explanation complexity grows exponentially with input dimension\nbut polynomially with error tolerance for Lipschitz functions; and (3) a\ncharacterization of the gap between local and global explainability,\ndemonstrating that local explanations can be significantly simpler while\nmaintaining accuracy in relevant regions. We further establish a regulatory\nimpossibility theorem proving that no governance framework can simultaneously\npursue unrestricted AI capabilities, human-interpretable explanations, and\nnegligible error. These results highlight considerations likely to be relevant\nto the design, evaluation, and oversight of explainable AI systems.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u7b97\u6cd5\u4fe1\u606f\u7406\u8bba\u4e3aAI\u53ef\u89e3\u91ca\u6027\u7684\u57fa\u672c\u9650\u5236\u5efa\u7acb\u4e86\u7406\u8bba\u57fa\u7840\uff0c\u91cf\u5316\u4e86\u8fd1\u4f3c\u8bef\u5dee\u548c\u89e3\u91ca\u590d\u6742\u6027\uff0c\u5e76\u63d0\u51fa\u4e86\u590d\u6742\u6027\u5dee\u8ddd\u5b9a\u7406\u3001\u7cbe\u786e\u8fb9\u754c\u4ee5\u53ca\u5c40\u90e8\u4e0e\u5168\u5c40\u53ef\u89e3\u91ca\u6027\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002", "motivation": "\u7814\u7a76AI\u53ef\u89e3\u91ca\u6027\u7684\u7406\u8bba\u9650\u5236\uff0c\u4e3a\u8bbe\u8ba1\u548c\u8bc4\u4f30\u53ef\u89e3\u91caAI\u7cfb\u7edf\u63d0\u4f9b\u7406\u8bba\u57fa\u7840\u3002", "method": "\u4f7f\u7528Kolmogorov\u590d\u6742\u6027\u91cf\u5316\u89e3\u91ca\u590d\u6742\u6027\uff0c\u63d0\u51fa\u590d\u6742\u6027\u5dee\u8ddd\u5b9a\u7406\u548c\u7cbe\u786e\u8fb9\u754c\uff0c\u5206\u6790\u5c40\u90e8\u4e0e\u5168\u5c40\u53ef\u89e3\u91ca\u6027\u7684\u5dee\u5f02\u3002", "result": "\u8bc1\u660e\u89e3\u91ca\u590d\u6742\u6027\u968f\u8f93\u5165\u7ef4\u5ea6\u6307\u6570\u589e\u957f\uff0c\u4f46\u5bf9Lipschitz\u51fd\u6570\u8bef\u5dee\u5bb9\u5fcd\u5ea6\u591a\u9879\u5f0f\u589e\u957f\uff1b\u5c40\u90e8\u89e3\u91ca\u5728\u76f8\u5173\u533a\u57df\u53ef\u663e\u8457\u7b80\u5316\u3002", "conclusion": "\u63ed\u793a\u4e86AI\u53ef\u89e3\u91ca\u6027\u7684\u57fa\u672c\u9650\u5236\uff0c\u5f3a\u8c03\u4e86\u5728\u8bbe\u8ba1\u3001\u8bc4\u4f30\u548c\u76d1\u7ba1\u53ef\u89e3\u91caAI\u7cfb\u7edf\u65f6\u9700\u8003\u8651\u7684\u7406\u8bba\u95ee\u9898\u3002"}}
{"id": "2504.20379", "pdf": "https://arxiv.org/pdf/2504.20379", "abs": "https://arxiv.org/abs/2504.20379", "authors": ["Jongwon Lee", "Timothy Bretl"], "title": "GSFeatLoc: Visual Localization Using Feature Correspondence on 3D Gaussian Splatting", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "In this paper, we present a method for localizing a query image with respect\nto a precomputed 3D Gaussian Splatting (3DGS) scene representation. First, the\nmethod uses 3DGS to render a synthetic RGBD image at some initial pose\nestimate. Second, it establishes 2D-2D correspondences between the query image\nand this synthetic image. Third, it uses the depth map to lift the 2D-2D\ncorrespondences to 2D-3D correspondences and solves a perspective-n-point (PnP)\nproblem to produce a final pose estimate. Results from evaluation across three\nexisting datasets with 38 scenes and over 2,700 test images show that our\nmethod significantly reduces both inference time (by over two orders of\nmagnitude, from more than 10 seconds to as fast as 0.1 seconds) and estimation\nerror compared to baseline methods that use photometric loss minimization.\nResults also show that our method tolerates large errors in the initial pose\nestimate of up to 55{\\deg} in rotation and 1.1 units in translation (normalized\nby scene scale), achieving final pose errors of less than 5{\\deg} in rotation\nand 0.05 units in translation on 90% of images from the Synthetic NeRF and\nMip-NeRF360 datasets and on 42% of images from the more challenging Tanks and\nTemples dataset.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e3D\u9ad8\u65af\u6cfc\u6e85\uff083DGS\uff09\u573a\u666f\u8868\u793a\u7684\u67e5\u8be2\u56fe\u50cf\u5b9a\u4f4d\u65b9\u6cd5\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u63a8\u7406\u65f6\u95f4\u548c\u4f30\u8ba1\u8bef\u5dee\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u5728\u5b9a\u4f4d\u67e5\u8be2\u56fe\u50cf\u65f6\u63a8\u7406\u65f6\u95f4\u957f\u548c\u8bef\u5dee\u5927\u7684\u95ee\u9898\u3002", "method": "1. \u4f7f\u75283DGS\u6e32\u67d3\u5408\u6210RGBD\u56fe\u50cf\uff1b2. \u5efa\u7acb\u67e5\u8be2\u56fe\u50cf\u4e0e\u5408\u6210\u56fe\u50cf\u76842D-2D\u5bf9\u5e94\u5173\u7cfb\uff1b3. \u901a\u8fc7\u6df1\u5ea6\u56fe\u5c062D-2D\u5bf9\u5e94\u63d0\u5347\u4e3a2D-3D\u5bf9\u5e94\uff0c\u5e76\u6c42\u89e3PnP\u95ee\u9898\u5f97\u5230\u6700\u7ec8\u4f4d\u59ff\u4f30\u8ba1\u3002", "result": "\u5728\u4e09\u4e2a\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\uff0c\u63a8\u7406\u65f6\u95f4\u4ece10\u79d2\u964d\u81f30.1\u79d2\uff0c\u4f4d\u59ff\u8bef\u5dee\u663e\u8457\u964d\u4f4e\uff0c\u4e14\u5bf9\u521d\u59cb\u4f4d\u59ff\u4f30\u8ba1\u7684\u5927\u8bef\u5dee\u5177\u6709\u9c81\u68d2\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u901f\u5ea6\u548c\u7cbe\u5ea6\u4e0a\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u573a\u666f\u3002"}}
{"id": "2504.20756", "pdf": "https://arxiv.org/pdf/2504.20756", "abs": "https://arxiv.org/abs/2504.20756", "authors": ["Moirangthem Tiken Singh"], "title": "Graph-Based Fault Diagnosis for Rotating Machinery: Adaptive Segmentation and Structural Feature Integration", "categories": ["cs.AI"], "comment": null, "summary": "This paper proposes a novel graph-based framework for robust and\ninterpretable multiclass fault diagnosis in rotating machinery. The method\nintegrates entropy-optimized signal segmentation, time-frequency feature\nextraction, and graph-theoretic modeling to transform vibration signals into\nstructured representations suitable for classification. Graph metrics, such as\naverage shortest path length, modularity, and spectral gap, are computed and\ncombined with local features to capture global and segment-level fault\ncharacteristics. The proposed method achieves high diagnostic accuracy when\nevaluated on two benchmark datasets, the CWRU bearing dataset (under 0-3 HP\nloads) and the SU gearbox and bearing datasets (under different speed-load\nconfigurations). Classification scores reach up to 99.8% accuracy on Case\nWestern Reserve University (CWRU) and 100% accuracy on the Southeast University\ndatasets using a logistic regression classifier. Furthermore, the model\nexhibits strong noise resilience, maintaining over 95.4% accuracy at high noise\nlevels (standard deviation = 0.5), and demonstrates excellent cross-domain\ntransferability with up to 99.7% F1-score in load-transfer scenarios. Compared\nto traditional techniques, this approach requires no deep learning\narchitecture, enabling lower complexity while ensuring interpretability. The\nresults confirm the method's scalability, reliability, and potential for\nreal-time deployment in industrial diagnostics.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u56fe\u7684\u65b0\u578b\u6846\u67b6\uff0c\u7528\u4e8e\u65cb\u8f6c\u673a\u68b0\u7684\u591a\u7c7b\u6545\u969c\u8bca\u65ad\uff0c\u5177\u6709\u9c81\u68d2\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf\u65b9\u6cd5\u5728\u65cb\u8f6c\u673a\u68b0\u6545\u969c\u8bca\u65ad\u4e2d\u590d\u6742\u5ea6\u9ad8\u3001\u53ef\u89e3\u91ca\u6027\u5dee\u7684\u95ee\u9898\u3002", "method": "\u7ed3\u5408\u71b5\u4f18\u5316\u4fe1\u53f7\u5206\u5272\u3001\u65f6\u9891\u7279\u5f81\u63d0\u53d6\u548c\u56fe\u8bba\u5efa\u6a21\uff0c\u5c06\u632f\u52a8\u4fe1\u53f7\u8f6c\u6362\u4e3a\u7ed3\u6784\u5316\u8868\u793a\uff0c\u5e76\u8ba1\u7b97\u56fe\u5ea6\u91cf\u4ee5\u6355\u83b7\u6545\u969c\u7279\u5f81\u3002", "result": "\u5728CWRU\u548cSU\u6570\u636e\u96c6\u4e0a\u5206\u522b\u8fbe\u523099.8%\u548c100%\u7684\u51c6\u786e\u7387\uff0c\u4e14\u5bf9\u566a\u58f0\u548c\u8de8\u57df\u573a\u666f\u8868\u73b0\u51fa\u5f3a\u9c81\u68d2\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u65e0\u9700\u6df1\u5ea6\u5b66\u4e60\u67b6\u6784\uff0c\u590d\u6742\u5ea6\u4f4e\u4e14\u53ef\u89e3\u91ca\u6027\u5f3a\uff0c\u9002\u7528\u4e8e\u5de5\u4e1a\u5b9e\u65f6\u8bca\u65ad\u3002"}}
{"id": "2504.20383", "pdf": "https://arxiv.org/pdf/2504.20383", "abs": "https://arxiv.org/abs/2504.20383", "authors": ["Shiyin Jiang", "Zhenghao Chen", "Minghao Han", "Xingyu Zhou", "Leheng Zhang", "Shuhang Gu"], "title": "Neural Stereo Video Compression with Hybrid Disparity Compensation", "categories": ["cs.CV", "eess.IV"], "comment": null, "summary": "Disparity compensation represents the primary strategy in stereo video\ncompression (SVC) for exploiting cross-view redundancy. These mechanisms can be\nbroadly categorized into two types: one that employs explicit horizontal\nshifting, and another that utilizes an implicit cross-attention mechanism to\nreduce cross-view disparity redundancy. In this work, we propose a hybrid\ndisparity compensation (HDC) strategy that leverages explicit pixel\ndisplacement as a robust prior feature to simplify optimization and perform\nimplicit cross-attention mechanisms for subsequent warping operations, thereby\ncapturing a broader range of disparity information. Specifically, HDC first\ncomputes a similarity map by fusing the horizontally shifted cross-view\nfeatures to capture pixel displacement information. This similarity map is then\nnormalized into an \"explicit pixel-wise attention score\" to perform the\ncross-attention mechanism, implicitly aligning features from one view to\nanother. Building upon HDC, we introduce a novel end-to-end optimized neural\nstereo video compression framework, which integrates HDC-based modules into key\ncoding operations, including cross-view feature extraction and reconstruction\n(HDC-FER) and cross-view entropy modeling (HDC-EM). Extensive experiments on\nSVC benchmarks, including KITTI 2012, KITTI 2015, and Nagoya, which cover both\nautonomous driving and general scenes, demonstrate that our framework\noutperforms both neural and traditional SVC methodologies.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6df7\u5408\u89c6\u5dee\u8865\u507f\uff08HDC\uff09\u7b56\u7565\uff0c\u7ed3\u5408\u663e\u5f0f\u548c\u9690\u5f0f\u65b9\u6cd5\u4f18\u5316\u7acb\u4f53\u89c6\u9891\u538b\u7f29\uff0c\u5e76\u6784\u5efa\u4e86\u4e00\u4e2a\u7aef\u5230\u7aef\u4f18\u5316\u7684\u795e\u7ecf\u6846\u67b6\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u7acb\u4f53\u89c6\u9891\u538b\u7f29\u4e2d\u89c6\u5dee\u8865\u507f\u662f\u4e3b\u8981\u7b56\u7565\uff0c\u73b0\u6709\u65b9\u6cd5\u5206\u4e3a\u663e\u5f0f\u6c34\u5e73\u4f4d\u79fb\u548c\u9690\u5f0f\u4ea4\u53c9\u6ce8\u610f\u529b\u673a\u5236\u3002\u4e3a\u4e86\u66f4\u5168\u9762\u5730\u6355\u6349\u89c6\u5dee\u4fe1\u606f\uff0c\u63d0\u51fa\u6df7\u5408\u7b56\u7565\u3002", "method": "HDC\u7ed3\u5408\u663e\u5f0f\u50cf\u7d20\u4f4d\u79fb\u548c\u9690\u5f0f\u4ea4\u53c9\u6ce8\u610f\u529b\u673a\u5236\uff0c\u901a\u8fc7\u76f8\u4f3c\u6027\u56fe\u548c\u5f52\u4e00\u5316\u6ce8\u610f\u529b\u5206\u6570\u5b9e\u73b0\u7279\u5f81\u5bf9\u9f50\u3002\u6784\u5efa\u4e86HDC-FER\u548cHDC-EM\u6a21\u5757\u3002", "result": "\u5728KITTI 2012\u3001KITTI 2015\u548cNagoya\u7b49\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u6027\u80fd\u4f18\u4e8e\u4f20\u7edf\u548c\u795e\u7ecfSVC\u65b9\u6cd5\u3002", "conclusion": "HDC\u7b56\u7565\u6709\u6548\u63d0\u5347\u4e86\u7acb\u4f53\u89c6\u9891\u538b\u7f29\u6027\u80fd\uff0c\u9002\u7528\u4e8e\u81ea\u52a8\u9a7e\u9a76\u548c\u901a\u7528\u573a\u666f\u3002"}}
{"id": "2504.20784", "pdf": "https://arxiv.org/pdf/2504.20784", "abs": "https://arxiv.org/abs/2504.20784", "authors": ["Malte Luttermann", "Jan Speller", "Marcel Gehrke", "Tanya Braun", "Ralf M\u00f6ller", "Mattis Hartwig"], "title": "Approximate Lifted Model Construction", "categories": ["cs.AI", "cs.DS", "cs.LG"], "comment": "Extended version of paper accepted to the Proceedings of the 34th\n  International Joint Conference on Artificial Intelligence (IJCAI-2025)", "summary": "Probabilistic relational models such as parametric factor graphs enable\nefficient (lifted) inference by exploiting the indistinguishability of objects.\nIn lifted inference, a representative of indistinguishable objects is used for\ncomputations. To obtain a relational (i.e., lifted) representation, the\nAdvanced Colour Passing (ACP) algorithm is the state of the art. The ACP\nalgorithm, however, requires underlying distributions, encoded as\npotential-based factorisations, to exactly match to identify and exploit\nindistinguishabilities. Hence, ACP is unsuitable for practical applications\nwhere potentials learned from data inevitably deviate even if associated\nobjects are indistinguishable. To mitigate this problem, we introduce the\n$\\varepsilon$-Advanced Colour Passing ($\\varepsilon$-ACP) algorithm, which\nallows for a deviation of potentials depending on a hyperparameter\n$\\varepsilon$. $\\varepsilon$-ACP efficiently uncovers and exploits\nindistinguishabilities that are not exact. We prove that the approximation\nerror induced by $\\varepsilon$-ACP is strictly bounded and our experiments show\nthat the approximation error is close to zero in practice.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdb\u7684\u7b97\u6cd5\u03b5-ACP\uff0c\u7528\u4e8e\u5904\u7406\u6982\u7387\u5173\u7cfb\u6a21\u578b\u4e2d\u56e0\u6570\u636e\u5b66\u4e60\u5bfc\u81f4\u7684\u6f5c\u5728\u504f\u5dee\u95ee\u9898\uff0c\u5141\u8bb8\u901a\u8fc7\u8d85\u53c2\u6570\u03b5\u63a7\u5236\u504f\u5dee\u8303\u56f4\uff0c\u540c\u65f6\u4e25\u683c\u9650\u5236\u8fd1\u4f3c\u8bef\u5dee\u3002", "motivation": "\u4f20\u7edf\u7684ACP\u7b97\u6cd5\u8981\u6c42\u6f5c\u5728\u5206\u5e03\u5b8c\u5168\u5339\u914d\u624d\u80fd\u5229\u7528\u4e0d\u53ef\u533a\u5206\u6027\uff0c\u4f46\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\uff0c\u6570\u636e\u5b66\u4e60\u7684\u6f5c\u5728\u5206\u5e03\u96be\u514d\u5b58\u5728\u504f\u5dee\uff0c\u5bfc\u81f4ACP\u4e0d\u9002\u7528\u3002", "method": "\u63d0\u51fa\u4e86\u03b5-ACP\u7b97\u6cd5\uff0c\u901a\u8fc7\u5f15\u5165\u8d85\u53c2\u6570\u03b5\uff0c\u5141\u8bb8\u6f5c\u5728\u5206\u5e03\u5b58\u5728\u4e00\u5b9a\u504f\u5dee\uff0c\u4ece\u800c\u9ad8\u6548\u5730\u8bc6\u522b\u548c\u5229\u7528\u975e\u7cbe\u786e\u7684\u4e0d\u53ef\u533a\u5206\u6027\u3002", "result": "\u7406\u8bba\u8bc1\u660e\u03b5-ACP\u7684\u8fd1\u4f3c\u8bef\u5dee\u4e25\u683c\u6709\u754c\uff0c\u5b9e\u9a8c\u8868\u660e\u5b9e\u9645\u8bef\u5dee\u63a5\u8fd1\u4e8e\u96f6\u3002", "conclusion": "\u03b5-ACP\u7b97\u6cd5\u89e3\u51b3\u4e86ACP\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u6982\u7387\u5173\u7cfb\u6a21\u578b\u7684\u63a8\u7406\u63d0\u4f9b\u4e86\u66f4\u5b9e\u7528\u7684\u5de5\u5177\u3002"}}
{"id": "2504.20384", "pdf": "https://arxiv.org/pdf/2504.20384", "abs": "https://arxiv.org/abs/2504.20384", "authors": ["Yanan Guo", "Wenhui Dong", "Jun Song", "Shiding Zhu", "Xuan Zhang", "Hanqing Yang", "Yingbo Wang", "Yang Du", "Xianing Chen", "Bo Zheng"], "title": "FiLA-Video: Spatio-Temporal Compression for Fine-Grained Long Video Understanding", "categories": ["cs.CV"], "comment": "8 pages, 6 figures", "summary": "Recent advancements in video understanding within visual large language\nmodels (VLLMs) have led to notable progress. However, the complexity of video\ndata and contextual processing limitations still hinder long-video\ncomprehension. A common approach is video feature compression to reduce token\ninput to large language models, yet many methods either fail to prioritize\nessential features, leading to redundant inter-frame information, or introduce\ncomputationally expensive modules.To address these issues, we propose\nFiLA(Fine-grained Vision Language Model)-Video, a novel framework that\nleverages a lightweight dynamic-weight multi-frame fusion strategy, which\nadaptively integrates multiple frames into a single representation while\npreserving key video information and reducing computational costs. To enhance\nframe selection for fusion, we introduce a keyframe selection strategy,\neffectively identifying informative frames from a larger pool for improved\nsummarization. Additionally, we present a simple yet effective long-video\ntraining data generation strategy, boosting model performance without extensive\nmanual annotation. Experimental results demonstrate that FiLA-Video achieves\nsuperior efficiency and accuracy in long-video comprehension compared to\nexisting methods.", "AI": {"tldr": "FiLA-Video\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u52a8\u6001\u6743\u91cd\u591a\u5e27\u878d\u5408\u7b56\u7565\uff0c\u7528\u4e8e\u957f\u89c6\u9891\u7406\u89e3\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u6574\u5408\u591a\u5e27\u4fe1\u606f\u5e76\u4fdd\u7559\u5173\u952e\u5185\u5bb9\uff0c\u540c\u65f6\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u3002", "motivation": "\u5f53\u524d\u89c6\u9891\u7406\u89e3\u65b9\u6cd5\u5728\u957f\u89c6\u9891\u5904\u7406\u4e2d\u5b58\u5728\u7279\u5f81\u5197\u4f59\u6216\u8ba1\u7b97\u6210\u672c\u9ad8\u7684\u95ee\u9898\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u91c7\u7528\u52a8\u6001\u6743\u91cd\u591a\u5e27\u878d\u5408\u7b56\u7565\u548c\u5173\u952e\u5e27\u9009\u62e9\u65b9\u6cd5\uff0c\u7ed3\u5408\u81ea\u52a8\u751f\u6210\u7684\u957f\u89c6\u9891\u8bad\u7ec3\u6570\u636e\u3002", "result": "FiLA-Video\u5728\u957f\u89c6\u9891\u7406\u89e3\u4e2d\u8868\u73b0\u51fa\u66f4\u9ad8\u7684\u6548\u7387\u548c\u51c6\u786e\u6027\u3002", "conclusion": "FiLA-Video\u901a\u8fc7\u8f7b\u91cf\u7ea7\u8bbe\u8ba1\u548c\u81ea\u9002\u5e94\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86\u957f\u89c6\u9891\u7406\u89e3\u7684\u6027\u80fd\u3002"}}
{"id": "2504.20797", "pdf": "https://arxiv.org/pdf/2504.20797", "abs": "https://arxiv.org/abs/2504.20797", "authors": ["Renye Zhang", "Yimin Yin", "Jinghua Zhang"], "title": "Partitioned Memory Storage Inspired Few-Shot Class-Incremental learning", "categories": ["cs.AI"], "comment": null, "summary": "Current mainstream deep learning techniques exhibit an over-reliance on\nextensive training data and a lack of adaptability to the dynamic world,\nmarking a considerable disparity from human intelligence. To bridge this gap,\nFew-Shot Class-Incremental Learning (FSCIL) has emerged, focusing on continuous\nlearning of new categories with limited samples without forgetting old\nknowledge. Existing FSCIL studies typically use a single model to learn\nknowledge across all sessions, inevitably leading to the stability-plasticity\ndilemma. Unlike machines, humans store varied knowledge in different cerebral\ncortices. Inspired by this characteristic, our paper aims to develop a method\nthat learns independent models for each session. It can inherently prevent\ncatastrophic forgetting. During the testing stage, our method integrates\nUncertainty Quantification (UQ) for model deployment. Our method provides a\nfresh viewpoint for FSCIL and demonstrates the state-of-the-art performance on\nCIFAR-100 and mini-ImageNet datasets.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cdFew-Shot Class-Incremental Learning\uff08FSCIL\uff09\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u4e3a\u6bcf\u4e2a\u4f1a\u8bdd\u5b66\u4e60\u72ec\u7acb\u6a21\u578b\u907f\u514d\u707e\u96be\u6027\u9057\u5fd8\uff0c\u5e76\u5f15\u5165\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u6df1\u5ea6\u5b66\u4e60\u4f9d\u8d56\u5927\u91cf\u6570\u636e\u4e14\u7f3a\u4e4f\u52a8\u6001\u9002\u5e94\u6027\uff0c\u4e0e\u4eba\u7c7b\u667a\u80fd\u5dee\u8ddd\u5927\u3002FSCIL\u65e8\u5728\u7528\u5c11\u91cf\u6837\u672c\u6301\u7eed\u5b66\u4e60\u65b0\u7c7b\u522b\u800c\u4e0d\u9057\u5fd8\u65e7\u77e5\u8bc6\u3002", "method": "\u4e3a\u6bcf\u4e2a\u4f1a\u8bdd\u5b66\u4e60\u72ec\u7acb\u6a21\u578b\uff0c\u907f\u514d\u7a33\u5b9a\u6027-\u53ef\u5851\u6027\u56f0\u5883\uff1b\u6d4b\u8bd5\u9636\u6bb5\u5f15\u5165\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\uff08UQ\uff09\u8fdb\u884c\u6a21\u578b\u90e8\u7f72\u3002", "result": "\u5728CIFAR-100\u548cmini-ImageNet\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3aFSCIL\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u707e\u96be\u6027\u9057\u5fd8\u95ee\u9898\u3002"}}
{"id": "2504.20409", "pdf": "https://arxiv.org/pdf/2504.20409", "abs": "https://arxiv.org/abs/2504.20409", "authors": ["Jingfeng Guo", "Jinnan Chen", "Weikai Chen", "Zhenyu Sun", "Lanjiong Li", "Baozhu Zhao", "Lingting Zhu", "Xin Wang", "Qi Liu"], "title": "GarmentX: Autoregressive Parametric Representations for High-Fidelity 3D Garment Generation", "categories": ["cs.CV"], "comment": null, "summary": "This work presents GarmentX, a novel framework for generating diverse,\nhigh-fidelity, and wearable 3D garments from a single input image. Traditional\ngarment reconstruction methods directly predict 2D pattern edges and their\nconnectivity, an overly unconstrained approach that often leads to severe\nself-intersections and physically implausible garment structures. In contrast,\nGarmentX introduces a structured and editable parametric representation\ncompatible with GarmentCode, ensuring that the decoded sewing patterns always\nform valid, simulation-ready 3D garments while allowing for intuitive\nmodifications of garment shape and style. To achieve this, we employ a masked\nautoregressive model that sequentially predicts garment parameters, leveraging\nautoregressive modeling for structured generation while mitigating\ninconsistencies in direct pattern prediction. Additionally, we introduce\nGarmentX dataset, a large-scale dataset of 378,682 garment parameter-image\npairs, constructed through an automatic data generation pipeline that\nsynthesizes diverse and high-quality garment images conditioned on parametric\ngarment representations. Through integrating our method with GarmentX dataset,\nwe achieve state-of-the-art performance in geometric fidelity and input image\nalignment, significantly outperforming prior approaches. We will release\nGarmentX dataset upon publication.", "AI": {"tldr": "GarmentX\u662f\u4e00\u4e2a\u4ece\u5355\u5f20\u8f93\u5165\u56fe\u50cf\u751f\u6210\u591a\u6837\u3001\u9ad8\u4fdd\u771f\u4e14\u53ef\u7a7f\u62343D\u670d\u88c5\u7684\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u53c2\u6570\u5316\u8868\u793a\u548c\u81ea\u56de\u5f52\u6a21\u578b\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u7684\u81ea\u76f8\u4ea4\u548c\u7269\u7406\u4e0d\u5408\u7406\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u670d\u88c5\u91cd\u5efa\u65b9\u6cd5\u76f4\u63a5\u9884\u6d4b2D\u56fe\u6848\u8fb9\u7f18\u53ca\u5176\u8fde\u63a5\u6027\uff0c\u5bfc\u81f4\u81ea\u76f8\u4ea4\u548c\u7269\u7406\u4e0d\u5408\u7406\u7ed3\u6784\uff0cGarmentX\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u91c7\u7528\u7ed3\u6784\u5316\u53ef\u7f16\u8f91\u53c2\u6570\u5316\u8868\u793a\uff08\u517c\u5bb9GarmentCode\uff09\u548c\u63a9\u7801\u81ea\u56de\u5f52\u6a21\u578b\uff0c\u987a\u5e8f\u9884\u6d4b\u670d\u88c5\u53c2\u6570\uff0c\u786e\u4fdd\u751f\u6210\u6709\u6548\u76843D\u670d\u88c5\u3002", "result": "\u901a\u8fc7GarmentX\u6570\u636e\u96c6\uff08378,682\u5bf9\u53c2\u6570-\u56fe\u50cf\uff09\u548c\u65b9\u6cd5\u7684\u7ed3\u5408\uff0c\u5b9e\u73b0\u4e86\u51e0\u4f55\u4fdd\u771f\u5ea6\u548c\u8f93\u5165\u56fe\u50cf\u5bf9\u9f50\u7684\u5148\u8fdb\u6027\u80fd\u3002", "conclusion": "GarmentX\u5728\u751f\u6210\u9ad8\u8d28\u91cf3D\u670d\u88c5\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u5c06\u516c\u5f00\u6570\u636e\u96c6\u3002"}}
{"id": "2504.20828", "pdf": "https://arxiv.org/pdf/2504.20828", "abs": "https://arxiv.org/abs/2504.20828", "authors": ["Azam Ikram", "Xiang Li", "Sameh Elnikety", "Saurabh Bagchi"], "title": "Ascendra: Dynamic Request Prioritization for Efficient LLM Serving", "categories": ["cs.AI"], "comment": null, "summary": "The rapid advancement of Large Language Models (LLMs) has driven the need for\nmore efficient serving strategies. In this context, efficiency refers to the\nproportion of requests that meet their Service Level Objectives (SLOs),\nparticularly for Time To First Token (TTFT) and Time Between Tokens (TBT).\nHowever, existing systems often prioritize one metric at the cost of the other.\nWe present Ascendra, an LLM serving system designed to meet both TTFT and TBT\nSLOs simultaneously. The core insight behind Ascendra is that a request's\nurgency evolves as it approaches its deadline. To leverage this, Ascendra\npartitions GPU resources into two types of instances: low-priority and\nhigh-priority. Low-priority instances maximize throughput by processing\nrequests out of arrival order, but at the risk of request starvation. To\naddress this, Ascendra employs a performance model to predict requests at risk\nof missing their SLOs and proactively offloads them to high-priority instances.\nHigh-priority instances are optimized for low-latency execution and handle\nurgent requests nearing their deadlines. This partitioned architecture enables\nAscendra to effectively balance high throughput and low latency. Extensive\nevaluation shows that Ascendra improves system throughput by up to 1.7x\ncompared to vLLM and Sarathi-Serve while meeting both TTFT and TBT SLOs.", "AI": {"tldr": "Ascendra\u662f\u4e00\u79cdLLM\u670d\u52a1\u7cfb\u7edf\uff0c\u901a\u8fc7\u5206\u533aGPU\u8d44\u6e90\u4e3a\u9ad8\u4f4e\u4f18\u5148\u7ea7\u5b9e\u4f8b\uff0c\u540c\u65f6\u6ee1\u8db3TTFT\u548cTBT\u7684SLO\u8981\u6c42\uff0c\u63d0\u5347\u541e\u5410\u91cf1.7\u500d\u3002", "motivation": "\u73b0\u6709\u7cfb\u7edf\u5e38\u727a\u7272\u4e00\u4e2a\u6307\u6807\u4ee5\u6ee1\u8db3\u53e6\u4e00\u4e2a\uff0c\u65e0\u6cd5\u540c\u65f6\u4f18\u5316TTFT\u548cTBT\u7684SLO\u3002", "method": "Ascendra\u5c06GPU\u8d44\u6e90\u5206\u4e3a\u9ad8\u4f4e\u4f18\u5148\u7ea7\u5b9e\u4f8b\uff0c\u4f4e\u4f18\u5148\u7ea7\u5b9e\u4f8b\u4f18\u5316\u541e\u5410\u91cf\uff0c\u9ad8\u4f18\u5148\u7ea7\u5b9e\u4f8b\u5904\u7406\u7d27\u6025\u8bf7\u6c42\u3002", "result": "\u76f8\u6bd4vLLM\u548cSarathi-Serve\uff0cAscendra\u541e\u5410\u91cf\u63d0\u53471.7\u500d\uff0c\u540c\u65f6\u6ee1\u8db3TTFT\u548cTBT\u7684SLO\u3002", "conclusion": "Ascendra\u901a\u8fc7\u52a8\u6001\u8d44\u6e90\u5206\u533a\uff0c\u6709\u6548\u5e73\u8861\u9ad8\u541e\u5410\u548c\u4f4e\u5ef6\u8fdf\uff0c\u4f18\u4e8e\u73b0\u6709\u7cfb\u7edf\u3002"}}
{"id": "2504.20419", "pdf": "https://arxiv.org/pdf/2504.20419", "abs": "https://arxiv.org/abs/2504.20419", "authors": ["Konstantinos I. Roumeliotis", "Ranjan Sapkota", "Manoj Karkee", "Nikolaos D. Tselikas", "Dimitrios K. Nasiopoulos"], "title": "Plant Disease Detection through Multimodal Large Language Models and Convolutional Neural Networks", "categories": ["cs.CV"], "comment": null, "summary": "Automation in agriculture plays a vital role in addressing challenges related\nto crop monitoring and disease management, particularly through early detection\nsystems. This study investigates the effectiveness of combining multimodal\nLarge Language Models (LLMs), specifically GPT-4o, with Convolutional Neural\nNetworks (CNNs) for automated plant disease classification using leaf imagery.\nLeveraging the PlantVillage dataset, we systematically evaluate model\nperformance across zero-shot, few-shot, and progressive fine-tuning scenarios.\nA comparative analysis between GPT-4o and the widely used ResNet-50 model was\nconducted across three resolutions (100, 150, and 256 pixels) and two plant\nspecies (apple and corn). Results indicate that fine-tuned GPT-4o models\nachieved slightly better performance compared to the performance of ResNet-50,\nachieving up to 98.12% classification accuracy on apple leaf images, compared\nto 96.88% achieved by ResNet-50, with improved generalization and near-zero\ntraining loss. However, zero-shot performance of GPT-4o was significantly\nlower, underscoring the need for minimal training. Additional evaluations on\ncross-resolution and cross-plant generalization revealed the models'\nadaptability and limitations when applied to new domains. The findings\nhighlight the promise of integrating multimodal LLMs into automated disease\ndetection pipelines, enhancing the scalability and intelligence of precision\nagriculture systems while reducing the dependence on large, labeled datasets\nand high-resolution sensor infrastructure. Large Language Models, Vision\nLanguage Models, LLMs and CNNs, Disease Detection with Vision Language Models,\nVLMs", "AI": {"tldr": "\u7814\u7a76\u7ed3\u5408GPT-4o\u548cCNN\u8fdb\u884c\u690d\u7269\u75c5\u5bb3\u5206\u7c7b\uff0c\u53d1\u73b0\u5fae\u8c03\u540e\u7684GPT-4o\u6027\u80fd\u7565\u4f18\u4e8eResNet-50\uff0c\u4f46\u96f6\u6837\u672c\u8868\u73b0\u8f83\u5dee\u3002", "motivation": "\u89e3\u51b3\u519c\u4e1a\u81ea\u52a8\u5316\u4e2d\u4f5c\u7269\u76d1\u6d4b\u548c\u75c5\u5bb3\u7ba1\u7406\u7684\u6311\u6218\uff0c\u5c24\u5176\u662f\u65e9\u671f\u68c0\u6d4b\u7cfb\u7edf\u3002", "method": "\u4f7f\u7528PlantVillage\u6570\u636e\u96c6\uff0c\u7ed3\u5408GPT-4o\u548cCNN\uff0c\u8bc4\u4f30\u96f6\u6837\u672c\u3001\u5c11\u6837\u672c\u548c\u6e10\u8fdb\u5fae\u8c03\u573a\u666f\u3002", "result": "\u5fae\u8c03GPT-4o\u5728\u82f9\u679c\u53f6\u56fe\u50cf\u4e0a\u8fbe\u523098.12%\u51c6\u786e\u7387\uff0c\u4f18\u4e8eResNet-50\u768496.88%\uff0c\u4f46\u96f6\u6837\u672c\u8868\u73b0\u4e0d\u4f73\u3002", "conclusion": "\u591a\u6a21\u6001LLMs\u5728\u75c5\u5bb3\u68c0\u6d4b\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u53ef\u63d0\u5347\u7cbe\u51c6\u519c\u4e1a\u7684\u667a\u80fd\u5316\u548c\u6269\u5c55\u6027\u3002"}}
{"id": "2504.20846", "pdf": "https://arxiv.org/pdf/2504.20846", "abs": "https://arxiv.org/abs/2504.20846", "authors": ["Robert F. Downey", "S. S. Ravi"], "title": "Disjunctive and Conjunctive Normal Form Explanations of Clusters Using Auxiliary Information", "categories": ["cs.AI", "I.2"], "comment": null, "summary": "We consider generating post-hoc explanations of clusters generated from\nvarious datasets using auxiliary information which was not used by clustering\nalgorithms. Following terminology used in previous work, we refer to the\nauxiliary information as tags. Our focus is on two forms of explanations,\nnamely disjunctive form (where the explanation for a cluster consists of a set\nof tags) and a two-clause conjunctive normal form (CNF) explanation (where the\nexplanation consists of two sets of tags, combined through the AND operator).\nWe use integer linear programming (ILP) as well as heuristic methods to\ngenerate these explanations. We experiment with a variety of datasets and\ndiscuss the insights obtained from our explanations. We also present\nexperimental results regarding the scalability of our explanation methods.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u5982\u4f55\u5229\u7528\u672a\u7528\u4e8e\u805a\u7c7b\u7684\u8f85\u52a9\u4fe1\u606f\uff08\u6807\u7b7e\uff09\u751f\u6210\u805a\u7c7b\u7684\u4e8b\u540e\u89e3\u91ca\uff0c\u5305\u62ec\u6790\u53d6\u5f62\u5f0f\u548c\u4e24\u5b50\u53e5\u5408\u53d6\u8303\u5f0f\uff08CNF\uff09\u89e3\u91ca\uff0c\u5e76\u91c7\u7528\u6574\u6570\u7ebf\u6027\u89c4\u5212\uff08ILP\uff09\u548c\u542f\u53d1\u5f0f\u65b9\u6cd5\u751f\u6210\u89e3\u91ca\u3002", "motivation": "\u63a2\u7d22\u5982\u4f55\u901a\u8fc7\u8f85\u52a9\u6807\u7b7e\u4e3a\u805a\u7c7b\u7ed3\u679c\u63d0\u4f9b\u53ef\u89e3\u91ca\u6027\uff0c\u5e2e\u52a9\u7406\u89e3\u805a\u7c7b\u80cc\u540e\u7684\u903b\u8f91\u3002", "method": "\u4f7f\u7528\u6574\u6570\u7ebf\u6027\u89c4\u5212\uff08ILP\uff09\u548c\u542f\u53d1\u5f0f\u65b9\u6cd5\u751f\u6210\u6790\u53d6\u5f62\u5f0f\u548c\u4e24\u5b50\u53e5CNF\u5f62\u5f0f\u7684\u89e3\u91ca\u3002", "result": "\u901a\u8fc7\u591a\u79cd\u6570\u636e\u96c6\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u5e76\u5c55\u793a\u4e86\u751f\u6210\u89e3\u91ca\u7684\u53ef\u6269\u5c55\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u751f\u6210\u805a\u7c7b\u89e3\u91ca\uff0c\u4e3a\u7406\u89e3\u805a\u7c7b\u7ed3\u679c\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\u3002"}}
{"id": "2504.20435", "pdf": "https://arxiv.org/pdf/2504.20435", "abs": "https://arxiv.org/abs/2504.20435", "authors": ["Love Panta", "Suraj Prasai", "Karishma Malla Vaidya", "Shyam Shrestha", "Suresh Manandhar"], "title": "AI Assisted Cervical Cancer Screening for Cytology Samples in Developing Countries", "categories": ["cs.CV"], "comment": null, "summary": "Cervical cancer remains a significant health challenge, with high incidence\nand mortality rates, particularly in transitioning countries. Conventional\nLiquid-Based Cytology(LBC) is a labor-intensive process, requires expert\npathologists and is highly prone to errors, highlighting the need for more\nefficient screening methods. This paper introduces an innovative approach that\nintegrates low-cost biological microscopes with our simple and efficient AI\nalgorithms for automated whole-slide analysis. Our system uses a motorized\nmicroscope to capture cytology images, which are then processed through an AI\npipeline involving image stitching, cell segmentation, and classification. We\nutilize the lightweight UNet-based model involving human-in-the-loop approach\nto train our segmentation model with minimal ROIs. CvT-based classification\nmodel, trained on the SIPaKMeD dataset, accurately categorizes five cell types.\nOur framework offers enhanced accuracy and efficiency in cervical cancer\nscreening compared to various state-of-art methods, as demonstrated by\ndifferent evaluation metrics.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u4f4e\u6210\u672c\u751f\u7269\u663e\u5fae\u955c\u548c\u9ad8\u6548AI\u7b97\u6cd5\u7684\u81ea\u52a8\u5316\u5bab\u9888\u764c\u7b5b\u67e5\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u51c6\u786e\u6027\u548c\u6548\u7387\u3002", "motivation": "\u5bab\u9888\u764c\u5728\u8f6c\u578b\u56fd\u5bb6\u53d1\u75c5\u7387\u9ad8\uff0c\u4f20\u7edf\u6db2\u57fa\u7ec6\u80de\u5b66\u68c0\u6d4b\uff08LBC\uff09\u52b3\u52a8\u5bc6\u96c6\u4e14\u6613\u51fa\u9519\uff0c\u4e9f\u9700\u66f4\u9ad8\u6548\u7684\u7b5b\u67e5\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528\u7535\u52a8\u663e\u5fae\u955c\u6355\u83b7\u7ec6\u80de\u56fe\u50cf\uff0c\u901a\u8fc7AI\u6d41\u7a0b\uff08\u56fe\u50cf\u62fc\u63a5\u3001\u7ec6\u80de\u5206\u5272\u548c\u5206\u7c7b\uff09\u5904\u7406\u3002\u91c7\u7528\u8f7b\u91cf\u7ea7UNet\u6a21\u578b\u548cCvT\u5206\u7c7b\u6a21\u578b\uff0c\u7ed3\u5408\u4eba\u673a\u4ea4\u4e92\u8bad\u7ec3\u3002", "result": "\u5728SIPaKMeD\u6570\u636e\u96c6\u4e0a\uff0c\u5206\u7c7b\u6a21\u578b\u51c6\u786e\u8bc6\u522b\u4e94\u79cd\u7ec6\u80de\u7c7b\u578b\uff0c\u7cfb\u7edf\u5728\u591a\u79cd\u8bc4\u4f30\u6307\u6807\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u5bab\u9888\u764c\u7b5b\u67e5\u63d0\u4f9b\u4e86\u66f4\u51c6\u786e\u3001\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5177\u6709\u5e7f\u6cdb\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2504.20879", "pdf": "https://arxiv.org/pdf/2504.20879", "abs": "https://arxiv.org/abs/2504.20879", "authors": ["Shivalika Singh", "Yiyang Nan", "Alex Wang", "Daniel D'Souza", "Sayash Kapoor", "Ahmet \u00dcst\u00fcn", "Sanmi Koyejo", "Yuntian Deng", "Shayne Longpre", "Noah Smith", "Beyza Ermis", "Marzieh Fadaee", "Sara Hooker"], "title": "The Leaderboard Illusion", "categories": ["cs.AI", "cs.CL", "cs.LG", "stat.ME"], "comment": "68 pages, 18 figures, 9 tables", "summary": "Measuring progress is fundamental to the advancement of any scientific field.\nAs benchmarks play an increasingly central role, they also grow more\nsusceptible to distortion. Chatbot Arena has emerged as the go-to leaderboard\nfor ranking the most capable AI systems. Yet, in this work we identify\nsystematic issues that have resulted in a distorted playing field. We find that\nundisclosed private testing practices benefit a handful of providers who are\nable to test multiple variants before public release and retract scores if\ndesired. We establish that the ability of these providers to choose the best\nscore leads to biased Arena scores due to selective disclosure of performance\nresults. At an extreme, we identify 27 private LLM variants tested by Meta in\nthe lead-up to the Llama-4 release. We also establish that proprietary closed\nmodels are sampled at higher rates (number of battles) and have fewer models\nremoved from the arena than open-weight and open-source alternatives. Both\nthese policies lead to large data access asymmetries over time. Providers like\nGoogle and OpenAI have received an estimated 19.2% and 20.4% of all data on the\narena, respectively. In contrast, a combined 83 open-weight models have only\nreceived an estimated 29.7% of the total data. We show that access to Chatbot\nArena data yields substantial benefits; even limited additional data can result\nin relative performance gains of up to 112% on the arena distribution, based on\nour conservative estimates. Together, these dynamics result in overfitting to\nArena-specific dynamics rather than general model quality. The Arena builds on\nthe substantial efforts of both the organizers and an open community that\nmaintains this valuable evaluation platform. We offer actionable\nrecommendations to reform the Chatbot Arena's evaluation framework and promote\nfairer, more transparent benchmarking for the field", "AI": {"tldr": "\u8bba\u6587\u6307\u51faChatbot Arena\u5728AI\u7cfb\u7edf\u6392\u540d\u4e2d\u5b58\u5728\u7cfb\u7edf\u6027\u504f\u5dee\uff0c\u5305\u62ec\u672a\u516c\u5f00\u7684\u79c1\u4eba\u6d4b\u8bd5\u5b9e\u8df5\u3001\u9009\u62e9\u6027\u62ab\u9732\u7ed3\u679c\u4ee5\u53ca\u6570\u636e\u8bbf\u95ee\u4e0d\u5bf9\u79f0\uff0c\u5bfc\u81f4\u8bc4\u5206\u5931\u771f\u3002", "motivation": "\u63ed\u793aChatbot Arena\u5728AI\u6a21\u578b\u6392\u540d\u4e2d\u7684\u4e0d\u516c\u5e73\u73b0\u8c61\uff0c\u63a8\u52a8\u66f4\u900f\u660e\u548c\u516c\u6b63\u7684\u57fa\u51c6\u6d4b\u8bd5\u3002", "method": "\u901a\u8fc7\u5206\u6790\u79c1\u4eba\u6d4b\u8bd5\u5b9e\u8df5\u3001\u9009\u62e9\u6027\u62ab\u9732\u548c\u6570\u636e\u8bbf\u95ee\u4e0d\u5bf9\u79f0\uff0c\u91cf\u5316\u5176\u5bf9\u8bc4\u5206\u7684\u5f71\u54cd\u3002", "result": "\u53d1\u73b0\u79c1\u4eba\u6d4b\u8bd5\u548c\u6570\u636e\u8bbf\u95ee\u4e0d\u5bf9\u79f0\u5bfc\u81f4\u8bc4\u5206\u504f\u5dee\uff0c\u5f00\u653e\u6a21\u578b\u5728Arena\u4e2d\u5904\u4e8e\u52a3\u52bf\u3002", "conclusion": "\u63d0\u51fa\u6539\u8fdb\u5efa\u8bae\uff0c\u4ee5\u4fc3\u8fdb\u66f4\u516c\u5e73\u548c\u900f\u660e\u7684\u57fa\u51c6\u6d4b\u8bd5\u3002"}}
{"id": "2504.20438", "pdf": "https://arxiv.org/pdf/2504.20438", "abs": "https://arxiv.org/abs/2504.20438", "authors": ["Ziyang Xu", "Kangsheng Duan", "Xiaolei Shen", "Zhifeng Ding", "Wenyu Liu", "Xiaohu Ruan", "Xiaoxin Chen", "Xinggang Wang"], "title": "PixelHacker: Image Inpainting with Structural and Semantic Consistency", "categories": ["cs.CV"], "comment": null, "summary": "Image inpainting is a fundamental research area between image editing and\nimage generation. Recent state-of-the-art (SOTA) methods have explored novel\nattention mechanisms, lightweight architectures, and context-aware modeling,\ndemonstrating impressive performance. However, they often struggle with complex\nstructure (e.g., texture, shape, spatial relations) and semantics (e.g., color\nconsistency, object restoration, and logical correctness), leading to artifacts\nand inappropriate generation. To address this challenge, we design a simple yet\neffective inpainting paradigm called latent categories guidance, and further\npropose a diffusion-based model named PixelHacker. Specifically, we first\nconstruct a large dataset containing 14 million image-mask pairs by annotating\nforeground and background (potential 116 and 21 categories, respectively).\nThen, we encode potential foreground and background representations separately\nthrough two fixed-size embeddings, and intermittently inject these features\ninto the denoising process via linear attention. Finally, by pre-training on\nour dataset and fine-tuning on open-source benchmarks, we obtain PixelHacker.\nExtensive experiments show that PixelHacker comprehensively outperforms the\nSOTA on a wide range of datasets (Places2, CelebA-HQ, and FFHQ) and exhibits\nremarkable consistency in both structure and semantics. Project page at\nhttps://hustvl.github.io/projects/PixelHacker.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aPixelHacker\u7684\u6269\u6563\u6a21\u578b\uff0c\u901a\u8fc7\u6f5c\u5728\u7c7b\u522b\u5f15\u5bfc\u673a\u5236\u6539\u8fdb\u56fe\u50cf\u4fee\u590d\u4efb\u52a1\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u590d\u6742\u7ed3\u6784\u548c\u8bed\u4e49\u4e00\u81f4\u6027\u4e0a\u7684\u4e0d\u8db3\u3002", "motivation": "\u73b0\u6709\u56fe\u50cf\u4fee\u590d\u65b9\u6cd5\u5728\u5904\u7406\u590d\u6742\u7ed3\u6784\u548c\u8bed\u4e49\u4e00\u81f4\u6027\u65f6\u8868\u73b0\u4e0d\u4f73\uff0c\u5bfc\u81f4\u751f\u6210\u7ed3\u679c\u5b58\u5728\u4f2a\u5f71\u548c\u4e0d\u5408\u7406\u5185\u5bb9\u3002", "method": "\u901a\u8fc7\u6784\u5efa\u5305\u542b1400\u4e07\u56fe\u50cf-\u63a9\u7801\u5bf9\u7684\u6570\u636e\u96c6\uff0c\u5e76\u8bbe\u8ba1\u6f5c\u5728\u7c7b\u522b\u5f15\u5bfc\u673a\u5236\uff0c\u5c06\u524d\u666f\u548c\u80cc\u666f\u7279\u5f81\u5d4c\u5165\u6ce8\u5165\u53bb\u566a\u8fc7\u7a0b\u3002", "result": "PixelHacker\u5728\u591a\u4e2a\u6570\u636e\u96c6\uff08Places2\u3001CelebA-HQ\u3001FFHQ\uff09\u4e0a\u5168\u9762\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\uff0c\u8868\u73b0\u51fa\u5353\u8d8a\u7684\u7ed3\u6784\u548c\u8bed\u4e49\u4e00\u81f4\u6027\u3002", "conclusion": "\u6f5c\u5728\u7c7b\u522b\u5f15\u5bfc\u673a\u5236\u548c\u6269\u6563\u6a21\u578b\u7684\u7ed3\u5408\u4e3a\u56fe\u50cf\u4fee\u590d\u4efb\u52a1\u63d0\u4f9b\u4e86\u7b80\u5355\u800c\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2504.20898", "pdf": "https://arxiv.org/pdf/2504.20898", "abs": "https://arxiv.org/abs/2504.20898", "authors": ["Hasan Md Tusfiqur Alam", "Devansh Srivastav", "Abdulrahman Mohamed Selim", "Md Abdul Kadir", "Md Moktadiurl Hoque Shuvo", "Daniel Sonntag"], "title": "CBM-RAG: Demonstrating Enhanced Interpretability in Radiology Report Generation with Multi-Agent RAG and Concept Bottleneck Models", "categories": ["cs.AI", "cs.CV", "cs.IR"], "comment": "Accepted in the 17th ACM SIGCHI Symposium on Engineering Interactive\n  Computing Systems (EICS 2025)", "summary": "Advancements in generative Artificial Intelligence (AI) hold great promise\nfor automating radiology workflows, yet challenges in interpretability and\nreliability hinder clinical adoption. This paper presents an automated\nradiology report generation framework that combines Concept Bottleneck Models\n(CBMs) with a Multi-Agent Retrieval-Augmented Generation (RAG) system to bridge\nAI performance with clinical explainability. CBMs map chest X-ray features to\nhuman-understandable clinical concepts, enabling transparent disease\nclassification. Meanwhile, the RAG system integrates multi-agent collaboration\nand external knowledge to produce contextually rich, evidence-based reports.\nOur demonstration showcases the system's ability to deliver interpretable\npredictions, mitigate hallucinations, and generate high-quality, tailored\nreports with an interactive interface addressing accuracy, trust, and usability\nchallenges. This framework provides a pathway to improving diagnostic\nconsistency and empowering radiologists with actionable insights.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u6982\u5ff5\u74f6\u9888\u6a21\u578b\uff08CBM\uff09\u548c\u591a\u667a\u80fd\u4f53\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u7cfb\u7edf\u7684\u81ea\u52a8\u5316\u653e\u5c04\u5b66\u62a5\u544a\u751f\u6210\u6846\u67b6\uff0c\u65e8\u5728\u63d0\u5347AI\u7684\u53ef\u89e3\u91ca\u6027\u548c\u53ef\u9760\u6027\u3002", "motivation": "\u751f\u6210\u5f0fAI\u5728\u653e\u5c04\u5b66\u5de5\u4f5c\u6d41\u7a0b\u81ea\u52a8\u5316\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u4f46\u53ef\u89e3\u91ca\u6027\u548c\u53ef\u9760\u6027\u95ee\u9898\u963b\u788d\u4e86\u4e34\u5e8a\u91c7\u7528\u3002", "method": "\u901a\u8fc7CBM\u5c06\u80f8\u90e8X\u5c04\u7ebf\u7279\u5f81\u6620\u5c04\u4e3a\u4e34\u5e8a\u6982\u5ff5\uff0c\u7ed3\u5408\u591a\u667a\u80fd\u4f53RAG\u7cfb\u7edf\u751f\u6210\u57fa\u4e8e\u8bc1\u636e\u7684\u62a5\u544a\u3002", "result": "\u7cfb\u7edf\u80fd\u591f\u63d0\u4f9b\u53ef\u89e3\u91ca\u7684\u9884\u6d4b\u3001\u51cf\u5c11\u5e7b\u89c9\uff0c\u5e76\u751f\u6210\u9ad8\u8d28\u91cf\u3001\u5b9a\u5236\u5316\u7684\u62a5\u544a\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u63d0\u5347\u8bca\u65ad\u4e00\u81f4\u6027\u548c\u4e3a\u653e\u5c04\u79d1\u533b\u751f\u63d0\u4f9b\u53ef\u64cd\u4f5c\u7684\u89c1\u89e3\u63d0\u4f9b\u4e86\u9014\u5f84\u3002"}}
{"id": "2504.20466", "pdf": "https://arxiv.org/pdf/2504.20466", "abs": "https://arxiv.org/abs/2504.20466", "authors": ["Woo Yi Yang", "Jiarui Wang", "Sijing Wu", "Huiyu Duan", "Yuxin Zhu", "Liu Yang", "Kang Fu", "Guangtao Zhai", "Xiongkuo Min"], "title": "LMM4Gen3DHF: Benchmarking and Evaluating Multimodal 3D Human Face Generation with LMMs", "categories": ["cs.CV"], "comment": null, "summary": "The rapid advancement in generative artificial intelligence have enabled the\ncreation of 3D human faces (HFs) for applications including media production,\nvirtual reality, security, healthcare, and game development, etc. However,\nassessing the quality and realism of these AI-generated 3D human faces remains\na significant challenge due to the subjective nature of human perception and\ninnate perceptual sensitivity to facial features. To this end, we conduct a\ncomprehensive study on the quality assessment of AI-generated 3D human faces.\nWe first introduce Gen3DHF, a large-scale benchmark comprising 2,000 videos of\nAI-Generated 3D Human Faces along with 4,000 Mean Opinion Scores (MOS)\ncollected across two dimensions, i.e., quality and authenticity, 2,000\ndistortion-aware saliency maps and distortion descriptions. Based on Gen3DHF,\nwe propose LMME3DHF, a Large Multimodal Model (LMM)-based metric for Evaluating\n3DHF capable of quality and authenticity score prediction, distortion-aware\nvisual question answering, and distortion-aware saliency prediction.\nExperimental results show that LMME3DHF achieves state-of-the-art performance,\nsurpassing existing methods in both accurately predicting quality scores for\nAI-generated 3D human faces and effectively identifying distortion-aware\nsalient regions and distortion types, while maintaining strong alignment with\nhuman perceptual judgments. Both the Gen3DHF database and the LMME3DHF will be\nreleased upon the publication.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faGen3DHF\u57fa\u51c6\u548cLMME3DHF\u6a21\u578b\uff0c\u7528\u4e8e\u8bc4\u4f30AI\u751f\u6210\u76843D\u4eba\u8138\u7684\u8d28\u91cf\u548c\u771f\u5b9e\u6027\uff0c\u5b9e\u9a8c\u8868\u660e\u5176\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u7531\u4e8e\u4eba\u7c7b\u611f\u77e5\u7684\u4e3b\u89c2\u6027\u548c\u5bf9\u9762\u90e8\u7279\u5f81\u7684\u654f\u611f\u6027\uff0c\u8bc4\u4f30AI\u751f\u6210\u76843D\u4eba\u8138\u7684\u8d28\u91cf\u548c\u771f\u5b9e\u6027\u5177\u6709\u6311\u6218\u6027\u3002", "method": "\u5f15\u5165Gen3DHF\u57fa\u51c6\uff08\u5305\u542b2000\u4e2a\u89c6\u9891\u548c4000\u4e2aMOS\u8bc4\u5206\uff09\uff0c\u5e76\u63d0\u51fa\u57fa\u4e8eLMM\u7684LMME3DHF\u6a21\u578b\uff0c\u7528\u4e8e\u8d28\u91cf\u8bc4\u5206\u3001\u89c6\u89c9\u95ee\u7b54\u548c\u663e\u8457\u6027\u9884\u6d4b\u3002", "result": "LMME3DHF\u5728\u8d28\u91cf\u8bc4\u5206\u548c\u5931\u771f\u533a\u57df\u8bc6\u522b\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u4e0e\u4eba\u7c7b\u611f\u77e5\u9ad8\u5ea6\u4e00\u81f4\u3002", "conclusion": "Gen3DHF\u548cLMME3DHF\u4e3aAI\u751f\u6210\u76843D\u4eba\u8138\u8bc4\u4f30\u63d0\u4f9b\u4e86\u6709\u6548\u5de5\u5177\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2504.20921", "pdf": "https://arxiv.org/pdf/2504.20921", "abs": "https://arxiv.org/abs/2504.20921", "authors": ["Polycarp Nalela"], "title": "Leveraging Generative AI Through Prompt Engineering and Rigorous Validation to Create Comprehensive Synthetic Datasets for AI Training in Healthcare", "categories": ["cs.AI"], "comment": null, "summary": "Access to high-quality medical data is often restricted due to privacy\nconcerns, posing significant challenges for training artificial intelligence\n(AI) algorithms within Electronic Health Record (EHR) applications. In this\nstudy, prompt engineering with the GPT-4 API was employed to generate\nhigh-quality synthetic datasets aimed at overcoming this limitation. The\ngenerated data encompassed a comprehensive array of patient admission\ninformation, including healthcare provider details, hospital departments,\nwards, bed assignments, patient demographics, emergency contacts, vital signs,\nimmunizations, allergies, medical histories, appointments, hospital visits,\nlaboratory tests, diagnoses, treatment plans, medications, clinical notes,\nvisit logs, discharge summaries, and referrals. To ensure data quality and\nintegrity, advanced validation techniques were implemented utilizing models\nsuch as BERT's Next Sentence Prediction for sentence coherence, GPT-2 for\noverall plausibility, RoBERTa for logical consistency, autoencoders for anomaly\ndetection, and conducted diversity analysis. Synthetic data that met all\nvalidation criteria were integrated into a comprehensive PostgreSQL database,\nserving as the data management system for the EHR application. This approach\ndemonstrates that leveraging generative AI models with rigorous validation can\neffectively produce high-quality synthetic medical data, facilitating the\ntraining of AI algorithms while addressing privacy concerns associated with\nreal patient data.", "AI": {"tldr": "\u5229\u7528GPT-4 API\u751f\u6210\u9ad8\u8d28\u91cf\u5408\u6210\u533b\u7597\u6570\u636e\uff0c\u89e3\u51b3\u9690\u79c1\u95ee\u9898\uff0c\u5e76\u901a\u8fc7\u591a\u79cd\u6a21\u578b\u9a8c\u8bc1\u6570\u636e\u8d28\u91cf\u3002", "motivation": "\u533b\u7597\u6570\u636e\u56e0\u9690\u79c1\u95ee\u9898\u96be\u4ee5\u83b7\u53d6\uff0c\u9650\u5236\u4e86AI\u7b97\u6cd5\u7684\u8bad\u7ec3\u3002", "method": "\u91c7\u7528GPT-4 API\u751f\u6210\u5408\u6210\u6570\u636e\uff0c\u5e76\u4f7f\u7528BERT\u3001GPT-2\u3001RoBERTa\u7b49\u6a21\u578b\u8fdb\u884c\u9a8c\u8bc1\u3002", "result": "\u6210\u529f\u751f\u6210\u5e76\u901a\u8fc7\u9a8c\u8bc1\u7684\u9ad8\u8d28\u91cf\u5408\u6210\u6570\u636e\uff0c\u96c6\u6210\u5230PostgreSQL\u6570\u636e\u5e93\u4e2d\u3002", "conclusion": "\u751f\u6210\u5f0fAI\u7ed3\u5408\u4e25\u683c\u9a8c\u8bc1\u53ef\u6709\u6548\u89e3\u51b3\u9690\u79c1\u95ee\u9898\uff0c\u652f\u6301AI\u7b97\u6cd5\u8bad\u7ec3\u3002"}}
{"id": "2504.20468", "pdf": "https://arxiv.org/pdf/2504.20468", "abs": "https://arxiv.org/abs/2504.20468", "authors": ["Yuanchen Wu", "Lu Zhang", "Hang Yao", "Junlong Du", "Ke Yan", "Shouhong Ding", "Yunsheng Wu", "Xiaoqiang Li"], "title": "Antidote: A Unified Framework for Mitigating LVLM Hallucinations in Counterfactual Presupposition and Object Perception", "categories": ["cs.CV"], "comment": "Accepted to CVPR 2025", "summary": "Large Vision-Language Models (LVLMs) have achieved impressive results across\nvarious cross-modal tasks. However, hallucinations, i.e., the models generating\ncounterfactual responses, remain a challenge. Though recent studies have\nattempted to alleviate object perception hallucinations, they focus on the\nmodels' response generation, and overlooking the task question itself. This\npaper discusses the vulnerability of LVLMs in solving counterfactual\npresupposition questions (CPQs), where the models are prone to accept the\npresuppositions of counterfactual objects and produce severe hallucinatory\nresponses. To this end, we introduce \"Antidote\", a unified, synthetic\ndata-driven post-training framework for mitigating both types of hallucination\nabove. It leverages synthetic data to incorporate factual priors into questions\nto achieve self-correction, and decouple the mitigation process into a\npreference optimization problem. Furthermore, we construct \"CP-Bench\", a novel\nbenchmark to evaluate LVLMs' ability to correctly handle CPQs and produce\nfactual responses. Applied to the LLaVA series, Antidote can simultaneously\nenhance performance on CP-Bench by over 50%, POPE by 1.8-3.3%, and CHAIR & SHR\nby 30-50%, all without relying on external supervision from stronger LVLMs or\nhuman feedback and introducing noticeable catastrophic forgetting issues.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faAntidote\u6846\u67b6\uff0c\u901a\u8fc7\u5408\u6210\u6570\u636e\u9a71\u52a8\u7684\u540e\u8bad\u7ec3\u65b9\u6cd5\uff0c\u51cf\u5c11\u5927\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u89e3\u51b3\u53cd\u4e8b\u5b9e\u9884\u8bbe\u95ee\u9898\u65f6\u7684\u5e7b\u89c9\u73b0\u8c61\uff0c\u5e76\u6784\u5efaCP-Bench\u57fa\u51c6\u8fdb\u884c\u8bc4\u4f30\u3002", "motivation": "\u5927\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u8de8\u6a21\u6001\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5b58\u5728\u751f\u6210\u53cd\u4e8b\u5b9e\u54cd\u5e94\u7684\u5e7b\u89c9\u95ee\u9898\uff0c\u5c24\u5176\u662f\u9762\u5bf9\u53cd\u4e8b\u5b9e\u9884\u8bbe\u95ee\u9898\u65f6\u3002\u73b0\u6709\u7814\u7a76\u591a\u5173\u6ce8\u6a21\u578b\u54cd\u5e94\u751f\u6210\uff0c\u800c\u5ffd\u7565\u4e86\u95ee\u9898\u672c\u8eab\u7684\u9884\u8bbe\u3002", "method": "\u63d0\u51faAntidote\u6846\u67b6\uff0c\u5229\u7528\u5408\u6210\u6570\u636e\u5c06\u4e8b\u5b9e\u5148\u9a8c\u878d\u5165\u95ee\u9898\u4ee5\u5b9e\u73b0\u81ea\u6821\u6b63\uff0c\u5e76\u5c06\u7f13\u89e3\u8fc7\u7a0b\u5206\u89e3\u4e3a\u504f\u597d\u4f18\u5316\u95ee\u9898\u3002\u6784\u5efaCP-Bench\u57fa\u51c6\u8bc4\u4f30\u6a21\u578b\u6027\u80fd\u3002", "result": "Antidote\u5728LLaVA\u7cfb\u5217\u6a21\u578b\u4e0a\u5e94\u7528\u540e\uff0cCP-Bench\u6027\u80fd\u63d0\u5347\u8d8550%\uff0cPOPE\u63d0\u53471.8-3.3%\uff0cCHAIR & SHR\u63d0\u534730-50%\uff0c\u4e14\u65e0\u9700\u5916\u90e8\u76d1\u7763\u6216\u5f15\u5165\u707e\u96be\u6027\u9057\u5fd8\u95ee\u9898\u3002", "conclusion": "Antidote\u80fd\u6709\u6548\u51cf\u5c11\u5927\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u5e7b\u89c9\u73b0\u8c61\uff0c\u5c24\u5176\u5728\u53cd\u4e8b\u5b9e\u9884\u8bbe\u95ee\u9898\u4e0a\u8868\u73b0\u663e\u8457\uff0c\u4e3a\u6a21\u578b\u81ea\u6821\u6b63\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2504.20924", "pdf": "https://arxiv.org/pdf/2504.20924", "abs": "https://arxiv.org/abs/2504.20924", "authors": ["Beomjun Kim", "Kangyeon Kim", "Sunwoo Kim", "Heejin Ahn"], "title": "A Domain-Agnostic Scalable AI Safety Ensuring Framework", "categories": ["cs.AI"], "comment": "Experimental supplementary material will be available before May 22\n  23:59PM AOE", "summary": "Ensuring the safety of AI systems has recently emerged as a critical priority\nfor real-world deployment, particularly in physical AI applications. Current\napproaches to AI safety typically address predefined domain-specific safety\nconditions, limiting their ability to generalize across contexts.\n  We propose a novel AI safety framework that ensures AI systems comply with\n\\textbf{any user-defined constraint}, with \\textbf{any desired probability},\nand across \\textbf{various domains}.\n  In this framework, we combine an AI component (e.g., neural network) with an\noptimization problem to produce responses that minimize objectives while\nsatisfying user-defined constraints with probabilities exceeding user-defined\nthresholds. For credibility assessment of the AI component, we propose\n\\textit{internal test data}, a supplementary set of safety-labeled data, and a\n\\textit{conservative testing} methodology that provides statistical validity of\nusing internal test data. We also present an approximation method of a loss\nfunction and how to compute its gradient for training.\n  We mathematically prove that probabilistic constraint satisfaction is\nguaranteed under specific, mild conditions and prove a scaling law between\nsafety and the number of internal test data. We demonstrate our framework's\neffectiveness through experiments in diverse domains: demand prediction for\nproduction decision, safe reinforcement learning within the SafetyGym\nsimulator, and guarding AI chatbot outputs. Through these experiments, we\ndemonstrate that our method guarantees safety for user-specified constraints,\noutperforms {for \\textbf{up to several order of magnitudes}} existing methods\nin low safety threshold regions, and scales effectively with respect to the\nsize of internal test data.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578bAI\u5b89\u5168\u6846\u67b6\uff0c\u786e\u4fddAI\u7cfb\u7edf\u6ee1\u8db3\u7528\u6237\u5b9a\u4e49\u7684\u7ea6\u675f\u6761\u4ef6\uff0c\u5177\u6709\u9ad8\u6982\u7387\u4e14\u8de8\u9886\u57df\u9002\u7528\u3002", "motivation": "\u5f53\u524dAI\u5b89\u5168\u65b9\u6cd5\u5c40\u9650\u4e8e\u7279\u5b9a\u9886\u57df\uff0c\u96be\u4ee5\u6cdb\u5316\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u901a\u7528\u7684\u5b89\u5168\u6846\u67b6\u3002", "method": "\u7ed3\u5408AI\u7ec4\u4ef6\u4e0e\u4f18\u5316\u95ee\u9898\uff0c\u901a\u8fc7\u5185\u90e8\u6d4b\u8bd5\u6570\u636e\u548c\u4fdd\u5b88\u6d4b\u8bd5\u65b9\u6cd5\u786e\u4fdd\u7ea6\u675f\u6ee1\u8db3\u6982\u7387\u3002", "result": "\u6570\u5b66\u8bc1\u660e\u7ea6\u675f\u6ee1\u8db3\u6982\u7387\u7684\u4fdd\u8bc1\uff0c\u5b9e\u9a8c\u663e\u793a\u5728\u591a\u4e2a\u9886\u57df\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u6846\u67b6\u5728\u4fdd\u8bc1\u5b89\u5168\u6027\u7684\u540c\u65f6\uff0c\u5177\u6709\u9ad8\u6cdb\u5316\u80fd\u529b\u548c\u6269\u5c55\u6027\u3002"}}
{"id": "2504.20496", "pdf": "https://arxiv.org/pdf/2504.20496", "abs": "https://arxiv.org/abs/2504.20496", "authors": ["Shuo Sun", "Torsten Sattler", "Malcolm Mielle", "Achim J. Lilienthal", "Martin Magnusson"], "title": "Large-scale visual SLAM for in-the-wild videos", "categories": ["cs.CV"], "comment": "fix the overview figure", "summary": "Accurate and robust 3D scene reconstruction from casual, in-the-wild videos\ncan significantly simplify robot deployment to new environments. However,\nreliable camera pose estimation and scene reconstruction from such\nunconstrained videos remains an open challenge. Existing visual-only SLAM\nmethods perform well on benchmark datasets but struggle with real-world footage\nwhich often exhibits uncontrolled motion including rapid rotations and pure\nforward movements, textureless regions, and dynamic objects. We analyze the\nlimitations of current methods and introduce a robust pipeline designed to\nimprove 3D reconstruction from casual videos. We build upon recent deep visual\nodometry methods but increase robustness in several ways. Camera intrinsics are\nautomatically recovered from the first few frames using structure-from-motion.\nDynamic objects and less-constrained areas are masked with a predictive model.\nAdditionally, we leverage monocular depth estimates to regularize bundle\nadjustment, mitigating errors in low-parallax situations. Finally, we integrate\nplace recognition and loop closure to reduce long-term drift and refine both\nintrinsics and pose estimates through global bundle adjustment. We demonstrate\nlarge-scale contiguous 3D models from several online videos in various\nenvironments. In contrast, baseline methods typically produce locally\ninconsistent results at several points, producing separate segments or\ndistorted maps. In lieu of ground-truth pose data, we evaluate map consistency,\nexecution time and visual accuracy of re-rendered NeRF models. Our proposed\nsystem establishes a new baseline for visual reconstruction from casual\nuncontrolled videos found online, demonstrating more consistent reconstructions\nover longer sequences of in-the-wild videos than previously achieved.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9c81\u68d2\u76843D\u573a\u666f\u91cd\u5efa\u65b9\u6cd5\uff0c\u9488\u5bf9\u975e\u7ea6\u675f\u89c6\u9891\u4e2d\u7684\u76f8\u673a\u4f4d\u59ff\u4f30\u8ba1\u548c\u573a\u666f\u91cd\u5efa\u95ee\u9898\uff0c\u901a\u8fc7\u6539\u8fdb\u89c6\u89c9\u91cc\u7a0b\u8ba1\u3001\u52a8\u6001\u5bf9\u8c61\u63a9\u7801\u3001\u6df1\u5ea6\u4f30\u8ba1\u548c\u5168\u5c40\u4f18\u5316\uff0c\u5b9e\u73b0\u4e86\u66f4\u4e00\u81f4\u76843D\u91cd\u5efa\u6548\u679c\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u89c6\u89c9SLAM\u65b9\u6cd5\u5728\u975e\u7ea6\u675f\u89c6\u9891\uff08\u5982\u5feb\u901f\u65cb\u8f6c\u3001\u65e0\u7eb9\u7406\u533a\u57df\u548c\u52a8\u6001\u5bf9\u8c61\uff09\u4e2d\u8868\u73b0\u4e0d\u4f73\u7684\u95ee\u9898\u3002", "method": "\u7ed3\u5408\u6df1\u5ea6\u89c6\u89c9\u91cc\u7a0b\u8ba1\u3001\u52a8\u6001\u5bf9\u8c61\u63a9\u7801\u3001\u5355\u76ee\u6df1\u5ea6\u4f30\u8ba1\u548c\u5168\u5c40\u4f18\u5316\uff08\u5305\u62ec\u4f4d\u59ff\u8bc6\u522b\u548c\u95ed\u73af\u68c0\u6d4b\uff09\u3002", "result": "\u5728\u591a\u79cd\u73af\u5883\u4e2d\u5b9e\u73b0\u4e86\u5927\u89c4\u6a21\u8fde\u7eed\u76843D\u91cd\u5efa\uff0c\u76f8\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u66f4\u4e00\u81f4\u4e14\u51c6\u786e\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u975e\u7ea6\u675f\u89c6\u9891\u76843D\u91cd\u5efa\u63d0\u4f9b\u4e86\u65b0\u7684\u57fa\u51c6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u91cd\u5efa\u7684\u4e00\u81f4\u6027\u548c\u51c6\u786e\u6027\u3002"}}
{"id": "2504.20930", "pdf": "https://arxiv.org/pdf/2504.20930", "abs": "https://arxiv.org/abs/2504.20930", "authors": ["Ziqing Fan", "Cheng Liang", "Chaoyi Wu", "Ya Zhang", "Yanfeng Wang", "Weidi Xie"], "title": "ChestX-Reasoner: Advancing Radiology Foundation Models with Reasoning through Step-by-Step Verification", "categories": ["cs.AI", "cs.CL", "cs.CV"], "comment": null, "summary": "Recent advances in reasoning-enhanced large language models (LLMs) and\nmultimodal LLMs (MLLMs) have significantly improved performance in complex\ntasks, yet medical AI models often overlook the structured reasoning processes\ninherent in clinical practice. In this work, we present ChestX-Reasoner, a\nradiology diagnosis MLLM designed to leverage process supervision mined\ndirectly from clinical reports, reflecting the step-by-step reasoning followed\nby radiologists. We construct a large dataset by extracting and refining\nreasoning chains from routine radiology reports. Our two-stage training\nframework combines supervised fine-tuning and reinforcement learning guided by\nprocess rewards to better align model reasoning with clinical standards. We\nintroduce RadRBench-CXR, a comprehensive benchmark featuring 59K visual\nquestion answering samples with 301K clinically validated reasoning steps, and\npropose RadRScore, a metric evaluating reasoning factuality, completeness, and\neffectiveness. ChestX-Reasoner outperforms existing medical and general-domain\nMLLMs in both diagnostic accuracy and reasoning ability, achieving 16%, 5.9%,\nand 18% improvements in reasoning ability compared to the best medical MLLM,\nthe best general MLLM, and its base model, respectively, as well as 3.3%, 24%,\nand 27% improvements in outcome accuracy. All resources are open-sourced to\nfacilitate further research in medical reasoning MLLMs.", "AI": {"tldr": "ChestX-Reasoner\u662f\u4e00\u79cd\u653e\u5c04\u5b66\u8bca\u65ad\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLM\uff09\uff0c\u901a\u8fc7\u4e34\u5e8a\u62a5\u544a\u4e2d\u7684\u7ed3\u6784\u5316\u63a8\u7406\u8fc7\u7a0b\u63d0\u5347\u6027\u80fd\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u533b\u5b66\u548c\u901a\u7528\u9886\u57dfMLLM\u3002", "motivation": "\u533b\u5b66AI\u6a21\u578b\u5e38\u5ffd\u7565\u4e34\u5e8a\u5b9e\u8df5\u4e2d\u7684\u7ed3\u6784\u5316\u63a8\u7406\u8fc7\u7a0b\uff0cChestX-Reasoner\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u5229\u7528\u4e34\u5e8a\u62a5\u544a\u6784\u5efa\u5927\u89c4\u6a21\u6570\u636e\u96c6\uff0c\u91c7\u7528\u4e24\u9636\u6bb5\u8bad\u7ec3\u6846\u67b6\uff08\u76d1\u7763\u5fae\u8c03\u548c\u5f3a\u5316\u5b66\u4e60\uff09\uff0c\u5e76\u5f15\u5165\u65b0\u57fa\u51c6\u548c\u8bc4\u4f30\u6307\u6807\u3002", "result": "\u5728\u8bca\u65ad\u51c6\u786e\u6027\u548c\u63a8\u7406\u80fd\u529b\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\uff0c\u63a8\u7406\u80fd\u529b\u63d0\u534716%\u30015.9%\u548c18%\uff0c\u7ed3\u679c\u51c6\u786e\u6027\u63d0\u53473.3%\u300124%\u548c27%\u3002", "conclusion": "ChestX-Reasoner\u901a\u8fc7\u7ed3\u6784\u5316\u63a8\u7406\u548c\u4e34\u5e8a\u5bf9\u9f50\uff0c\u663e\u8457\u63d0\u5347\u533b\u5b66MLLM\u6027\u80fd\uff0c\u6240\u6709\u8d44\u6e90\u5df2\u5f00\u6e90\u3002"}}
{"id": "2504.20498", "pdf": "https://arxiv.org/pdf/2504.20498", "abs": "https://arxiv.org/abs/2504.20498", "authors": ["Jianhong Han", "Yupei Wang", "Liang Chen"], "title": "Style-Adaptive Detection Transformer for Single-Source Domain Generalized Object Detection", "categories": ["cs.CV"], "comment": "Manuscript submitted to IEEE Transactions on Multimedia", "summary": "Single-source Domain Generalization (SDG) in object detection aims to develop\na detector using only data from a source domain that can exhibit strong\ngeneralization capability when applied to unseen target domains. Existing\nmethods are built upon CNN-based detectors and primarily improve robustness by\nemploying carefully designed data augmentation strategies integrated with\nfeature alignment techniques. However, data augmentation methods have inherent\ndrawbacks; they are only effective when the augmented sample distribution\napproximates or covers the unseen scenarios, thus failing to enhance\ngeneralization across all unseen domains. Furthermore, while the recent\nDetection Transformer (DETR) has demonstrated superior generalization\ncapability in domain adaptation tasks due to its efficient global information\nextraction, its potential in SDG tasks remains unexplored. To this end, we\nintroduce a strong DETR-based detector named the Style-Adaptive Detection\nTransformer (SA-DETR) for SDG in object detection. Specifically, we present a\ndomain style adapter that projects the style representation of the unseen\ntarget domain into the training domain, enabling dynamic style adaptation.\nThen, we propose an object-aware contrastive learning module to guide the\ndetector in extracting domain-invariant features through contrastive learning.\nBy using object-aware gating masks to constrain feature aggregation in both\nspatial and semantic dimensions, this module achieves cross-domain contrast of\ninstance-level features, thereby enhancing generalization. Extensive\nexperiments demonstrate the superior performance and generalization capability\nof SA-DETR across five different weather scenarios. Code is released at\nhttps://github.com/h751410234/SA-DETR.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eDETR\u7684\u68c0\u6d4b\u5668SA-DETR\uff0c\u7528\u4e8e\u5355\u6e90\u57df\u6cdb\u5316\uff08SDG\uff09\u4efb\u52a1\uff0c\u901a\u8fc7\u52a8\u6001\u98ce\u683c\u9002\u914d\u548c\u5bf9\u8c61\u611f\u77e5\u5bf9\u6bd4\u5b66\u4e60\u6a21\u5757\u63d0\u5347\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u6570\u636e\u589e\u5f3a\u548c\u7279\u5f81\u5bf9\u9f50\uff0c\u4f46\u65e0\u6cd5\u8986\u76d6\u6240\u6709\u672a\u89c1\u57df\u3002DETR\u5728\u57df\u9002\u5e94\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5176\u5728SDG\u4efb\u52a1\u4e2d\u7684\u6f5c\u529b\u5c1a\u672a\u63a2\u7d22\u3002", "method": "\u63d0\u51faSA-DETR\uff0c\u5305\u542b\u57df\u98ce\u683c\u9002\u914d\u5668\u548c\u5bf9\u8c61\u611f\u77e5\u5bf9\u6bd4\u5b66\u4e60\u6a21\u5757\uff0c\u52a8\u6001\u9002\u5e94\u76ee\u6807\u57df\u98ce\u683c\u5e76\u63d0\u53d6\u57df\u4e0d\u53d8\u7279\u5f81\u3002", "result": "\u5b9e\u9a8c\u8868\u660eSA-DETR\u5728\u4e94\u79cd\u4e0d\u540c\u5929\u6c14\u573a\u666f\u4e2d\u5177\u6709\u4f18\u8d8a\u6027\u80fd\u548c\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "SA-DETR\u901a\u8fc7\u52a8\u6001\u98ce\u683c\u9002\u914d\u548c\u5bf9\u6bd4\u5b66\u4e60\u663e\u8457\u63d0\u5347\u4e86SDG\u4efb\u52a1\u7684\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2504.20980", "pdf": "https://arxiv.org/pdf/2504.20980", "abs": "https://arxiv.org/abs/2504.20980", "authors": ["Neil F. Johnson", "Frank Yingjie Huo"], "title": "Jekyll-and-Hyde Tipping Point in an AI's Behavior", "categories": ["cs.AI", "cs.CY", "nlin.AO", "physics.comp-ph", "physics.soc-ph"], "comment": null, "summary": "Trust in AI is undermined by the fact that there is no science that predicts\n-- or that can explain to the public -- when an LLM's output (e.g. ChatGPT) is\nlikely to tip mid-response to become wrong, misleading, irrelevant or\ndangerous. With deaths and trauma already being blamed on LLMs, this\nuncertainty is even pushing people to treat their 'pet' LLM more politely to\n'dissuade' it (or its future Artificial General Intelligence offspring) from\nsuddenly turning on them. Here we address this acute need by deriving from\nfirst principles an exact formula for when a Jekyll-and-Hyde tipping point\noccurs at LLMs' most basic level. Requiring only secondary school mathematics,\nit shows the cause to be the AI's attention spreading so thin it suddenly\nsnaps. This exact formula provides quantitative predictions for how the\ntipping-point can be delayed or prevented by changing the prompt and the AI's\ntraining. Tailored generalizations will provide policymakers and the public\nwith a firm platform for discussing any of AI's broader uses and risks, e.g. as\na personal counselor, medical advisor, decision-maker for when to use force in\na conflict situation. It also meets the need for clear and transparent answers\nto questions like ''should I be polite to my LLM?''", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u57fa\u672c\u539f\u7406\u7684\u516c\u5f0f\uff0c\u7528\u4e8e\u9884\u6d4bLLM\uff08\u5982ChatGPT\uff09\u8f93\u51fa\u4f55\u65f6\u4f1a\u7a81\u7136\u53d8\u5f97\u9519\u8bef\u3001\u8bef\u5bfc\u3001\u65e0\u5173\u6216\u5371\u9669\uff0c\u4ece\u800c\u89e3\u51b3\u516c\u4f17\u5bf9AI\u4fe1\u4efb\u7684\u95ee\u9898\u3002", "motivation": "\u5f53\u524d\u7f3a\u4e4f\u79d1\u5b66\u65b9\u6cd5\u9884\u6d4b\u6216\u89e3\u91caLLM\u8f93\u51fa\u7684\u7a81\u7136\u53d8\u5316\uff0c\u5bfc\u81f4\u516c\u4f17\u5bf9AI\u7684\u4fe1\u4efb\u4e0d\u8db3\uff0c\u751a\u81f3\u5f15\u53d1\u5bf9LLM\u7684\u8fc7\u5ea6\u793c\u8c8c\u884c\u4e3a\u3002", "method": "\u901a\u8fc7\u4ece\u57fa\u672c\u539f\u7406\u63a8\u5bfc\u51fa\u4e00\u4e2a\u7cbe\u786e\u516c\u5f0f\uff0c\u5206\u6790LLM\u6ce8\u610f\u529b\u5206\u6563\u5bfc\u81f4\u8f93\u51fa\u7a81\u53d8\u7684\u4e34\u754c\u70b9\uff0c\u4ec5\u9700\u4e2d\u5b66\u6570\u5b66\u77e5\u8bc6\u3002", "result": "\u8be5\u516c\u5f0f\u80fd\u5b9a\u91cf\u9884\u6d4b\u5982\u4f55\u901a\u8fc7\u6539\u53d8\u63d0\u793a\u6216\u8bad\u7ec3\u5ef6\u8fdf\u6216\u9632\u6b62\u7a81\u53d8\uff0c\u5e76\u4e3a\u653f\u7b56\u5236\u5b9a\u8005\u63d0\u4f9b\u8ba8\u8bbaAI\u98ce\u9669\u7684\u5e73\u53f0\u3002", "conclusion": "\u7814\u7a76\u4e3a\u516c\u4f17\u548c\u653f\u7b56\u5236\u5b9a\u8005\u63d0\u4f9b\u4e86\u900f\u660e\u3001\u6e05\u6670\u7684\u7b54\u6848\uff0c\u89e3\u51b3\u4e86LLM\u4f7f\u7528\u4e2d\u7684\u4fe1\u4efb\u548c\u98ce\u9669\u95ee\u9898\u3002"}}
{"id": "2504.20509", "pdf": "https://arxiv.org/pdf/2504.20509", "abs": "https://arxiv.org/abs/2504.20509", "authors": ["Yichu Xu", "Di Wang", "Hongzan Jiao", "Lefei Zhang", "Liangpei Zhang"], "title": "MambaMoE: Mixture-of-Spectral-Spatial-Experts State Space Model for Hyperspectral Image Classification", "categories": ["cs.CV"], "comment": null, "summary": "The Mamba model has recently demonstrated strong potential in hyperspectral\nimage (HSI) classification, owing to its ability to perform context modeling\nwith linear computational complexity. However, existing Mamba-based methods\nusually neglect the spectral and spatial directional characteristics related to\nheterogeneous objects in hyperspectral scenes, leading to limited\nclassification performance. To address these issues, we propose MambaMoE, a\nnovel spectral-spatial mixture-of-experts framework, representing the first\nMoE-based approach in the HSI classification community. Specifically, we design\na Mixture of Mamba Expert Block (MoMEB) that leverages sparse expert activation\nto enable adaptive spectral-spatial modeling. Furthermore, we introduce an\nuncertainty-guided corrective learning (UGCL) strategy to encourage the model's\nattention toward complex regions prone to prediction ambiguity. Extensive\nexperiments on multiple public HSI benchmarks demonstrate that MambaMoE\nachieves state-of-the-art performance in both accuracy and efficiency compared\nto existing advanced approaches, especially for Mamba-based methods. Code will\nbe released.", "AI": {"tldr": "MambaMoE\u662f\u4e00\u79cd\u65b0\u578b\u7684\u6df7\u5408\u4e13\u5bb6\u6846\u67b6\uff0c\u7528\u4e8e\u9ad8\u5149\u8c31\u56fe\u50cf\u5206\u7c7b\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u5efa\u6a21\u548c\u4e0d\u786e\u5b9a\u6027\u5f15\u5bfc\u5b66\u4e60\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u73b0\u6709Mamba\u65b9\u6cd5\u5ffd\u89c6\u9ad8\u5149\u8c31\u573a\u666f\u4e2d\u5f02\u6784\u7269\u4f53\u7684\u5149\u8c31\u548c\u7a7a\u95f4\u65b9\u5411\u7279\u6027\uff0c\u5bfc\u81f4\u5206\u7c7b\u6027\u80fd\u53d7\u9650\u3002", "method": "\u63d0\u51faMambaMoE\u6846\u67b6\uff0c\u8bbe\u8ba1\u6df7\u5408Mamba\u4e13\u5bb6\u5757\uff08MoMEB\uff09\u548c\u4e0d\u786e\u5b9a\u6027\u5f15\u5bfc\u6821\u6b63\u5b66\u4e60\uff08UGCL\uff09\u7b56\u7565\u3002", "result": "\u5728\u591a\u4e2a\u516c\u5f00HSI\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cMambaMoE\u5728\u7cbe\u5ea6\u548c\u6548\u7387\u4e0a\u5747\u8fbe\u5230\u6700\u4f18\u3002", "conclusion": "MambaMoE\u662fHSI\u5206\u7c7b\u9886\u57df\u7684\u9996\u4e2aMoE\u65b9\u6cd5\uff0c\u6027\u80fd\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2504.20983", "pdf": "https://arxiv.org/pdf/2504.20983", "abs": "https://arxiv.org/abs/2504.20983", "authors": ["Giuseppe De Giacomo", "Gianmarco Parretti", "Shufang Zhu"], "title": "LTLf Adaptive Synthesis for Multi-Tier Goals in Nondeterministic Domains", "categories": ["cs.AI"], "comment": null, "summary": "We study a variant of LTLf synthesis that synthesizes adaptive strategies for\nachieving a multi-tier goal, consisting of multiple increasingly challenging\nLTLf objectives in nondeterministic planning domains. Adaptive strategies are\nstrategies that at any point of their execution (i) enforce the satisfaction of\nas many objectives as possible in the multi-tier goal, and (ii) exploit\npossible cooperation from the environment to satisfy as many as possible of the\nremaining ones. This happens dynamically: if the environment cooperates (ii)\nand an objective becomes enforceable (i), then our strategies will enforce it.\nWe provide a game-theoretic technique to compute adaptive strategies that is\nsound and complete. Notably, our technique is polynomial, in fact quadratic, in\nthe number of objectives. In other words, it handles multi-tier goals with only\na minor overhead compared to standard LTLf synthesis.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u591a\u76ee\u6807LTLf\u5408\u6210\u7684\u81ea\u9002\u5e94\u7b56\u7565\u65b9\u6cd5\uff0c\u80fd\u591f\u5728\u975e\u786e\u5b9a\u6027\u89c4\u5212\u9886\u57df\u4e2d\u52a8\u6001\u6ee1\u8db3\u591a\u5c42\u7ea7\u76ee\u6807\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u89e3\u51b3\u5728\u975e\u786e\u5b9a\u6027\u73af\u5883\u4e2d\u5982\u4f55\u52a8\u6001\u6ee1\u8db3\u591a\u5c42\u7ea7LTLf\u76ee\u6807\u7684\u95ee\u9898\uff0c\u540c\u65f6\u5229\u7528\u73af\u5883\u53ef\u80fd\u7684\u5408\u4f5c\u6765\u4f18\u5316\u7b56\u7565\u3002", "method": "\u91c7\u7528\u535a\u5f08\u8bba\u6280\u672f\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u9879\u5f0f\u65f6\u95f4\uff08\u4e8c\u6b21\uff09\u7b97\u6cd5\u6765\u8ba1\u7b97\u81ea\u9002\u5e94\u7b56\u7565\uff0c\u786e\u4fdd\u7b56\u7565\u7684\u5b8c\u5907\u6027\u548c\u6b63\u786e\u6027\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u5904\u7406\u591a\u5c42\u7ea7\u76ee\u6807\u65f6\u4ec5\u6bd4\u6807\u51c6LTLf\u5408\u6210\u589e\u52a0\u5c11\u91cf\u8ba1\u7b97\u5f00\u9500\u3002", "conclusion": "\u7ed3\u8bba\u662f\u8be5\u65b9\u6cd5\u9ad8\u6548\u4e14\u5b9e\u7528\uff0c\u9002\u7528\u4e8e\u590d\u6742\u73af\u5883\u4e2d\u7684\u591a\u76ee\u6807\u5408\u6210\u95ee\u9898\u3002"}}
{"id": "2504.20510", "pdf": "https://arxiv.org/pdf/2504.20510", "abs": "https://arxiv.org/abs/2504.20510", "authors": ["Irina Ruzavina", "Lisa Sophie Theis", "Jesse Lemeer", "Rutger de Groen", "Leo Ebeling", "Andrej Hulak", "Jouaria Ali", "Guangzhi Tang", "Rico Mockel"], "title": "SteelBlastQC: Shot-blasted Steel Surface Dataset with Interpretable Detection of Surface Defects", "categories": ["cs.CV", "cs.NE"], "comment": "Accepted by IJCNN 2025", "summary": "Automating the quality control of shot-blasted steel surfaces is crucial for\nimproving manufacturing efficiency and consistency. This study presents a\ndataset of 1654 labeled RGB images (512x512) of steel surfaces, classified as\neither \"ready for paint\" or \"needs shot-blasting.\" The dataset captures\nreal-world surface defects, including discoloration, welding lines, scratches\nand corrosion, making it well-suited for training computer vision models.\nAdditionally, three classification approaches were evaluated: Compact\nConvolutional Transformers (CCT), Support Vector Machines (SVM) with ResNet-50\nfeature extraction, and a Convolutional Autoencoder (CAE). The supervised\nmethods (CCT and SVM) achieve 95% classification accuracy on the test set, with\nCCT leveraging transformer-based attention mechanisms and SVM offering a\ncomputationally efficient alternative. The CAE approach, while less effective,\nestablishes a baseline for unsupervised quality control. We present\ninterpretable decision-making by all three neural networks, allowing industry\nusers to visually pinpoint problematic regions and understand the model's\nrationale. By releasing the dataset and baseline codes, this work aims to\nsupport further research in defect detection, advance the development of\ninterpretable computer vision models for quality control, and encourage the\nadoption of automated inspection systems in industrial applications.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u7528\u4e8e\u94a2\u8868\u9762\u8d28\u91cf\u63a7\u5236\u7684\u6807\u8bb0\u56fe\u50cf\u6570\u636e\u96c6\uff0c\u5e76\u8bc4\u4f30\u4e86\u4e09\u79cd\u5206\u7c7b\u65b9\u6cd5\uff0c\u5176\u4e2d\u76d1\u7763\u65b9\u6cd5\uff08CCT\u548cSVM\uff09\u8fbe\u523095%\u7684\u51c6\u786e\u7387\uff0c\u540c\u65f6\u652f\u6301\u53ef\u89e3\u91ca\u7684\u51b3\u7b56\u3002", "motivation": "\u81ea\u52a8\u5316\u94a2\u8868\u9762\u55b7\u7802\u8d28\u91cf\u68c0\u67e5\u5bf9\u63d0\u9ad8\u5236\u9020\u6548\u7387\u548c\u4e00\u81f4\u6027\u81f3\u5173\u91cd\u8981\u3002", "method": "\u4f7f\u75281654\u5f20\u6807\u8bb0\u7684RGB\u56fe\u50cf\u6570\u636e\u96c6\uff0c\u8bc4\u4f30\u4e86CCT\u3001SVM\uff08\u57fa\u4e8eResNet-50\u7279\u5f81\u63d0\u53d6\uff09\u548cCAE\u4e09\u79cd\u5206\u7c7b\u65b9\u6cd5\u3002", "result": "CCT\u548cSVM\u5728\u6d4b\u8bd5\u96c6\u4e0a\u8fbe\u523095%\u7684\u5206\u7c7b\u51c6\u786e\u7387\uff0cCAE\u4f5c\u4e3a\u65e0\u76d1\u7763\u57fa\u7ebf\u8868\u73b0\u8f83\u5dee\u3002", "conclusion": "\u8be5\u7814\u7a76\u901a\u8fc7\u516c\u5f00\u6570\u636e\u96c6\u548c\u4ee3\u7801\uff0c\u652f\u6301\u7f3a\u9677\u68c0\u6d4b\u7814\u7a76\uff0c\u63a8\u52a8\u53ef\u89e3\u91ca\u8ba1\u7b97\u673a\u89c6\u89c9\u6a21\u578b\u5728\u5de5\u4e1a\u8d28\u91cf\u63a7\u5236\u4e2d\u7684\u5e94\u7528\u3002"}}
{"id": "2504.20047", "pdf": "https://arxiv.org/pdf/2504.20047", "abs": "https://arxiv.org/abs/2504.20047", "authors": ["Mohammad S. Ahmad", "Zan A. Naeem", "Micha\u00ebl Aupetit", "Ahmed Elmagarmid", "Mohamed Eltabakh", "Xiasong Ma", "Mourad Ouzzani", "Chaoyi Ruan"], "title": "HCT-QA: A Benchmark for Question Answering on Human-Centric Tables", "categories": ["cs.IR", "cs.AI", "cs.DB"], "comment": "12 pages", "summary": "Tabular data embedded within PDF files, web pages, and other document formats\nare prevalent across numerous sectors such as government, engineering, science,\nand business. These human-centric tables (HCTs) possess a unique combination of\nhigh business value, intricate layouts, limited operational power at scale, and\nsometimes serve as the only data source for critical insights. However, their\ncomplexity poses significant challenges to traditional data extraction,\nprocessing, and querying methods. While current solutions focus on transforming\nthese tables into relational formats for SQL queries, they fall short in\nhandling the diverse and complex layouts of HCTs and hence being amenable to\nquerying. This paper describes HCT-QA, an extensive benchmark of HCTs, natural\nlanguage queries, and related answers on thousands of tables. Our dataset\nincludes 2,188 real-world HCTs with 9,835 QA pairs and 4,679 synthetic tables\nwith 67.5K QA pairs. While HCTs can be potentially processed by different type\nof query engines, in this paper, we focus on Large Language Models as potential\nengines and assess their ability in processing and querying such tables.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86HCT-QA\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5904\u7406\u590d\u6742\u4eba\u7c7b\u4e2d\u5fc3\u8868\u683c\uff08HCTs\uff09\u7684\u80fd\u529b\uff0c\u5305\u542b\u771f\u5b9e\u548c\u5408\u6210\u8868\u683c\u53ca\u95ee\u7b54\u5bf9\u3002", "motivation": "HCTs\u5177\u6709\u9ad8\u5546\u4e1a\u4ef7\u503c\u4f46\u590d\u6742\u5e03\u5c40\uff0c\u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u5904\u7406\uff0c\u9700\u65b0\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u6784\u5efaHCT-QA\u57fa\u51c6\uff0c\u5305\u542b\u771f\u5b9e\u548c\u5408\u6210\u8868\u683c\u53ca\u95ee\u7b54\u5bf9\uff0c\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u67e5\u8be2\u80fd\u529b\u3002", "result": "\u6570\u636e\u96c6\u5305\u542b2,188\u771f\u5b9eHCTs\u548c4,679\u5408\u6210\u8868\u683c\uff0c\u5206\u522b\u67099,835\u548c67.5K\u95ee\u7b54\u5bf9\u3002", "conclusion": "HCT-QA\u4e3a\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5904\u7406\u590d\u6742\u8868\u683c\u63d0\u4f9b\u4e86\u57fa\u51c6\uff0c\u586b\u8865\u4e86\u4f20\u7edf\u65b9\u6cd5\u7684\u4e0d\u8db3\u3002"}}
{"id": "2504.20518", "pdf": "https://arxiv.org/pdf/2504.20518", "abs": "https://arxiv.org/abs/2504.20518", "authors": ["Zhongqi Wang", "Jie Zhang", "Shiguang Shan", "Xilin Chen"], "title": "Dynamic Attention Analysis for Backdoor Detection in Text-to-Image Diffusion Models", "categories": ["cs.CV"], "comment": null, "summary": "Recent studies have revealed that text-to-image diffusion models are\nvulnerable to backdoor attacks, where attackers implant stealthy textual\ntriggers to manipulate model outputs. Previous backdoor detection methods\nprimarily focus on the static features of backdoor samples. However, a vital\nproperty of diffusion models is their inherent dynamism. This study introduces\na novel backdoor detection perspective named Dynamic Attention Analysis (DAA),\nshowing that these dynamic characteristics serve as better indicators for\nbackdoor detection. Specifically, by examining the dynamic evolution of\ncross-attention maps, we observe that backdoor samples exhibit distinct feature\nevolution patterns at the $<$EOS$>$ token compared to benign samples. To\nquantify these dynamic anomalies, we first introduce DAA-I, which treats the\ntokens' attention maps as spatially independent and measures dynamic feature\nusing the Frobenius norm. Furthermore, to better capture the interactions\nbetween attention maps and refine the feature, we propose a dynamical\nsystem-based approach, referred to as DAA-S. This model formulates the spatial\ncorrelations among attention maps using a graph-based state equation and we\ntheoretically analyze the global asymptotic stability of this method. Extensive\nexperiments across five representative backdoor attack scenarios demonstrate\nthat our approach significantly surpasses existing detection methods, achieving\nan average F1 Score of 79.49% and an AUC of 87.67%. The code is available at\nhttps://github.com/Robin-WZQ/DAA.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u52a8\u6001\u6ce8\u610f\u529b\u5206\u6790\uff08DAA\uff09\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u68c0\u6d4b\u6587\u672c\u5230\u56fe\u50cf\u6269\u6563\u6a21\u578b\u4e2d\u7684\u540e\u95e8\u653b\u51fb\uff0c\u901a\u8fc7\u5206\u6790\u52a8\u6001\u7279\u5f81\u663e\u8457\u63d0\u9ad8\u4e86\u68c0\u6d4b\u6548\u679c\u3002", "motivation": "\u73b0\u6709\u540e\u95e8\u68c0\u6d4b\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u9759\u6001\u7279\u5f81\uff0c\u800c\u6269\u6563\u6a21\u578b\u7684\u52a8\u6001\u7279\u6027\u672a\u88ab\u5145\u5206\u5229\u7528\u3002\u672c\u6587\u65e8\u5728\u5229\u7528\u52a8\u6001\u7279\u5f81\u6539\u8fdb\u68c0\u6d4b\u6548\u679c\u3002", "method": "\u63d0\u51fa\u4e86\u4e24\u79cd\u65b9\u6cd5\uff1aDAA-I\uff08\u72ec\u7acb\u5206\u6790\u6ce8\u610f\u529b\u56fe\uff09\u548cDAA-S\uff08\u57fa\u4e8e\u52a8\u6001\u7cfb\u7edf\u7684\u56fe\u6a21\u578b\u5206\u6790\u7a7a\u95f4\u76f8\u5173\u6027\uff09\u3002", "result": "\u5728\u4e94\u79cd\u5178\u578b\u540e\u95e8\u653b\u51fb\u573a\u666f\u4e2d\uff0cDAA\u65b9\u6cd5\u5e73\u5747F1\u5206\u6570\u4e3a79.49%\uff0cAUC\u4e3a87.67%\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u52a8\u6001\u6ce8\u610f\u529b\u5206\u6790\u662f\u68c0\u6d4b\u6269\u6563\u6a21\u578b\u540e\u95e8\u653b\u51fb\u7684\u6709\u6548\u65b9\u6cd5\uff0c\u5176\u52a8\u6001\u7279\u5f81\u5206\u6790\u663e\u8457\u63d0\u5347\u4e86\u68c0\u6d4b\u6027\u80fd\u3002"}}
{"id": "2504.20055", "pdf": "https://arxiv.org/pdf/2504.20055", "abs": "https://arxiv.org/abs/2504.20055", "authors": ["Juan D. Pinto", "Luc Paquette"], "title": "A constraints-based approach to fully interpretable neural networks for detecting learner behaviors", "categories": ["cs.LG", "cs.AI"], "comment": "Accepted to International Conference on Educational Data Mining (EDM)\n  2025", "summary": "The increasing use of complex machine learning models in education has led to\nconcerns about their interpretability, which in turn has spurred interest in\ndeveloping explainability techniques that are both faithful to the model's\ninner workings and intelligible to human end-users. In this paper, we describe\na novel approach to creating a neural-network-based behavior detection model\nthat is interpretable by design. Our model is fully interpretable, meaning that\nthe parameters we extract for our explanations have a clear interpretation,\nfully capture the model's learned knowledge about the learner behavior of\ninterest, and can be used to create explanations that are both faithful and\nintelligible. We achieve this by implementing a series of constraints to the\nmodel that both simplify its inference process and bring it closer to a human\nconception of the task at hand. We train the model to detect gaming-the-system\nbehavior, evaluate its performance on this task, and compare its learned\npatterns to those identified by human experts. Our results show that the model\nis successfully able to learn patterns indicative of gaming-the-system behavior\nwhile providing evidence for fully interpretable explanations. We discuss the\nimplications of our approach and suggest ways to evaluate explainability using\na human-grounded approach.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8bbe\u8ba1\u4e0a\u53ef\u89e3\u91ca\u7684\u795e\u7ecf\u7f51\u7edc\u884c\u4e3a\u68c0\u6d4b\u6a21\u578b\uff0c\u901a\u8fc7\u7ea6\u675f\u6a21\u578b\u5b9e\u73b0\u5b8c\u5168\u53ef\u89e3\u91ca\u6027\uff0c\u5e76\u5728\u68c0\u6d4b\u201c\u6e38\u620f\u7cfb\u7edf\u201d\u884c\u4e3a\u4efb\u52a1\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u968f\u7740\u590d\u6742\u673a\u5668\u5b66\u4e60\u6a21\u578b\u5728\u6559\u80b2\u4e2d\u7684\u5e94\u7528\u589e\u52a0\uff0c\u5176\u53ef\u89e3\u91ca\u6027\u95ee\u9898\u5f15\u53d1\u5173\u6ce8\uff0c\u9700\u8981\u5f00\u53d1\u65e2\u5fe0\u5b9e\u4e8e\u6a21\u578b\u5185\u90e8\u673a\u5236\u53c8\u6613\u4e8e\u4eba\u7c7b\u7406\u89e3\u7684\u89e3\u91ca\u6280\u672f\u3002", "method": "\u901a\u8fc7\u4e00\u7cfb\u5217\u7ea6\u675f\u8bbe\u8ba1\u5b8c\u5168\u53ef\u89e3\u91ca\u7684\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\uff0c\u7b80\u5316\u63a8\u7406\u8fc7\u7a0b\u5e76\u4f7f\u5176\u66f4\u63a5\u8fd1\u4eba\u7c7b\u5bf9\u4efb\u52a1\u7684\u7406\u89e3\u3002", "result": "\u6a21\u578b\u6210\u529f\u5b66\u4e60\u4e86\u201c\u6e38\u620f\u7cfb\u7edf\u201d\u884c\u4e3a\u7684\u6a21\u5f0f\uff0c\u5e76\u80fd\u63d0\u4f9b\u5b8c\u5168\u53ef\u89e3\u91ca\u7684\u89e3\u91ca\uff0c\u4e0e\u4eba\u7c7b\u4e13\u5bb6\u7684\u8bc6\u522b\u6a21\u5f0f\u4e00\u81f4\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5c55\u793a\u4e86\u8bbe\u8ba1\u53ef\u89e3\u91ca\u6a21\u578b\u7684\u53ef\u884c\u6027\uff0c\u5e76\u63d0\u51fa\u4e86\u57fa\u4e8e\u4eba\u7c7b\u7406\u89e3\u7684\u89e3\u91ca\u6027\u8bc4\u4f30\u65b9\u6cd5\u3002"}}
{"id": "2504.20525", "pdf": "https://arxiv.org/pdf/2504.20525", "abs": "https://arxiv.org/abs/2504.20525", "authors": ["Huan Zheng", "Wencheng Han", "Tianyi Yan", "Cheng-zhong Xu", "Jianbing Shen"], "title": "Geometry-aware Temporal Aggregation Network for Monocular 3D Lane Detection", "categories": ["cs.CV"], "comment": null, "summary": "Monocular 3D lane detection aims to estimate 3D position of lanes from\nfrontal-view (FV) images. However, current monocular 3D lane detection methods\nsuffer from two limitations, including inaccurate geometric information of the\npredicted 3D lanes and difficulties in maintaining lane integrity. To address\nthese issues, we seek to fully exploit the potential of multiple input frames.\nFirst, we aim at enhancing the ability to perceive the geometry of scenes by\nleveraging temporal geometric consistency. Second, we strive to improve the\nintegrity of lanes by revealing more instance information from temporal\nsequences. Therefore, we propose a novel Geometry-aware Temporal Aggregation\nNetwork (GTA-Net) for monocular 3D lane detection. On one hand, we develop the\nTemporal Geometry Enhancement Module (TGEM), which exploits geometric\nconsistency across successive frames, facilitating effective geometry\nperception. On the other hand, we present the Temporal Instance-aware Query\nGeneration (TIQG), which strategically incorporates temporal cues into query\ngeneration, thereby enabling the exploration of comprehensive instance\ninformation. Experiments demonstrate that our GTA-Net achieves SoTA results,\nsurpassing existing monocular 3D lane detection solutions.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aGTA-Net\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u5229\u7528\u591a\u5e27\u8f93\u5165\u7684\u65f6\u95f4\u51e0\u4f55\u4e00\u81f4\u6027\u548c\u5b9e\u4f8b\u4fe1\u606f\uff0c\u89e3\u51b3\u4e86\u5355\u76ee3D\u8f66\u9053\u68c0\u6d4b\u4e2d\u51e0\u4f55\u4fe1\u606f\u4e0d\u51c6\u786e\u548c\u8f66\u9053\u5b8c\u6574\u6027\u96be\u4ee5\u4fdd\u6301\u7684\u95ee\u9898\u3002", "motivation": "\u5f53\u524d\u5355\u76ee3D\u8f66\u9053\u68c0\u6d4b\u65b9\u6cd5\u5b58\u5728\u51e0\u4f55\u4fe1\u606f\u4e0d\u51c6\u786e\u548c\u8f66\u9053\u5b8c\u6574\u6027\u96be\u4ee5\u4fdd\u6301\u7684\u5c40\u9650\u6027\uff0c\u4f5c\u8005\u5e0c\u671b\u901a\u8fc7\u5229\u7528\u591a\u5e27\u8f93\u5165\u7684\u65f6\u95f4\u4fe1\u606f\u6765\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86GTA-Net\u7f51\u7edc\uff0c\u5305\u542bTemporal Geometry Enhancement Module\uff08TGEM\uff09\u548cTemporal Instance-aware Query Generation\uff08TIQG\uff09\u4e24\u4e2a\u6a21\u5757\uff0c\u5206\u522b\u7528\u4e8e\u589e\u5f3a\u51e0\u4f55\u611f\u77e5\u548c\u63d0\u5347\u8f66\u9053\u5b8c\u6574\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cGTA-Net\u5728\u5355\u76ee3D\u8f66\u9053\u68c0\u6d4b\u4efb\u52a1\u4e2d\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "\u901a\u8fc7\u5145\u5206\u5229\u7528\u591a\u5e27\u8f93\u5165\u7684\u65f6\u95f4\u4fe1\u606f\uff0cGTA-Net\u663e\u8457\u63d0\u5347\u4e86\u5355\u76ee3D\u8f66\u9053\u68c0\u6d4b\u7684\u51e0\u4f55\u51c6\u786e\u6027\u548c\u8f66\u9053\u5b8c\u6574\u6027\u3002"}}
{"id": "2504.20059", "pdf": "https://arxiv.org/pdf/2504.20059", "abs": "https://arxiv.org/abs/2504.20059", "authors": ["Joey Chan", "Qiao Jin", "Nicholas Wan", "Charalampos S. Floudas", "Elisabetta Xue", "Zhiyong Lu"], "title": "Recommending Clinical Trials for Online Patient Cases using Artificial Intelligence", "categories": ["cs.IR", "cs.AI", "cs.CL"], "comment": "10 pages with 2 figures and 2 tables", "summary": "Clinical trials are crucial for assessing new treatments; however,\nrecruitment challenges - such as limited awareness, complex eligibility\ncriteria, and referral barriers - hinder their success. With the growth of\nonline platforms, patients increasingly turn to social media and health\ncommunities for support, research, and advocacy, expanding recruitment pools\nand established enrollment pathways. Recognizing this potential, we utilized\nTrialGPT, a framework that leverages a large language model (LLM) as its\nbackbone, to match 50 online patient cases (collected from published case\nreports and a social media website) to clinical trials and evaluate performance\nagainst traditional keyword-based searches. Our results show that TrialGPT\noutperforms traditional methods by 46% in identifying eligible trials, with\neach patient, on average, being eligible for around 7 trials. Additionally, our\noutreach efforts to case authors and trial organizers regarding these\npatient-trial matches yielded highly positive feedback, which we present from\nboth perspectives.", "AI": {"tldr": "TrialGPT\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5339\u914d\u60a3\u8005\u6848\u4f8b\u4e0e\u4e34\u5e8a\u8bd5\u9a8c\uff0c\u8868\u73b0\u4f18\u4e8e\u4f20\u7edf\u5173\u952e\u8bcd\u641c\u7d2246%\uff0c\u5e76\u83b7\u79ef\u6781\u53cd\u9988\u3002", "motivation": "\u89e3\u51b3\u4e34\u5e8a\u8bd5\u9a8c\u62db\u52df\u4e2d\u7684\u6311\u6218\uff0c\u5982\u60a3\u8005\u610f\u8bc6\u4e0d\u8db3\u548c\u590d\u6742\u8d44\u683c\u6807\u51c6\uff0c\u5229\u7528\u5728\u7ebf\u5e73\u53f0\u6f5c\u529b\u3002", "method": "\u4f7f\u7528TrialGPT\u6846\u67b6\uff08\u57fa\u4e8eLLM\uff09\u5339\u914d50\u4f8b\u5728\u7ebf\u60a3\u8005\u6848\u4f8b\u81f3\u4e34\u5e8a\u8bd5\u9a8c\uff0c\u5bf9\u6bd4\u4f20\u7edf\u5173\u952e\u8bcd\u641c\u7d22\u3002", "result": "TrialGPT\u8bc6\u522b\u5408\u683c\u8bd5\u9a8c\u7684\u80fd\u529b\u6bd4\u4f20\u7edf\u65b9\u6cd5\u9ad846%\uff0c\u5e73\u5747\u6bcf\u4f4d\u60a3\u8005\u5339\u914d7\u9879\u8bd5\u9a8c\u3002", "conclusion": "TrialGPT\u5728\u4e34\u5e8a\u8bd5\u9a8c\u5339\u914d\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4e14\u83b7\u5f97\u60a3\u8005\u548c\u8bd5\u9a8c\u7ec4\u7ec7\u8005\u7684\u79ef\u6781\u53cd\u9988\u3002"}}
{"id": "2504.20530", "pdf": "https://arxiv.org/pdf/2504.20530", "abs": "https://arxiv.org/abs/2504.20530", "authors": ["Wenxuan Liu", "Xian Zhong", "Zhuo Zhou", "Siyuan Yang", "Chia-Wen Lin", "Alex Chichung Kot"], "title": "Beyond the Horizon: Decoupling UAVs Multi-View Action Recognition via Partial Order Transfer", "categories": ["cs.CV"], "comment": "11 pages", "summary": "Action recognition in unmanned aerial vehicles (UAVs) poses unique challenges\ndue to significant view variations along the vertical spatial axis. Unlike\ntraditional ground-based settings, UAVs capture actions from a wide range of\naltitudes, resulting in considerable appearance discrepancies. We introduce a\nmulti-view formulation tailored to varying UAV altitudes and empirically\nobserve a partial order among views, where recognition accuracy consistently\ndecreases as the altitude increases. This motivates a novel approach that\nexplicitly models the hierarchical structure of UAV views to improve\nrecognition performance across altitudes. To this end, we propose the Partial\nOrder Guided Multi-View Network (POG-MVNet), designed to address drastic view\nvariations by effectively leveraging view-dependent information across\ndifferent altitude levels. The framework comprises three key components: a View\nPartition (VP) module, which uses the head-to-body ratio to group views by\naltitude; an Order-aware Feature Decoupling (OFD) module, which disentangles\naction-relevant and view-specific features under partial order guidance; and an\nAction Partial Order Guide (APOG), which leverages the partial order to\ntransfer informative knowledge from easier views to support learning in more\nchallenging ones. We conduct experiments on Drone-Action, MOD20, and UAV\ndatasets, demonstrating that POG-MVNet significantly outperforms competing\nmethods. For example, POG-MVNet achieves a 4.7% improvement on Drone-Action\ndataset and a 3.5% improvement on UAV dataset compared to state-of-the-art\nmethods ASAT and FAR. The code for POG-MVNet will be made available soon.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u65e0\u4eba\u673a\uff08UAV\uff09\u52a8\u4f5c\u8bc6\u522b\u7684\u591a\u89c6\u89d2\u65b9\u6cd5POG-MVNet\uff0c\u901a\u8fc7\u5efa\u6a21\u89c6\u89d2\u7684\u5c42\u6b21\u7ed3\u6784\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4e0d\u540c\u9ad8\u5ea6\u4e0b\u7684\u8bc6\u522b\u6027\u80fd\u3002", "motivation": "\u65e0\u4eba\u673a\u52a8\u4f5c\u8bc6\u522b\u9762\u4e34\u89c6\u89d2\u53d8\u5316\u5927\u7684\u6311\u6218\uff0c\u5c24\u5176\u662f\u5782\u76f4\u7a7a\u95f4\u8f74\u4e0a\u7684\u9ad8\u5ea6\u53d8\u5316\u5bfc\u81f4\u7684\u5916\u89c2\u5dee\u5f02\u3002\u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u5e94\u5bf9\u8fd9\u79cd\u591a\u89c6\u89d2\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86POG-MVNet\u6846\u67b6\uff0c\u5305\u62ec\u4e09\u4e2a\u6a21\u5757\uff1aView Partition\uff08VP\uff09\u6309\u9ad8\u5ea6\u5206\u7ec4\u89c6\u89d2\uff0cOrder-aware Feature Decoupling\uff08OFD\uff09\u5206\u79bb\u52a8\u4f5c\u548c\u89c6\u89d2\u7279\u5f81\uff0cAction Partial Order Guide\uff08APOG\uff09\u5229\u7528\u89c6\u89d2\u95f4\u7684\u90e8\u5206\u987a\u5e8f\u4f20\u9012\u77e5\u8bc6\u3002", "result": "\u5728Drone-Action\u3001MOD20\u548cUAV\u6570\u636e\u96c6\u4e0a\uff0cPOG-MVNet\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u4f8b\u5982\u5728Drone-Action\u4e0a\u63d0\u53474.7%\uff0c\u5728UAV\u4e0a\u63d0\u53473.5%\u3002", "conclusion": "POG-MVNet\u901a\u8fc7\u5efa\u6a21\u89c6\u89d2\u5c42\u6b21\u7ed3\u6784\u548c\u77e5\u8bc6\u4f20\u9012\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u65e0\u4eba\u673a\u52a8\u4f5c\u8bc6\u522b\u4e2d\u7684\u591a\u89c6\u89d2\u95ee\u9898\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2504.20069", "pdf": "https://arxiv.org/pdf/2504.20069", "abs": "https://arxiv.org/abs/2504.20069", "authors": ["Junhong Lai", "Jiyu Wei", "Lin Yao", "Yueming Wang"], "title": "A Simple Review of EEG Foundation Models: Datasets, Advancements and Future Perspectives", "categories": ["cs.LG", "cs.AI", "eess.SP"], "comment": null, "summary": "Electroencephalogram (EEG) signals play a crucial role in understanding brain\nactivity and diagnosing neurological disorders. This review focuses on the\nrecent development of EEG foundation models(EEG-FMs), which have shown great\npotential in processing and analyzing EEG data. We discuss various EEG-FMs,\nincluding their architectures, pre-training strategies, their pre-training and\ndownstream datasets and other details. The review also highlights the\nchallenges and future directions in this field, aiming to provide a\ncomprehensive overview for researchers and practitioners interested in EEG\nanalysis and related EEG-FMs.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86EEG\u57fa\u7840\u6a21\u578b\uff08EEG-FMs\uff09\u7684\u6700\u65b0\u53d1\u5c55\uff0c\u5305\u62ec\u5176\u67b6\u6784\u3001\u9884\u8bad\u7ec3\u7b56\u7565\u3001\u6570\u636e\u96c6\u53ca\u6311\u6218\u4e0e\u672a\u6765\u65b9\u5411\u3002", "motivation": "EEG\u4fe1\u53f7\u5728\u7406\u89e3\u5927\u8111\u6d3b\u52a8\u548c\u8bca\u65ad\u795e\u7ecf\u75be\u75c5\u4e2d\u81f3\u5173\u91cd\u8981\uff0cEEG-FMs\u4e3aEEG\u6570\u636e\u5904\u7406\u63d0\u4f9b\u4e86\u65b0\u65b9\u6cd5\u3002", "method": "\u8ba8\u8bba\u4e86\u591a\u79cdEEG-FMs\u7684\u67b6\u6784\u3001\u9884\u8bad\u7ec3\u7b56\u7565\u53ca\u6570\u636e\u96c6\u3002", "result": "EEG-FMs\u5728EEG\u6570\u636e\u5904\u7406\u4e2d\u5c55\u73b0\u51fa\u5de8\u5927\u6f5c\u529b\u3002", "conclusion": "\u7efc\u8ff0\u4e3a\u7814\u7a76\u8005\u63d0\u4f9b\u4e86EEG-FMs\u7684\u5168\u9762\u6982\u8ff0\uff0c\u5e76\u6307\u51fa\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002"}}
{"id": "2504.20541", "pdf": "https://arxiv.org/pdf/2504.20541", "abs": "https://arxiv.org/abs/2504.20541", "authors": ["Daniele Pannone", "Danilo Avola"], "title": "Autoencoder Models for Point Cloud Environmental Synthesis from WiFi Channel State Information: A Preliminary Study", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "This paper introduces a deep learning framework for generating point clouds\nfrom WiFi Channel State Information data. We employ a two-stage autoencoder\napproach: a PointNet autoencoder with convolutional layers for point cloud\ngeneration, and a Convolutional Neural Network autoencoder to map CSI data to a\nmatching latent space. By aligning these latent spaces, our method enables\naccurate environmental point cloud reconstruction from WiFi data. Experimental\nresults validate the effectiveness of our approach, highlighting its potential\nfor wireless sensing and environmental mapping applications.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eWiFi\u4fe1\u9053\u72b6\u6001\u4fe1\u606f\uff08CSI\uff09\u6570\u636e\u751f\u6210\u70b9\u4e91\u7684\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\u3002", "motivation": "\u5229\u7528WiFi\u6570\u636e\u8fdb\u884c\u73af\u5883\u70b9\u4e91\u91cd\u5efa\uff0c\u4e3a\u65e0\u7ebf\u4f20\u611f\u548c\u73af\u5883\u6620\u5c04\u63d0\u4f9b\u65b0\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u81ea\u7f16\u7801\u5668\uff1aPointNet\u81ea\u7f16\u7801\u5668\u751f\u6210\u70b9\u4e91\uff0cCNN\u81ea\u7f16\u7801\u5668\u5c06CSI\u6570\u636e\u6620\u5c04\u5230\u5339\u914d\u7684\u6f5c\u5728\u7a7a\u95f4\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u80fd\u591f\u51c6\u786e\u91cd\u5efa\u73af\u5883\u70b9\u4e91\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u65e0\u7ebf\u4f20\u611f\u548c\u73af\u5883\u6620\u5c04\u9886\u57df\u5177\u6709\u6f5c\u5728\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2504.20073", "pdf": "https://arxiv.org/pdf/2504.20073", "abs": "https://arxiv.org/abs/2504.20073", "authors": ["Zihan Wang", "Kangrui Wang", "Qineng Wang", "Pingyue Zhang", "Linjie Li", "Zhengyuan Yang", "Kefan Yu", "Minh Nhat Nguyen", "Licheng Liu", "Eli Gottlieb", "Monica Lam", "Yiping Lu", "Kyunghyun Cho", "Jiajun Wu", "Li Fei-Fei", "Lijuan Wang", "Yejin Choi", "Manling Li"], "title": "RAGEN: Understanding Self-Evolution in LLM Agents via Multi-Turn Reinforcement Learning", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Training large language models (LLMs) as interactive agents presents unique\nchallenges including long-horizon decision making and interacting with\nstochastic environment feedback. While reinforcement learning (RL) has enabled\nprogress in static tasks, multi-turn agent RL training remains underexplored.\nWe propose StarPO (State-Thinking-Actions-Reward Policy Optimization), a\ngeneral framework for trajectory-level agent RL, and introduce RAGEN, a modular\nsystem for training and evaluating LLM agents. Our study on three stylized\nenvironments reveals three core findings. First, our agent RL training shows a\nrecurring mode of Echo Trap where reward variance cliffs and gradient spikes;\nwe address this with StarPO-S, a stabilized variant with trajectory filtering,\ncritic incorporation, and decoupled clipping. Second, we find the shaping of RL\nrollouts would benefit from diverse initial states, medium interaction\ngranularity and more frequent sampling. Third, we show that without\nfine-grained, reasoning-aware reward signals, agent reasoning hardly emerge\nthrough multi-turn RL and they may show shallow strategies or hallucinated\nthoughts. Code and environments are available at\nhttps://github.com/RAGEN-AI/RAGEN.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86StarPO\u6846\u67b6\u548cRAGEN\u7cfb\u7edf\uff0c\u7528\u4e8e\u8bad\u7ec3LLM\u4f5c\u4e3a\u4ea4\u4e92\u5f0f\u4ee3\u7406\uff0c\u89e3\u51b3\u4e86\u591a\u8f6eRL\u8bad\u7ec3\u4e2d\u7684\u6311\u6218\uff0c\u5982Echo Trap\u6a21\u5f0f\uff0c\u5e76\u63d0\u51fa\u4e86\u4f18\u5316\u65b9\u6cd5\u3002", "motivation": "\u8bad\u7ec3\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u4f5c\u4e3a\u4ea4\u4e92\u5f0f\u4ee3\u7406\u9762\u4e34\u957f\u671f\u51b3\u7b56\u548c\u968f\u673a\u73af\u5883\u53cd\u9988\u7684\u6311\u6218\uff0c\u591a\u8f6e\u4ee3\u7406RL\u8bad\u7ec3\u5c1a\u672a\u5145\u5206\u63a2\u7d22\u3002", "method": "\u63d0\u51faStarPO\u6846\u67b6\u548cRAGEN\u7cfb\u7edf\uff0c\u901a\u8fc7\u8f68\u8ff9\u7ea7RL\u8bad\u7ec3\u4ee3\u7406\uff0c\u5e76\u5f15\u5165StarPO-S\u4f18\u5316\u65b9\u6cd5\u89e3\u51b3Echo Trap\u95ee\u9898\u3002", "result": "\u7814\u7a76\u53d1\u73b0Echo Trap\u6a21\u5f0f\uff0c\u4f18\u5316\u65b9\u6cd5\u6709\u6548\uff1bRL\u8bad\u7ec3\u9700\u591a\u6837\u521d\u59cb\u72b6\u6001\u548c\u9002\u4e2d\u4ea4\u4e92\u7c92\u5ea6\uff1b\u7ec6\u7c92\u5ea6\u5956\u52b1\u4fe1\u53f7\u5bf9\u4ee3\u7406\u63a8\u7406\u81f3\u5173\u91cd\u8981\u3002", "conclusion": "StarPO\u548cRAGEN\u4e3aLLM\u4ee3\u7406RL\u8bad\u7ec3\u63d0\u4f9b\u4e86\u6709\u6548\u6846\u67b6\uff0c\u4f18\u5316\u65b9\u6cd5\u89e3\u51b3\u4e86\u5173\u952e\u95ee\u9898\uff0c\u5f3a\u8c03\u4e86\u5956\u52b1\u4fe1\u53f7\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2504.20599", "pdf": "https://arxiv.org/pdf/2504.20599", "abs": "https://arxiv.org/abs/2504.20599", "authors": ["Qiaochu Wang", "Chufeng Xiao", "Manfred Lau", "Hongbo Fu"], "title": "PartHOI: Part-based Hand-Object Interaction Transfer via Generalized Cylinders", "categories": ["cs.CV"], "comment": "14 pages, 12 figures, this paper has been accepted by Computational\n  Visual Media Journal (CVMJ) but has not been published yet", "summary": "Learning-based methods to understand and model hand-object interactions (HOI)\nrequire a large amount of high-quality HOI data. One way to create HOI data is\nto transfer hand poses from a source object to another based on the objects'\ngeometry. However, current methods for transferring hand poses between objects\nrely on shape matching, limiting the ability to transfer poses across different\ncategories due to differences in their shapes and sizes. We observe that HOI\noften involves specific semantic parts of objects, which often have more\nconsistent shapes across categories. In addition, constructing size-invariant\ncorrespondences between these parts is important for cross-category transfer.\nBased on these insights, we introduce a novel method PartHOI for part-based HOI\ntransfer. Using a generalized cylinder representation to parameterize an object\nparts' geometry, PartHOI establishes a robust geometric correspondence between\nobject parts, and enables the transfer of contact points. Given the transferred\npoints, we optimize a hand pose to fit the target object well. Qualitative and\nquantitative results demonstrate that our method can generalize HOI transfers\nwell even for cross-category objects, and produce high-fidelity results that\nare superior to the existing methods.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u90e8\u4ef6\u7684\u624b-\u7269\u4f53\u4ea4\u4e92\uff08HOI\uff09\u8f6c\u79fb\u65b9\u6cd5PartHOI\uff0c\u901a\u8fc7\u53c2\u6570\u5316\u7269\u4f53\u90e8\u4ef6\u51e0\u4f55\u5f62\u72b6\uff0c\u5b9e\u73b0\u8de8\u7c7b\u522b\u7684\u9ad8\u8d28\u91cfHOI\u6570\u636e\u751f\u6210\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u5f62\u72b6\u5339\u914d\uff0c\u96be\u4ee5\u8de8\u7c7b\u522b\u8f6c\u79fb\u624b\u90e8\u59ff\u52bf\uff0c\u800cHOI\u901a\u5e38\u6d89\u53ca\u7269\u4f53\u7684\u7279\u5b9a\u8bed\u4e49\u90e8\u4ef6\uff0c\u8fd9\u4e9b\u90e8\u4ef6\u5728\u4e0d\u540c\u7c7b\u522b\u95f4\u5f62\u72b6\u66f4\u4e00\u81f4\u3002", "method": "\u4f7f\u7528\u5e7f\u4e49\u5706\u67f1\u4f53\u8868\u793a\u53c2\u6570\u5316\u7269\u4f53\u90e8\u4ef6\u51e0\u4f55\uff0c\u5efa\u7acb\u90e8\u4ef6\u95f4\u7684\u51e0\u4f55\u5bf9\u5e94\u5173\u7cfb\uff0c\u8f6c\u79fb\u63a5\u89e6\u70b9\u5e76\u4f18\u5316\u624b\u90e8\u59ff\u52bf\u4ee5\u9002\u5e94\u76ee\u6807\u7269\u4f53\u3002", "result": "\u5b9a\u6027\u548c\u5b9a\u91cf\u7ed3\u679c\u8868\u660e\uff0cPartHOI\u5728\u8de8\u7c7b\u522bHOI\u8f6c\u79fb\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u751f\u6210\u7ed3\u679c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "PartHOI\u901a\u8fc7\u90e8\u4ef6\u51e0\u4f55\u5bf9\u5e94\u548c\u63a5\u89e6\u70b9\u8f6c\u79fb\uff0c\u5b9e\u73b0\u4e86\u8de8\u7c7b\u522b\u7684\u9ad8\u4fdd\u771fHOI\u6570\u636e\u751f\u6210\uff0c\u4e3a\u5b66\u4e60HOI\u63d0\u4f9b\u4e86\u9ad8\u8d28\u91cf\u6570\u636e\u652f\u6301\u3002"}}
{"id": "2504.20074", "pdf": "https://arxiv.org/pdf/2504.20074", "abs": "https://arxiv.org/abs/2504.20074", "authors": ["Khurram Khalil", "Khaza Anuarul Hoque"], "title": "EPSILON: Adaptive Fault Mitigation in Approximate Deep Neural Network using Statistical Signatures", "categories": ["cs.DC", "cs.AI", "cs.LG"], "comment": "Accepted at the International Joint Conference on Neural Networks\n  (IJCNN), June 30th - July 5th, 2025 in Rome, Italy", "summary": "The increasing adoption of approximate computing in deep neural network\naccelerators (AxDNNs) promises significant energy efficiency gains. However,\npermanent faults in AxDNNs can severely degrade their performance compared to\ntheir accurate counterparts (AccDNNs). Traditional fault detection and\nmitigation approaches, while effective for AccDNNs, introduce substantial\noverhead and latency, making them impractical for energy-constrained real-time\ndeployment. To address this, we introduce EPSILON, a lightweight framework that\nleverages pre-computed statistical signatures and layer-wise importance metrics\nfor efficient fault detection and mitigation in AxDNNs. Our framework\nintroduces a novel non-parametric pattern-matching algorithm that enables\nconstant-time fault detection without interrupting normal execution while\ndynamically adapting to different network architectures and fault patterns.\nEPSILON maintains model accuracy by intelligently adjusting mitigation\nstrategies based on a statistical analysis of weight distribution and layer\ncriticality while preserving the energy benefits of approximate computing.\nExtensive evaluations across various approximate multipliers, AxDNN\narchitectures, popular datasets (MNIST, CIFAR-10, CIFAR-100, ImageNet-1k), and\nfault scenarios demonstrate that EPSILON maintains 80.05\\% accuracy while\noffering 22\\% improvement in inference time and 28\\% improvement in energy\nefficiency, establishing EPSILON as a practical solution for deploying reliable\nAxDNNs in safety-critical edge applications.", "AI": {"tldr": "EPSILON\u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u8fd1\u4f3c\u8ba1\u7b97\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\uff08AxDNNs\uff09\u4e2d\u9ad8\u6548\u68c0\u6d4b\u548c\u7f13\u89e3\u6545\u969c\uff0c\u4fdd\u6301\u9ad8\u7cbe\u5ea6\u548c\u80fd\u6548\u3002", "motivation": "\u8fd1\u4f3c\u8ba1\u7b97\u5728AxDNNs\u4e2d\u80fd\u663e\u8457\u63d0\u9ad8\u80fd\u6548\uff0c\u4f46\u6c38\u4e45\u6027\u6545\u969c\u4f1a\u4e25\u91cd\u964d\u4f4e\u6027\u80fd\uff0c\u4f20\u7edf\u65b9\u6cd5\u56e0\u9ad8\u5f00\u9500\u548c\u5ef6\u8fdf\u4e0d\u9002\u7528\u3002", "method": "EPSILON\u5229\u7528\u9884\u8ba1\u7b97\u7684\u7edf\u8ba1\u7279\u5f81\u548c\u5c42\u91cd\u8981\u6027\u6307\u6807\uff0c\u91c7\u7528\u975e\u53c2\u6570\u6a21\u5f0f\u5339\u914d\u7b97\u6cd5\u5b9e\u73b0\u5b9e\u65f6\u6545\u969c\u68c0\u6d4b\u548c\u52a8\u6001\u9002\u5e94\u3002", "result": "EPSILON\u5728\u591a\u79cd\u573a\u666f\u4e0b\u4fdd\u630180.05%\u7684\u51c6\u786e\u7387\uff0c\u63a8\u7406\u65f6\u95f4\u63d0\u534722%\uff0c\u80fd\u6548\u63d0\u534728%\u3002", "conclusion": "EPSILON\u662f\u5b89\u5168\u5173\u952e\u8fb9\u7f18\u5e94\u7528\u4e2d\u90e8\u7f72\u53ef\u9760AxDNNs\u7684\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2504.20602", "pdf": "https://arxiv.org/pdf/2504.20602", "abs": "https://arxiv.org/abs/2504.20602", "authors": ["Siwei Wang", "Zhiwei Chen", "Liujuan Cao", "Rongrong Ji"], "title": "Purifying, Labeling, and Utilizing: A High-Quality Pipeline for Small Object Detection", "categories": ["cs.CV"], "comment": null, "summary": "Small object detection is a broadly investigated research task and is\ncommonly conceptualized as a \"pipeline-style\" engineering process. In the\nupstream, images serve as raw materials for processing in the detection\npipeline, where pre-trained models are employed to generate initial feature\nmaps. In the midstream, an assigner selects training positive and negative\nsamples. Subsequently, these samples and features are fed into the downstream\nfor classification and regression. Previous small object detection methods\noften focused on improving isolated stages of the pipeline, thereby neglecting\nholistic optimization and consequently constraining overall performance gains.\nTo address this issue, we have optimized three key aspects, namely Purifying,\nLabeling, and Utilizing, in this pipeline, proposing a high-quality Small\nobject detection framework termed PLUSNet. Specifically, PLUSNet comprises\nthree sequential components: the Hierarchical Feature Purifier (HFP) for\npurifying upstream features, the Multiple Criteria Label Assignment (MCLA) for\nimproving the quality of midstream training samples, and the Frequency\nDecoupled Head (FDHead) for more effectively exploiting information to\naccomplish downstream tasks. The proposed PLUS modules are readily integrable\ninto various object detectors, thus enhancing their detection capabilities in\nmulti-scale scenarios. Extensive experiments demonstrate the proposed PLUSNet\nconsistently achieves significant and consistent improvements across multiple\ndatasets for small object detection.", "AI": {"tldr": "PLUSNet\u4f18\u5316\u4e86\u5c0f\u76ee\u6807\u68c0\u6d4b\u6d41\u7a0b\u7684\u4e09\u4e2a\u5173\u952e\u73af\u8282\uff08Purifying\u3001Labeling\u3001Utilizing\uff09\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u8d28\u91cf\u7684\u5c0f\u76ee\u6807\u68c0\u6d4b\u6846\u67b6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u5b64\u7acb\u4f18\u5316\u68c0\u6d4b\u6d41\u7a0b\u7684\u67d0\u4e2a\u9636\u6bb5\uff0c\u5ffd\u89c6\u4e86\u6574\u4f53\u4f18\u5316\uff0c\u9650\u5236\u4e86\u6027\u80fd\u63d0\u5347\u3002", "method": "PLUSNet\u5305\u542b\u4e09\u4e2a\u6a21\u5757\uff1aHFP\uff08\u51c0\u5316\u4e0a\u6e38\u7279\u5f81\uff09\u3001MCLA\uff08\u4f18\u5316\u4e2d\u6e38\u6837\u672c\u5206\u914d\uff09\u3001FDHead\uff08\u9ad8\u6548\u5229\u7528\u4e0b\u6e38\u4fe1\u606f\uff09\u3002", "result": "\u5b9e\u9a8c\u8868\u660ePLUSNet\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u663e\u8457\u63d0\u5347\u4e86\u5c0f\u76ee\u6807\u68c0\u6d4b\u6027\u80fd\u3002", "conclusion": "PLUSNet\u901a\u8fc7\u6574\u4f53\u4f18\u5316\u68c0\u6d4b\u6d41\u7a0b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5c0f\u76ee\u6807\u68c0\u6d4b\u7684\u6548\u679c\uff0c\u4e14\u6613\u4e8e\u96c6\u6210\u5230\u73b0\u6709\u68c0\u6d4b\u5668\u4e2d\u3002"}}
{"id": "2504.20607", "pdf": "https://arxiv.org/pdf/2504.20607", "abs": "https://arxiv.org/abs/2504.20607", "authors": ["Hao Tian", "Rui Liu", "Wen Shen", "Yilong Hu", "Zhihao Zheng", "Xiaolin Qin"], "title": "EfficientHuman: Efficient Training and Reconstruction of Moving Human using Articulated 2D Gaussian", "categories": ["cs.CV"], "comment": "11 pages, 3 figures", "summary": "3D Gaussian Splatting (3DGS) has been recognized as a pioneering technique in\nscene reconstruction and novel view synthesis. Recent work on reconstructing\nthe 3D human body using 3DGS attempts to leverage prior information on human\npose to enhance rendering quality and improve training speed. However, it\nstruggles to effectively fit dynamic surface planes due to multi-view\ninconsistency and redundant Gaussians. This inconsistency arises because\nGaussian ellipsoids cannot accurately represent the surfaces of dynamic\nobjects, which hinders the rapid reconstruction of the dynamic human body.\nMeanwhile, the prevalence of redundant Gaussians means that the training time\nof these works is still not ideal for quickly fitting a dynamic human body. To\naddress these, we propose EfficientHuman, a model that quickly accomplishes the\ndynamic reconstruction of the human body using Articulated 2D Gaussian while\nensuring high rendering quality. The key innovation involves encoding Gaussian\nsplats as Articulated 2D Gaussian surfels in canonical space and then\ntransforming them to pose space via Linear Blend Skinning (LBS) to achieve\nefficient pose transformations. Unlike 3D Gaussians, Articulated 2D Gaussian\nsurfels can quickly conform to the dynamic human body while ensuring\nview-consistent geometries. Additionally, we introduce a pose calibration\nmodule and an LBS optimization module to achieve precise fitting of dynamic\nhuman poses, enhancing the model's performance. Extensive experiments on the\nZJU-MoCap dataset demonstrate that EfficientHuman achieves rapid 3D dynamic\nhuman reconstruction in less than a minute on average, which is 20 seconds\nfaster than the current state-of-the-art method, while also reducing the number\nof redundant Gaussians.", "AI": {"tldr": "EfficientHuman\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eArticulated 2D Gaussian surfels\u7684\u52a8\u6001\u4eba\u4f53\u91cd\u5efa\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e863DGS\u5728\u52a8\u6001\u4eba\u4f53\u91cd\u5efa\u4e2d\u7684\u591a\u89c6\u89d2\u4e0d\u4e00\u81f4\u548c\u9ad8\u65af\u5197\u4f59\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8bad\u7ec3\u901f\u5ea6\u548c\u6e32\u67d3\u8d28\u91cf\u3002", "motivation": "3DGS\u5728\u52a8\u6001\u4eba\u4f53\u91cd\u5efa\u4e2d\u56e0\u591a\u89c6\u89d2\u4e0d\u4e00\u81f4\u548c\u9ad8\u65af\u5197\u4f59\u95ee\u9898\u5bfc\u81f4\u6548\u679c\u4e0d\u4f73\uff0cEfficientHuman\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u4f7f\u7528Articulated 2D Gaussian surfels\u5728\u89c4\u8303\u7a7a\u95f4\u4e2d\u7f16\u7801\uff0c\u5e76\u901a\u8fc7LBS\u8f6c\u6362\u5230\u59ff\u6001\u7a7a\u95f4\uff0c\u7ed3\u5408\u59ff\u6001\u6821\u51c6\u548cLBS\u4f18\u5316\u6a21\u5757\u3002", "result": "\u5728ZJU-MoCap\u6570\u636e\u96c6\u4e0a\uff0cEfficientHuman\u5e73\u5747\u91cd\u5efa\u65f6\u95f4\u5c11\u4e8e1\u5206\u949f\uff0c\u6bd4\u73b0\u6709\u65b9\u6cd5\u5feb20\u79d2\uff0c\u4e14\u51cf\u5c11\u4e86\u9ad8\u65af\u5197\u4f59\u3002", "conclusion": "EfficientHuman\u5728\u52a8\u6001\u4eba\u4f53\u91cd\u5efa\u4e2d\u5b9e\u73b0\u4e86\u5feb\u901f\u4e14\u9ad8\u8d28\u91cf\u7684\u6e32\u67d3\uff0c\u89e3\u51b3\u4e863DGS\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2504.20079", "pdf": "https://arxiv.org/pdf/2504.20079", "abs": "https://arxiv.org/abs/2504.20079", "authors": ["Xuan Rao", "Bo Zhao", "Derong Liu", "Cesare Alippi"], "title": "FX-DARTS: Designing Topology-unconstrained Architectures with Differentiable Architecture Search and Entropy-based Super-network Shrinking", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Strong priors are imposed on the search space of Differentiable Architecture\nSearch (DARTS), such that cells of the same type share the same topological\nstructure and each intermediate node retains two operators from distinct nodes.\nWhile these priors reduce optimization difficulties and improve the\napplicability of searched architectures, they hinder the subsequent development\nof automated machine learning (Auto-ML) and prevent the optimization algorithm\nfrom exploring more powerful neural networks through improved architectural\nflexibility. This paper aims to reduce these prior constraints by eliminating\nrestrictions on cell topology and modifying the discretization mechanism for\nsuper-networks. Specifically, the Flexible DARTS (FX-DARTS) method, which\nleverages an Entropy-based Super-Network Shrinking (ESS) framework, is\npresented to address the challenges arising from the elimination of prior\nconstraints. Notably, FX-DARTS enables the derivation of neural architectures\nwithout strict prior rules while maintaining the stability in the enlarged\nsearch space. Experimental results on image classification benchmarks\ndemonstrate that FX-DARTS is capable of exploring a set of neural architectures\nwith competitive trade-offs between performance and computational complexity\nwithin a single search procedure.", "AI": {"tldr": "FX-DARTS\u901a\u8fc7\u6d88\u9664DARTS\u4e2d\u7684\u5148\u9a8c\u7ea6\u675f\uff0c\u63d0\u51faESS\u6846\u67b6\uff0c\u63d0\u5347\u67b6\u6784\u7075\u6d3b\u6027\uff0c\u5b9e\u73b0\u7a33\u5b9a\u641c\u7d22\u3002", "motivation": "DARTS\u4e2d\u7684\u5f3a\u5148\u9a8c\u7ea6\u675f\u9650\u5236\u4e86\u67b6\u6784\u641c\u7d22\u7684\u7075\u6d3b\u6027\uff0c\u963b\u788d\u4e86Auto-ML\u7684\u53d1\u5c55\u3002", "method": "\u63d0\u51faFX-DARTS\u65b9\u6cd5\uff0c\u53d6\u6d88\u5355\u5143\u62d3\u6251\u9650\u5236\uff0c\u6539\u8fdb\u8d85\u7f51\u7edc\u79bb\u6563\u5316\u673a\u5236\uff0c\u5229\u7528ESS\u6846\u67b6\u7a33\u5b9a\u641c\u7d22\u3002", "result": "\u5728\u56fe\u50cf\u5206\u7c7b\u4efb\u52a1\u4e2d\uff0cFX-DARTS\u80fd\u641c\u7d22\u5230\u6027\u80fd\u4e0e\u8ba1\u7b97\u590d\u6742\u5ea6\u5e73\u8861\u7684\u67b6\u6784\u3002", "conclusion": "FX-DARTS\u901a\u8fc7\u51cf\u5c11\u5148\u9a8c\u7ea6\u675f\uff0c\u63d0\u5347\u4e86\u67b6\u6784\u641c\u7d22\u7684\u7075\u6d3b\u6027\u548c\u7a33\u5b9a\u6027\u3002"}}
{"id": "2504.20629", "pdf": "https://arxiv.org/pdf/2504.20629", "abs": "https://arxiv.org/abs/2504.20629", "authors": ["Jeongsoo Choi", "Ji-Hoon Kim", "Kim Sung-Bin", "Tae-Hyun Oh", "Joon Son Chung"], "title": "AlignDiT: Multimodal Aligned Diffusion Transformer for Synchronized Speech Generation", "categories": ["cs.CV", "cs.AI", "cs.MM"], "comment": null, "summary": "In this paper, we address the task of multimodal-to-speech generation, which\naims to synthesize high-quality speech from multiple input modalities: text,\nvideo, and reference audio. This task has gained increasing attention due to\nits wide range of applications, such as film production, dubbing, and virtual\navatars. Despite recent progress, existing methods still suffer from\nlimitations in speech intelligibility, audio-video synchronization, speech\nnaturalness, and voice similarity to the reference speaker. To address these\nchallenges, we propose AlignDiT, a multimodal Aligned Diffusion Transformer\nthat generates accurate, synchronized, and natural-sounding speech from aligned\nmultimodal inputs. Built upon the in-context learning capability of the DiT\narchitecture, AlignDiT explores three effective strategies to align multimodal\nrepresentations. Furthermore, we introduce a novel multimodal classifier-free\nguidance mechanism that allows the model to adaptively balance information from\neach modality during speech synthesis. Extensive experiments demonstrate that\nAlignDiT significantly outperforms existing methods across multiple benchmarks\nin terms of quality, synchronization, and speaker similarity. Moreover,\nAlignDiT exhibits strong generalization capability across various multimodal\ntasks, such as video-to-speech synthesis and visual forced alignment,\nconsistently achieving state-of-the-art performance. The demo page is available\nat https://mm.kaist.ac.kr/projects/AlignDiT .", "AI": {"tldr": "AlignDiT\u662f\u4e00\u79cd\u591a\u6a21\u6001\u5bf9\u9f50\u6269\u6563\u53d8\u6362\u5668\uff0c\u7528\u4e8e\u4ece\u6587\u672c\u3001\u89c6\u9891\u548c\u53c2\u8003\u97f3\u9891\u751f\u6210\u9ad8\u8d28\u91cf\u8bed\u97f3\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u8bed\u97f3\u6e05\u6670\u5ea6\u3001\u540c\u6b65\u6027\u548c\u81ea\u7136\u6027\u4e0a\u7684\u4e0d\u8db3\u3002", "motivation": "\u591a\u6a21\u6001\u8bed\u97f3\u751f\u6210\u5728\u7535\u5f71\u5236\u4f5c\u3001\u914d\u97f3\u548c\u865a\u62df\u5f62\u8c61\u7b49\u9886\u57df\u6709\u5e7f\u6cdb\u5e94\u7528\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5728\u8bed\u97f3\u6e05\u6670\u5ea6\u3001\u540c\u6b65\u6027\u548c\u81ea\u7136\u6027\u4e0a\u5b58\u5728\u5c40\u9650\u3002", "method": "\u63d0\u51faAlignDiT\uff0c\u5229\u7528\u6269\u6563\u53d8\u6362\u5668\u7684\u4e0a\u4e0b\u6587\u5b66\u4e60\u80fd\u529b\uff0c\u901a\u8fc7\u4e09\u79cd\u7b56\u7565\u5bf9\u9f50\u591a\u6a21\u6001\u8868\u793a\uff0c\u5e76\u5f15\u5165\u591a\u6a21\u6001\u65e0\u5206\u7c7b\u5668\u5f15\u5bfc\u673a\u5236\u3002", "result": "\u5b9e\u9a8c\u8868\u660eAlignDiT\u5728\u8d28\u91cf\u3001\u540c\u6b65\u6027\u548c\u8bf4\u8bdd\u4eba\u76f8\u4f3c\u6027\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u5728\u591a\u6a21\u6001\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "AlignDiT\u5728\u591a\u6a21\u6001\u8bed\u97f3\u751f\u6210\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5177\u6709\u5e7f\u6cdb\u7684\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2504.20080", "pdf": "https://arxiv.org/pdf/2504.20080", "abs": "https://arxiv.org/abs/2504.20080", "authors": ["Xuan Rao", "Bo Zhao", "Derong Liu"], "title": "DNAD: Differentiable Neural Architecture Distillation", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "To meet the demand for designing efficient neural networks with appropriate\ntrade-offs between model performance (e.g., classification accuracy) and\ncomputational complexity, the differentiable neural architecture distillation\n(DNAD) algorithm is developed based on two cores, namely search by deleting and\nsearch by imitating. Primarily, to derive neural architectures in a space where\ncells of the same type no longer share the same topology, the super-network\nprogressive shrinking (SNPS) algorithm is developed based on the framework of\ndifferentiable architecture search (DARTS), i.e., search by deleting. Unlike\nconventional DARTS-based approaches which yield neural architectures with\nsimple structures and derive only one architecture during the search procedure,\nSNPS is able to derive a Pareto-optimal set of architectures with flexible\nstructures by forcing the dynamic super-network shrink from a dense structure\nto a sparse one progressively. Furthermore, since knowledge distillation (KD)\nhas shown great effectiveness to train a compact network with the assistance of\nan over-parameterized model, we integrate SNPS with KD to formulate the DNAD\nalgorithm, i.e., search by imitating. By minimizing behavioral differences\nbetween the super-network and teacher network, the over-fitting of one-level\nDARTS is avoided and well-performed neural architectures are derived.\nExperiments on CIFAR-10 and ImageNet classification tasks demonstrate that both\nSNPS and DNAD are able to derive a set of architectures which achieve similar\nor lower error rates with fewer parameters and FLOPs. Particularly, DNAD\nachieves the top-1 error rate of 23.7% on ImageNet classification with a model\nof 6.0M parameters and 598M FLOPs, which outperforms most DARTS-based methods.", "AI": {"tldr": "DNAD\u7b97\u6cd5\u901a\u8fc7\u7ed3\u5408\u641c\u7d22\u5220\u9664\u548c\u6a21\u4eff\u641c\u7d22\uff0c\u8bbe\u8ba1\u9ad8\u6548\u795e\u7ecf\u7f51\u7edc\uff0c\u5e73\u8861\u6027\u80fd\u548c\u8ba1\u7b97\u590d\u6742\u5ea6\u3002SNPS\u548cDNAD\u5728CIFAR-10\u548cImageNet\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u8bbe\u8ba1\u9ad8\u6548\u795e\u7ecf\u7f51\u7edc\uff0c\u5e73\u8861\u6a21\u578b\u6027\u80fd\u548c\u8ba1\u7b97\u590d\u6742\u5ea6\u3002", "method": "\u5f00\u53d1DNAD\u7b97\u6cd5\uff0c\u57fa\u4e8e\u641c\u7d22\u5220\u9664\uff08SNPS\uff09\u548c\u6a21\u4eff\u641c\u7d22\uff08KD\u7ed3\u5408SNPS\uff09\u3002SNPS\u901a\u8fc7\u52a8\u6001\u6536\u7f29\u8d85\u7ea7\u7f51\u7edc\u751f\u6210\u7075\u6d3b\u7ed3\u6784\u3002", "result": "SNPS\u548cDNAD\u5728CIFAR-10\u548cImageNet\u4e0a\u5b9e\u73b0\u4f4e\u9519\u8bef\u7387\u548c\u8f83\u5c11\u53c2\u6570/FLOPs\u3002DNAD\u5728ImageNet\u4e0a\u8fbe\u523023.7% top-1\u9519\u8bef\u7387\u3002", "conclusion": "DNAD\u548cSNPS\u80fd\u751f\u6210\u9ad8\u6027\u80fd\u3001\u4f4e\u590d\u6742\u5ea6\u7684\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\uff0c\u4f18\u4e8e\u4f20\u7edfDARTS\u65b9\u6cd5\u3002"}}
{"id": "2504.20645", "pdf": "https://arxiv.org/pdf/2504.20645", "abs": "https://arxiv.org/abs/2504.20645", "authors": ["Weiqin Jiao", "Hao Cheng", "George Vosselman", "Claudio Persello"], "title": "LDPoly: Latent Diffusion for Polygonal Road Outline Extraction in Large-Scale Topographic Mapping", "categories": ["cs.CV"], "comment": null, "summary": "Polygonal road outline extraction from high-resolution aerial images is an\nimportant task in large-scale topographic mapping, where roads are represented\nas vectorized polygons, capturing essential geometric features with minimal\nvertex redundancy. Despite its importance, no existing method has been\nexplicitly designed for this task. While polygonal building outline extraction\nhas been extensively studied, the unique characteristics of roads, such as\nbranching structures and topological connectivity, pose challenges to these\nmethods. To address this gap, we introduce LDPoly, the first dedicated\nframework for extracting polygonal road outlines from high-resolution aerial\nimages. Our method leverages a novel Dual-Latent Diffusion Model with a\nChannel-Embedded Fusion Module, enabling the model to simultaneously generate\nroad masks and vertex heatmaps. A tailored polygonization method is then\napplied to obtain accurate vectorized road polygons with minimal vertex\nredundancy. We evaluate LDPoly on a new benchmark dataset, Map2ImLas, which\ncontains detailed polygonal annotations for various topographic objects in\nseveral Dutch regions. Our experiments include both in-region and cross-region\nevaluations, with the latter designed to assess the model's generalization\nperformance on unseen regions. Quantitative and qualitative results demonstrate\nthat LDPoly outperforms state-of-the-art polygon extraction methods across\nvarious metrics, including pixel-level coverage, vertex efficiency, polygon\nregularity, and road connectivity. We also design two new metrics to assess\npolygon simplicity and boundary smoothness. Moreover, this work represents the\nfirst application of diffusion models for extracting precise vectorized object\noutlines without redundant vertices from remote-sensing imagery, paving the way\nfor future advancements in this field.", "AI": {"tldr": "LDPoly\u662f\u4e00\u4e2a\u4e13\u95e8\u7528\u4e8e\u4ece\u9ad8\u5206\u8fa8\u7387\u822a\u7a7a\u56fe\u50cf\u4e2d\u63d0\u53d6\u591a\u8fb9\u5f62\u9053\u8def\u8f6e\u5ed3\u7684\u6846\u67b6\uff0c\u9996\u6b21\u89e3\u51b3\u4e86\u8fd9\u4e00\u4efb\u52a1\u3002\u5b83\u7ed3\u5408\u4e86\u53cc\u6f5c\u5728\u6269\u6563\u6a21\u578b\u548c\u901a\u9053\u5d4c\u5165\u878d\u5408\u6a21\u5757\uff0c\u5e76\u5f15\u5165\u65b0\u7684\u8bc4\u4f30\u6307\u6807\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u672a\u4e13\u95e8\u9488\u5bf9\u591a\u8fb9\u5f62\u9053\u8def\u8f6e\u5ed3\u63d0\u53d6\u4efb\u52a1\uff0c\u4e14\u9053\u8def\u7684\u5206\u652f\u7ed3\u6784\u548c\u62d3\u6251\u8fde\u63a5\u6027\u5e26\u6765\u72ec\u7279\u6311\u6218\u3002", "method": "LDPoly\u91c7\u7528\u53cc\u6f5c\u5728\u6269\u6563\u6a21\u578b\u548c\u901a\u9053\u5d4c\u5165\u878d\u5408\u6a21\u5757\uff0c\u540c\u65f6\u751f\u6210\u9053\u8def\u63a9\u7801\u548c\u9876\u70b9\u70ed\u56fe\uff0c\u5e76\u901a\u8fc7\u5b9a\u5236\u591a\u8fb9\u5f62\u5316\u65b9\u6cd5\u51cf\u5c11\u9876\u70b9\u5197\u4f59\u3002", "result": "\u5728Map2ImLas\u6570\u636e\u96c6\u4e0a\uff0cLDPoly\u5728\u50cf\u7d20\u8986\u76d6\u7387\u3001\u9876\u70b9\u6548\u7387\u3001\u591a\u8fb9\u5f62\u89c4\u5219\u6027\u548c\u9053\u8def\u8fde\u63a5\u6027\u7b49\u6307\u6807\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "LDPoly\u662f\u9996\u4e2a\u5229\u7528\u6269\u6563\u6a21\u578b\u4ece\u9065\u611f\u56fe\u50cf\u4e2d\u63d0\u53d6\u7cbe\u786e\u77e2\u91cf\u5bf9\u8c61\u8f6e\u5ed3\u7684\u65b9\u6cd5\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2504.20083", "pdf": "https://arxiv.org/pdf/2504.20083", "abs": "https://arxiv.org/abs/2504.20083", "authors": ["Thuong Dang", "Qiqi Chen"], "title": "A model and package for German ColBERT", "categories": ["cs.IR", "cs.AI"], "comment": null, "summary": "In this work, we introduce a German version for ColBERT, a late interaction\nmulti-dense vector retrieval method, with a focus on RAG applications. We also\npresent the main features of our package for ColBERT models, supporting both\nretrieval and fine-tuning workflows.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86ColBERT\u7684\u5fb7\u8bed\u7248\u672c\uff0c\u4e13\u6ce8\u4e8eRAG\u5e94\u7528\uff0c\u5e76\u5c55\u793a\u4e86\u652f\u6301\u68c0\u7d22\u548c\u5fae\u8c03\u5de5\u4f5c\u6d41\u7a0b\u7684ColBERT\u6a21\u578b\u5305\u7684\u4e3b\u8981\u529f\u80fd\u3002", "motivation": "\u4e3a\u5fb7\u8bed\u7528\u6237\u63d0\u4f9bColBERT\u7684\u672c\u5730\u5316\u7248\u672c\uff0c\u5e76\u652f\u6301RAG\u5e94\u7528\uff0c\u540c\u65f6\u5f00\u53d1\u4e00\u4e2a\u529f\u80fd\u5168\u9762\u7684\u5de5\u5177\u5305\u3002", "method": "\u5f00\u53d1\u4e86ColBERT\u7684\u5fb7\u8bed\u7248\u672c\uff0c\u5e76\u8bbe\u8ba1\u4e86\u652f\u6301\u68c0\u7d22\u548c\u5fae\u8c03\u5de5\u4f5c\u6d41\u7a0b\u7684\u6a21\u578b\u5305\u3002", "result": "\u6210\u529f\u5b9e\u73b0\u4e86\u5fb7\u8bed\u7248ColBERT\uff0c\u5e76\u63d0\u4f9b\u4e86\u529f\u80fd\u5b8c\u5584\u7684\u6a21\u578b\u5305\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u4e3a\u5fb7\u8bed\u7528\u6237\u63d0\u4f9b\u4e86\u9ad8\u6548\u7684\u68c0\u7d22\u5de5\u5177\uff0c\u5e76\u6269\u5c55\u4e86ColBERT\u7684\u5e94\u7528\u8303\u56f4\u3002"}}
{"id": "2504.20648", "pdf": "https://arxiv.org/pdf/2504.20648", "abs": "https://arxiv.org/abs/2504.20648", "authors": ["Michael Ogezi", "Freda Shi"], "title": "SpaRE: Enhancing Spatial Reasoning in Vision-Language Models with Synthetic Data", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Vision-language models (VLMs) work well in tasks ranging from image\ncaptioning to visual question answering (VQA), yet they struggle with spatial\nreasoning, a key skill for understanding our physical world that humans excel\nat. We find that spatial relations are generally rare in widely used VL\ndatasets, with only a few being well represented, while most form a long tail\nof underrepresented relations. This gap leaves VLMs ill-equipped to handle\ndiverse spatial relationships. To bridge it, we construct a synthetic VQA\ndataset focused on spatial reasoning generated from hyper-detailed image\ndescriptions in Localized Narratives, DOCCI, and PixMo-Cap. Our dataset\nconsists of 455k samples containing 3.4 million QA pairs. Trained on this\ndataset, our Spatial-Reasoning Enhanced (SpaRE) VLMs show strong improvements\non spatial reasoning benchmarks, achieving up to a 49% performance gain on the\nWhat's Up benchmark, while maintaining strong results on general tasks. Our\nwork narrows the gap between human and VLM spatial reasoning and makes VLMs\nmore capable in real-world tasks such as robotics and navigation.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u589e\u5f3a\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u7a7a\u95f4\u63a8\u7406\u80fd\u529b\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u6784\u5efa\u5408\u6210VQA\u6570\u636e\u96c6SpaRE\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u5728\u7a7a\u95f4\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "motivation": "\u73b0\u6709\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u7a7a\u95f4\u63a8\u7406\u4efb\u52a1\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u4e3b\u8981\u56e0\u4e3a\u5e38\u7528\u6570\u636e\u96c6\u4e2d\u7684\u7a7a\u95f4\u5173\u7cfb\u6570\u636e\u7a00\u7f3a\u4e14\u5206\u5e03\u4e0d\u5747\u3002", "method": "\u5229\u7528Localized Narratives\u3001DOCCI\u548cPixMo-Cap\u4e2d\u7684\u8d85\u8be6\u7ec6\u56fe\u50cf\u63cf\u8ff0\uff0c\u6784\u5efa\u4e86\u4e00\u4e2a\u5305\u542b455k\u6837\u672c\u548c3.4\u767e\u4e07QA\u5bf9\u7684\u5408\u6210VQA\u6570\u636e\u96c6\uff0c\u5e76\u8bad\u7ec3\u4e86Spatial-Reasoning Enhanced (SpaRE) VLMs\u3002", "result": "SpaRE VLMs\u5728\u7a7a\u95f4\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u663e\u8457\u63d0\u5347\uff0c\u5982\u5728What's Up\u57fa\u51c6\u4e0a\u6027\u80fd\u63d0\u9ad8\u4e8649%\uff0c\u540c\u65f6\u4fdd\u6301\u901a\u7528\u4efb\u52a1\u7684\u5f3a\u6027\u80fd\u3002", "conclusion": "\u8be5\u7814\u7a76\u7f29\u5c0f\u4e86\u4eba\u7c7b\u4e0eVLM\u5728\u7a7a\u95f4\u63a8\u7406\u80fd\u529b\u4e0a\u7684\u5dee\u8ddd\uff0c\u63d0\u5347\u4e86VLM\u5728\u673a\u5668\u4eba\u6280\u672f\u548c\u5bfc\u822a\u7b49\u5b9e\u9645\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2504.20086", "pdf": "https://arxiv.org/pdf/2504.20086", "abs": "https://arxiv.org/abs/2504.20086", "authors": ["Sebastian Gehrmann", "Claire Huang", "Xian Teng", "Sergei Yurovski", "Iyanuoluwa Shode", "Chirag S. Patel", "Arjun Bhorkar", "Naveen Thomas", "John Doucette", "David Rosenberg", "Mark Dredze", "David Rabinowitz"], "title": "Understanding and Mitigating Risks of Generative AI in Financial Services", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.LG"], "comment": "Accepted to FAccT 2025", "summary": "To responsibly develop Generative AI (GenAI) products, it is critical to\ndefine the scope of acceptable inputs and outputs. What constitutes a \"safe\"\nresponse is an actively debated question. Academic work puts an outsized focus\non evaluating models by themselves for general purpose aspects such as\ntoxicity, bias, and fairness, especially in conversational applications being\nused by a broad audience. In contrast, less focus is put on considering\nsociotechnical systems in specialized domains. Yet, those specialized systems\ncan be subject to extensive and well-understood legal and regulatory scrutiny.\nThese product-specific considerations need to be set in industry-specific laws,\nregulations, and corporate governance requirements. In this paper, we aim to\nhighlight AI content safety considerations specific to the financial services\ndomain and outline an associated AI content risk taxonomy. We compare this\ntaxonomy to existing work in this space and discuss implications of risk\ncategory violations on various stakeholders. We evaluate how existing\nopen-source technical guardrail solutions cover this taxonomy by assessing them\non data collected via red-teaming activities. Our results demonstrate that\nthese guardrails fail to detect most of the content risks we discuss.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u5728\u91d1\u878d\u670d\u52a1\u4e1a\u4e2d\u751f\u6210\u5f0fAI\uff08GenAI\uff09\u7684\u5185\u5bb9\u5b89\u5168\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u4e2a\u98ce\u9669\u5206\u7c7b\u6cd5\uff0c\u5e76\u8bc4\u4f30\u4e86\u73b0\u6709\u5f00\u6e90\u6280\u672f\u62a4\u680f\u7684\u8986\u76d6\u60c5\u51b5\u3002", "motivation": "\u5f53\u524d\u5bf9AI\u6a21\u578b\u5b89\u5168\u6027\u7684\u7814\u7a76\u4e3b\u8981\u96c6\u4e2d\u5728\u901a\u7528\u9886\u57df\u7684\u6bd2\u6027\u3001\u504f\u89c1\u548c\u516c\u5e73\u6027\uff0c\u800c\u5ffd\u89c6\u4e86\u4e13\u4e1a\u9886\u57df\u7684\u6cd5\u5f8b\u548c\u76d1\u7ba1\u8981\u6c42\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u91d1\u878d\u670d\u52a1\u4e1a\u4e2dAI\u5185\u5bb9\u5b89\u5168\u7684\u7a7a\u767d\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2aAI\u5185\u5bb9\u98ce\u9669\u5206\u7c7b\u6cd5\uff0c\u5e76\u901a\u8fc7\u7ea2\u961f\u6d3b\u52a8\u6536\u96c6\u7684\u6570\u636e\u8bc4\u4f30\u73b0\u6709\u5f00\u6e90\u6280\u672f\u62a4\u680f\u7684\u8986\u76d6\u60c5\u51b5\u3002", "result": "\u73b0\u6709\u62a4\u680f\u672a\u80fd\u68c0\u6d4b\u5230\u5927\u90e8\u5206\u8ba8\u8bba\u7684\u5185\u5bb9\u98ce\u9669\u3002", "conclusion": "\u5f3a\u8c03\u4e86\u5728\u4e13\u4e1a\u9886\u57df\u4e2d\u5f00\u53d1AI\u4ea7\u54c1\u65f6\u9700\u8003\u8651\u7279\u5b9a\u6cd5\u5f8b\u548c\u76d1\u7ba1\u8981\u6c42\uff0c\u5e76\u6307\u51fa\u73b0\u6709\u6280\u672f\u62a4\u680f\u7684\u4e0d\u8db3\u3002"}}
{"id": "2504.20657", "pdf": "https://arxiv.org/pdf/2504.20657", "abs": "https://arxiv.org/abs/2504.20657", "authors": ["Alex Michie", "Simon J Doran"], "title": "Image deidentification in the XNAT ecosystem: use cases and solutions", "categories": ["cs.CV", "J.3"], "comment": "For submission to MELBA (Machine Learning for Biomedical Imaging)\n  special issue on the MIDI-B deidentification challenge\n  (https://www.synapse.org/Synapse:syn53065760). 11 pages, 1 fig, 2 tables; 1\n  supplementary data file (supplementary_tables_S1_S2_S3.xlsx) containing three\n  spreadsheet tabs", "summary": "XNAT is a server-based data management platform widely used in academia for\ncurating large databases of DICOM images for research projects. We describe in\ndetail a deidentification workflow for DICOM data using facilities in XNAT,\ntogether with independent tools in the XNAT \"ecosystem\". We list different\ncontexts in which deidentification might be needed, based on our prior\nexperience. The starting point for participation in the Medical Image\nDe-Identification Benchmark (MIDI-B) challenge was a set of pre-existing local\nmethodologies, which were adapted during the validation phase of the challenge.\nOur result in the test phase was 97.91\\%, considerably lower than our peers,\ndue largely to an arcane technical incompatibility of our methodology with the\nchallenge's Synapse platform, which prevented us receiving feedback during the\nvalidation phase. Post-submission, additional discrepancy reports from the\norganisers and via the MIDI-B Continuous Benchmarking facility, enabled us to\nimprove this score significantly to 99.61\\%. An entirely rule-based approach\nwas shown to be capable of removing all name-related information in the test\ncorpus, but exhibited failures in dealing fully with address data. Initial\nexperiments using published machine-learning models to remove addresses were\npartially successful but showed the models to be \"over-aggressive\" on other\ntypes of free-text data, leading to a slight overall degradation in performance\nto 99.54\\%. Future development will therefore focus on improving\naddress-recognition capabilities, but also on better removal of identifiable\ndata burned into the image pixels. Several technical aspects relating to the\n\"answer key\" are still under discussion with the challenge organisers, but we\nestimate that our percentage of genuine deidentification failures on the MIDI-B\ntest corpus currently stands at 0.19\\%. (Abridged from original for arXiv\nsubmission)", "AI": {"tldr": "XNAT\u5e73\u53f0\u7528\u4e8eDICOM\u56fe\u50cf\u53bb\u6807\u8bc6\u5316\uff0c\u53c2\u4e0eMIDI-B\u6311\u6218\uff0c\u521d\u59cb\u5f97\u520697.91%\uff0c\u540e\u6539\u8fdb\u81f399.61%\u3002\u89c4\u5219\u65b9\u6cd5\u80fd\u5b8c\u5168\u53bb\u9664\u59d3\u540d\u4fe1\u606f\uff0c\u4f46\u5730\u5740\u5904\u7406\u4e0d\u8db3\u3002\u673a\u5668\u5b66\u4e60\u6a21\u578b\u90e8\u5206\u6709\u6548\u4f46\u7a0d\u964d\u6027\u80fd\u81f399.54%\u3002\u672a\u6765\u5c06\u6539\u8fdb\u5730\u5740\u8bc6\u522b\u548c\u56fe\u50cf\u50cf\u7d20\u53bb\u6807\u8bc6\u3002", "motivation": "XNAT\u5e73\u53f0\u5728\u5b66\u672f\u7814\u7a76\u4e2d\u5e7f\u6cdb\u7528\u4e8eDICOM\u56fe\u50cf\u7ba1\u7406\uff0c\u4f46\u9700\u8981\u9ad8\u6548\u7684\u53bb\u6807\u8bc6\u5316\u65b9\u6cd5\u4ee5\u6ee1\u8db3\u9690\u79c1\u4fdd\u62a4\u9700\u6c42\u3002", "method": "\u7ed3\u5408XNAT\u5de5\u5177\u548c\u72ec\u7acb\u751f\u6001\u7cfb\u7edf\u5de5\u5177\uff0c\u91c7\u7528\u89c4\u5219\u548c\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u8fdb\u884cDICOM\u6570\u636e\u53bb\u6807\u8bc6\u5316\u3002", "result": "\u521d\u59cb\u5f97\u520697.91%\uff0c\u6539\u8fdb\u540e\u8fbe99.61%\u3002\u89c4\u5219\u65b9\u6cd5\u5b8c\u5168\u53bb\u9664\u59d3\u540d\u4fe1\u606f\uff0c\u4f46\u5730\u5740\u5904\u7406\u4e0d\u8db3\uff1b\u673a\u5668\u5b66\u4e60\u6a21\u578b\u90e8\u5206\u6709\u6548\u4f46\u7a0d\u964d\u6027\u80fd\u3002", "conclusion": "\u672a\u6765\u9700\u6539\u8fdb\u5730\u5740\u8bc6\u522b\u548c\u56fe\u50cf\u50cf\u7d20\u53bb\u6807\u8bc6\uff0c\u5f53\u524d\u53bb\u6807\u8bc6\u5931\u8d25\u7387\u4e3a0.19%\u3002"}}
{"id": "2504.20092", "pdf": "https://arxiv.org/pdf/2504.20092", "abs": "https://arxiv.org/abs/2504.20092", "authors": ["Ali Rostami"], "title": "An Integrated Framework for Contextual Personalized LLM-Based Food Recommendation", "categories": ["cs.IR", "cs.AI"], "comment": "Doctorate Thesis, University of California, Irvine 2024", "summary": "Personalized food recommendation systems (Food-RecSys) critically\nunderperform due to fragmented component understanding and the failure of\nconventional machine learning with vast, imbalanced food data. While Large\nLanguage Models (LLMs) offer promise, current generic Recommendation as\nLanguage Processing (RLP) strategies lack the necessary specialization for the\nfood domain's complexity. This thesis tackles these deficiencies by first\nidentifying and analyzing the essential components for effective Food-RecSys.\nWe introduce two key innovations: a multimedia food logging platform for rich\ncontextual data acquisition and the World Food Atlas, enabling unique\ngeolocation-based food analysis previously unavailable. Building on this\nfoundation, we pioneer the Food Recommendation as Language Processing (F-RLP)\nframework - a novel, integrated approach specifically architected for the food\ndomain. F-RLP leverages LLMs in a tailored manner, overcoming the limitations\nof generic models and providing a robust infrastructure for effective,\ncontextual, and truly personalized food recommendations.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u98df\u54c1\u63a8\u8350\u7cfb\u7edf\u7684\u4e2a\u6027\u5316\u6846\u67b6F-RLP\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u5728\u98df\u54c1\u9886\u57df\u7684\u4e0d\u8db3\uff0c\u5e76\u901a\u8fc7\u591a\u5a92\u4f53\u65e5\u5fd7\u5e73\u53f0\u548c\u5730\u7406\u5206\u6790\u5de5\u5177\u63d0\u5347\u4e86\u6570\u636e\u8d28\u91cf\u3002", "motivation": "\u4f20\u7edf\u98df\u54c1\u63a8\u8350\u7cfb\u7edf\u56e0\u7ec4\u4ef6\u7406\u89e3\u4e0d\u8db3\u548c\u6570\u636e\u4e0d\u5e73\u8861\u8868\u73b0\u4e0d\u4f73\uff0c\u800c\u73b0\u6709\u901a\u7528\u8bed\u8a00\u6a21\u578b\u7f3a\u4e4f\u9488\u5bf9\u6027\u3002", "method": "\u63d0\u51faF-RLP\u6846\u67b6\uff0c\u7ed3\u5408\u591a\u5a92\u4f53\u98df\u54c1\u65e5\u5fd7\u5e73\u53f0\u548c\u5730\u7406\u5206\u6790\u5de5\u5177\uff0c\u4e13\u95e8\u4f18\u5316\u98df\u54c1\u63a8\u8350\u3002", "result": "F-RLP\u6846\u67b6\u514b\u670d\u4e86\u901a\u7528\u6a21\u578b\u7684\u9650\u5236\uff0c\u5b9e\u73b0\u4e86\u66f4\u6709\u6548\u7684\u4e2a\u6027\u5316\u98df\u54c1\u63a8\u8350\u3002", "conclusion": "F-RLP\u4e3a\u98df\u54c1\u63a8\u8350\u9886\u57df\u63d0\u4f9b\u4e86\u521b\u65b0\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u663e\u8457\u63d0\u5347\u4e86\u63a8\u8350\u6548\u679c\u3002"}}
{"id": "2504.20669", "pdf": "https://arxiv.org/pdf/2504.20669", "abs": "https://arxiv.org/abs/2504.20669", "authors": ["Joy Battocchio", "Stefano Dell'Anna", "Andrea Montibeller", "Giulia Boato"], "title": "Advance Fake Video Detection via Vision Transformers", "categories": ["cs.CV", "cs.AI", "cs.MM"], "comment": null, "summary": "Recent advancements in AI-based multimedia generation have enabled the\ncreation of hyper-realistic images and videos, raising concerns about their\npotential use in spreading misinformation. The widespread accessibility of\ngenerative techniques, which allow for the production of fake multimedia from\nprompts or existing media, along with their continuous refinement, underscores\nthe urgent need for highly accurate and generalizable AI-generated media\ndetection methods, underlined also by new regulations like the European Digital\nAI Act. In this paper, we draw inspiration from Vision Transformer (ViT)-based\nfake image detection and extend this idea to video. We propose an {original}\n%innovative framework that effectively integrates ViT embeddings over time to\nenhance detection performance. Our method shows promising accuracy,\ngeneralization, and few-shot learning capabilities across a new, large and\ndiverse dataset of videos generated using five open source generative\ntechniques from the state-of-the-art, as well as a separate dataset containing\nvideos produced by proprietary generative methods.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eVision Transformer\uff08ViT\uff09\u7684\u521b\u65b0\u6846\u67b6\uff0c\u7528\u4e8e\u68c0\u6d4bAI\u751f\u6210\u7684\u89c6\u9891\uff0c\u89e3\u51b3\u4e86\u865a\u5047\u591a\u5a92\u4f53\u4f20\u64ad\u7684\u7d27\u8feb\u95ee\u9898\u3002", "motivation": "\u968f\u7740AI\u751f\u6210\u591a\u5a92\u4f53\u6280\u672f\u7684\u5feb\u901f\u53d1\u5c55\uff0c\u865a\u5047\u5185\u5bb9\u7684\u4f20\u64ad\u98ce\u9669\u589e\u52a0\uff0c\u4e9f\u9700\u9ad8\u7cbe\u5ea6\u3001\u6cdb\u5316\u6027\u5f3a\u7684\u68c0\u6d4b\u65b9\u6cd5\u3002", "method": "\u6269\u5c55ViT\u7528\u4e8e\u89c6\u9891\u68c0\u6d4b\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u6574\u5408\u65f6\u95f4\u7ef4\u5ea6\u7684\u6846\u67b6\uff0c\u63d0\u5347\u4e86\u68c0\u6d4b\u6027\u80fd\u3002", "result": "\u65b9\u6cd5\u5728\u65b0\u7684\u5927\u89c4\u6a21\u591a\u6837\u5316\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u9ad8\u51c6\u786e\u6027\u3001\u6cdb\u5316\u80fd\u529b\u548c\u5c11\u6837\u672c\u5b66\u4e60\u80fd\u529b\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3aAI\u751f\u6210\u89c6\u9891\u7684\u68c0\u6d4b\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u9002\u5e94\u5f00\u6e90\u548c\u4e13\u6709\u751f\u6210\u6280\u672f\u3002"}}
{"id": "2504.20093", "pdf": "https://arxiv.org/pdf/2504.20093", "abs": "https://arxiv.org/abs/2504.20093", "authors": ["Mohammad Baqar", "Rajat Khanda", "Saba Naqvi"], "title": "Self-Healing Software Systems: Lessons from Nature, Powered by AI", "categories": ["cs.SE", "cs.AI"], "comment": null, "summary": "As modern software systems grow in complexity and scale, their ability to\nautonomously detect, diagnose, and recover from failures becomes increasingly\nvital. Drawing inspiration from biological healing - where the human body\ndetects damage, signals the brain, and activates targeted recovery - this paper\nexplores the concept of self-healing software driven by artificial\nintelligence. We propose a novel framework that mimics this biological model\nsystem observability tools serve as sensory inputs, AI models function as the\ncognitive core for diagnosis and repair, and healing agents apply targeted code\nand test modifications. By combining log analysis, static code inspection, and\nAI-driven generation of patches or test updates, our approach aims to reduce\ndowntime, accelerate debugging, and enhance software resilience. We evaluate\nthe effectiveness of this model through case studies and simulations, comparing\nit against traditional manual debugging and recovery workflows. This work paves\nthe way toward intelligent, adaptive and self-reliant software systems capable\nof continuous healing, akin to living organisms.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u53d7\u751f\u7269\u542f\u53d1\u7684\u4eba\u5de5\u667a\u80fd\u9a71\u52a8\u7684\u81ea\u6108\u8f6f\u4ef6\u6846\u67b6\uff0c\u901a\u8fc7\u7cfb\u7edf\u53ef\u89c2\u6d4b\u6027\u5de5\u5177\u3001AI\u8bca\u65ad\u548c\u4fee\u590d\u4ee3\u7406\uff0c\u51cf\u5c11\u505c\u673a\u65f6\u95f4\u5e76\u589e\u5f3a\u8f6f\u4ef6\u5f39\u6027\u3002", "motivation": "\u968f\u7740\u8f6f\u4ef6\u7cfb\u7edf\u590d\u6742\u6027\u548c\u89c4\u6a21\u7684\u589e\u52a0\uff0c\u81ea\u4e3b\u68c0\u6d4b\u3001\u8bca\u65ad\u548c\u6062\u590d\u6545\u969c\u7684\u80fd\u529b\u53d8\u5f97\u81f3\u5173\u91cd\u8981\u3002", "method": "\u7ed3\u5408\u65e5\u5fd7\u5206\u6790\u3001\u9759\u6001\u4ee3\u7801\u68c0\u67e5\u548cAI\u9a71\u52a8\u7684\u8865\u4e01\u751f\u6210\uff0c\u6a21\u62df\u751f\u7269\u81ea\u6108\u673a\u5236\u3002", "result": "\u901a\u8fc7\u6848\u4f8b\u7814\u7a76\u548c\u6a21\u62df\u9a8c\u8bc1\uff0c\u76f8\u6bd4\u4f20\u7edf\u624b\u52a8\u8c03\u8bd5\uff0c\u8be5\u6846\u67b6\u80fd\u6709\u6548\u51cf\u5c11\u505c\u673a\u65f6\u95f4\u5e76\u52a0\u901f\u8c03\u8bd5\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u667a\u80fd\u3001\u81ea\u9002\u5e94\u548c\u81ea\u4f9d\u8d56\u7684\u8f6f\u4ef6\u7cfb\u7edf\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u7c7b\u4f3c\u4e8e\u751f\u7269\u4f53\u7684\u6301\u7eed\u81ea\u6108\u80fd\u529b\u3002"}}
{"id": "2504.20670", "pdf": "https://arxiv.org/pdf/2504.20670", "abs": "https://arxiv.org/abs/2504.20670", "authors": ["Yao Xiao", "Tingfa Xu", "Yu Xin", "Jianan Li"], "title": "FBRT-YOLO: Faster and Better for Real-Time Aerial Image Detection", "categories": ["cs.CV"], "comment": "AAAI 2025", "summary": "Embedded flight devices with visual capabilities have become essential for a\nwide range of applications. In aerial image detection, while many existing\nmethods have partially addressed the issue of small target detection,\nchallenges remain in optimizing small target detection and balancing detection\naccuracy with efficiency. These issues are key obstacles to the advancement of\nreal-time aerial image detection. In this paper, we propose a new family of\nreal-time detectors for aerial image detection, named FBRT-YOLO, to address the\nimbalance between detection accuracy and efficiency. Our method comprises two\nlightweight modules: Feature Complementary Mapping Module (FCM) and\nMulti-Kernel Perception Unit(MKP), designed to enhance object perception for\nsmall targets in aerial images. FCM focuses on alleviating the problem of\ninformation imbalance caused by the loss of small target information in deep\nnetworks. It aims to integrate spatial positional information of targets more\ndeeply into the network,better aligning with semantic information in the deeper\nlayers to improve the localization of small targets. We introduce MKP, which\nleverages convolutions with kernels of different sizes to enhance the\nrelationships between targets of various scales and improve the perception of\ntargets at different scales. Extensive experimental results on three major\naerial image datasets, including Visdrone, UAVDT, and AI-TOD,demonstrate that\nFBRT-YOLO outperforms various real-time detectors in terms of performance and\nspeed.", "AI": {"tldr": "FBRT-YOLO\u662f\u4e00\u79cd\u65b0\u578b\u5b9e\u65f6\u68c0\u6d4b\u5668\uff0c\u7528\u4e8e\u89e3\u51b3\u822a\u7a7a\u56fe\u50cf\u4e2d\u5c0f\u76ee\u6807\u68c0\u6d4b\u7684\u7cbe\u5ea6\u4e0e\u6548\u7387\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u901a\u8fc7\u4e24\u4e2a\u8f7b\u91cf\u7ea7\u6a21\u5757\uff08FCM\u548cMKP\uff09\u63d0\u5347\u5c0f\u76ee\u6807\u611f\u77e5\u80fd\u529b\u3002", "motivation": "\u822a\u7a7a\u56fe\u50cf\u4e2d\u5c0f\u76ee\u6807\u68c0\u6d4b\u7684\u7cbe\u5ea6\u4e0e\u6548\u7387\u4e0d\u5e73\u8861\u662f\u5b9e\u65f6\u68c0\u6d4b\u53d1\u5c55\u7684\u4e3b\u8981\u969c\u788d\u3002", "method": "\u63d0\u51faFCM\u6a21\u5757\u7f13\u89e3\u5c0f\u76ee\u6807\u4fe1\u606f\u4e22\u5931\u95ee\u9898\uff0c\u5f15\u5165MKP\u6a21\u5757\u901a\u8fc7\u591a\u5c3a\u5ea6\u5377\u79ef\u589e\u5f3a\u76ee\u6807\u611f\u77e5\u3002", "result": "\u5728Visdrone\u3001UAVDT\u548cAI-TOD\u6570\u636e\u96c6\u4e0a\uff0cFBRT-YOLO\u5728\u6027\u80fd\u548c\u901f\u5ea6\u4e0a\u4f18\u4e8e\u5176\u4ed6\u5b9e\u65f6\u68c0\u6d4b\u5668\u3002", "conclusion": "FBRT-YOLO\u6709\u6548\u5e73\u8861\u4e86\u68c0\u6d4b\u7cbe\u5ea6\u4e0e\u6548\u7387\uff0c\u63d0\u5347\u4e86\u822a\u7a7a\u56fe\u50cf\u4e2d\u5c0f\u76ee\u6807\u7684\u68c0\u6d4b\u80fd\u529b\u3002"}}
{"id": "2504.20099", "pdf": "https://arxiv.org/pdf/2504.20099", "abs": "https://arxiv.org/abs/2504.20099", "authors": ["Inmaculada Santamaria-Valenzuela", "Victor Rodriguez-Fernandez", "Javier Huertas-Tato", "Jong Hyuk Park", "David Camacho"], "title": "Decoding Latent Spaces: Assessing the Interpretability of Time Series Foundation Models for Visual Analytics", "categories": ["cs.LG", "cs.AI", "stat.ML"], "comment": "Currently under review at the International Journal of Interactive\n  Multimedia and Artificial Intelligence (IJIMAI)", "summary": "The present study explores the interpretability of latent spaces produced by\ntime series foundation models, focusing on their potential for visual analysis\ntasks. Specifically, we evaluate the MOMENT family of models, a set of\ntransformer-based, pre-trained architectures for multivariate time series tasks\nsuch as: imputation, prediction, classification, and anomaly detection. We\nevaluate the capacity of these models on five datasets to capture the\nunderlying structures in time series data within their latent space projection\nand validate whether fine tuning improves the clarity of the resulting\nembedding spaces. Notable performance improvements in terms of loss reduction\nwere observed after fine tuning. Visual analysis shows limited improvement in\nthe interpretability of the embeddings, requiring further work. Results suggest\nthat, although Time Series Foundation Models such as MOMENT are robust, their\nlatent spaces may require additional methodological refinements to be\nadequately interpreted, such as alternative projection techniques, loss\nfunctions, or data preprocessing strategies. Despite the limitations of MOMENT,\nfoundation models supose a big reduction in execution time and so a great\nadvance for interactive visual analytics.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u65f6\u95f4\u5e8f\u5217\u57fa\u7840\u6a21\u578b\uff08\u5982MOMENT\uff09\u6f5c\u5728\u7a7a\u95f4\u7684\u53ef\u89e3\u91ca\u6027\uff0c\u53d1\u73b0\u5176\u867d\u5728\u6027\u80fd\u4e0a\u6709\u63d0\u5347\uff0c\u4f46\u6f5c\u5728\u7a7a\u95f4\u7684\u89e3\u91ca\u6027\u4ecd\u9700\u6539\u8fdb\u3002", "motivation": "\u63a2\u7d22\u65f6\u95f4\u5e8f\u5217\u57fa\u7840\u6a21\u578b\u6f5c\u5728\u7a7a\u95f4\u7684\u53ef\u89e3\u91ca\u6027\uff0c\u4ee5\u652f\u6301\u53ef\u89c6\u5316\u5206\u6790\u4efb\u52a1\u3002", "method": "\u8bc4\u4f30MOMENT\u6a21\u578b\u5728\u4e94\u79cd\u6570\u636e\u96c6\u4e0a\u7684\u8868\u73b0\uff0c\u5206\u6790\u5176\u6f5c\u5728\u7a7a\u95f4\u6355\u6349\u65f6\u95f4\u5e8f\u5217\u7ed3\u6784\u7684\u80fd\u529b\uff0c\u5e76\u9a8c\u8bc1\u5fae\u8c03\u5bf9\u5d4c\u5165\u7a7a\u95f4\u6e05\u6670\u5ea6\u7684\u5f71\u54cd\u3002", "result": "\u5fae\u8c03\u540e\u6027\u80fd\u63d0\u5347\uff08\u635f\u5931\u51cf\u5c11\uff09\uff0c\u4f46\u6f5c\u5728\u7a7a\u95f4\u7684\u53ef\u89e3\u91ca\u6027\u6539\u8fdb\u6709\u9650\u3002", "conclusion": "\u65f6\u95f4\u5e8f\u5217\u57fa\u7840\u6a21\u578b\u867d\u9ad8\u6548\uff0c\u4f46\u6f5c\u5728\u7a7a\u95f4\u89e3\u91ca\u6027\u9700\u8fdb\u4e00\u6b65\u4f18\u5316\uff08\u5982\u6539\u8fdb\u6295\u5f71\u6280\u672f\u6216\u635f\u5931\u51fd\u6570\uff09\u3002"}}
{"id": "2504.20677", "pdf": "https://arxiv.org/pdf/2504.20677", "abs": "https://arxiv.org/abs/2504.20677", "authors": ["Paola Natalia Ca\u00f1as", "Alexander Diez", "David Galva\u00f1", "Marcos Nieto", "Igor Rodr\u00edguez"], "title": "Occlusion-aware Driver Monitoring System using the Driver Monitoring Dataset", "categories": ["cs.CV"], "comment": "Submitted for review to the IEEE International Conference on\n  Intelligent Transportation Systems (ITSC) 2025", "summary": "This paper presents a robust, occlusion-aware driver monitoring system (DMS)\nutilizing the Driver Monitoring Dataset (DMD). The system performs driver\nidentification, gaze estimation by regions, and face occlusion detection under\nvarying lighting conditions, including challenging low-light scenarios. Aligned\nwith EuroNCAP recommendations, the inclusion of occlusion detection enhances\nsituational awareness and system trustworthiness by indicating when the\nsystem's performance may be degraded. The system employs separate algorithms\ntrained on RGB and infrared (IR) images to ensure reliable functioning. We\ndetail the development and integration of these algorithms into a cohesive\npipeline, addressing the challenges of working with different sensors and\nreal-car implementation. Evaluation on the DMD and in real-world scenarios\ndemonstrates the effectiveness of the proposed system, highlighting the\nsuperior performance of RGB-based models and the pioneering contribution of\nrobust occlusion detection in DMS.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eRGB\u548c\u7ea2\u5916\u56fe\u50cf\u7684\u9c81\u68d2\u9a7e\u9a76\u5458\u76d1\u63a7\u7cfb\u7edf\uff0c\u652f\u6301\u9a7e\u9a76\u5458\u8bc6\u522b\u3001\u89c6\u7ebf\u533a\u57df\u4f30\u8ba1\u548c\u906e\u6321\u68c0\u6d4b\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u5149\u7167\u6761\u4ef6\u3002", "motivation": "\u63d0\u5347\u9a7e\u9a76\u5458\u76d1\u63a7\u7cfb\u7edf\u5728\u906e\u6321\u548c\u4f4e\u5149\u6761\u4ef6\u4e0b\u7684\u6027\u80fd\uff0c\u7b26\u5408EuroNCAP\u6807\u51c6\uff0c\u589e\u5f3a\u7cfb\u7edf\u53ef\u4fe1\u5ea6\u3002", "method": "\u4f7f\u7528RGB\u548c\u7ea2\u5916\u56fe\u50cf\u5206\u522b\u8bad\u7ec3\u7b97\u6cd5\uff0c\u6574\u5408\u4e3a\u7edf\u4e00\u6d41\u7a0b\uff0c\u89e3\u51b3\u591a\u4f20\u611f\u5668\u548c\u5b9e\u9645\u8f66\u8f86\u90e8\u7f72\u7684\u6311\u6218\u3002", "result": "\u5728DMD\u6570\u636e\u96c6\u548c\u5b9e\u9645\u573a\u666f\u4e2d\u9a8c\u8bc1\u4e86\u7cfb\u7edf\u6709\u6548\u6027\uff0cRGB\u6a21\u578b\u8868\u73b0\u66f4\u4f18\uff0c\u906e\u6321\u68c0\u6d4b\u529f\u80fd\u4e3a\u521b\u65b0\u70b9\u3002", "conclusion": "\u7cfb\u7edf\u5728\u590d\u6742\u6761\u4ef6\u4e0b\u8868\u73b0\u7a33\u5065\uff0c\u906e\u6321\u68c0\u6d4b\u529f\u80fd\u663e\u8457\u63d0\u5347\u4e86\u9a7e\u9a76\u5458\u76d1\u63a7\u7684\u53ef\u9760\u6027\u3002"}}
{"id": "2504.20101", "pdf": "https://arxiv.org/pdf/2504.20101", "abs": "https://arxiv.org/abs/2504.20101", "authors": ["Fei Fang", "Yifan Hua", "Shengze Wang", "Ruilin Zhou", "Yi Liu", "Chen Qian", "Xiaoxue Zhang"], "title": "GenTorrent: Scaling Large Language Model Serving with An Overley Network", "categories": ["cs.DC", "cs.AI"], "comment": null, "summary": "While significant progress has been made in research and development on\nopen-source and cost-efficient large-language models (LLMs), serving\nscalability remains a critical challenge, particularly for small organizations\nand individuals seeking to deploy and test their LLM innovations. Inspired by\npeer-to-peer networks that leverage decentralized overlay nodes to increase\nthroughput and availability, we propose GenTorrent, an LLM serving overlay that\nharnesses computing resources from decentralized contributors. We identify four\nkey research problems inherent to enabling such a decentralized infrastructure:\n1) overlay network organization; 2) LLM communication privacy; 3) overlay\nforwarding for resource efficiency; and 4) verification of serving quality.\nThis work presents the first systematic study of these fundamental problems in\nthe context of decentralized LLM serving. Evaluation results from a prototype\nimplemented on a set of decentralized nodes demonstrate that GenTorrent\nachieves a latency reduction of over 50% compared to the baseline design\nwithout overlay forwarding. Furthermore, the security features introduce\nminimal overhead to serving latency and throughput. We believe this work\npioneers a new direction for democratizing and scaling future AI serving\ncapabilities.", "AI": {"tldr": "GenTorrent\u63d0\u51fa\u4e86\u4e00\u79cd\u53bb\u4e2d\u5fc3\u5316\u7684LLM\u670d\u52a1\u8986\u76d6\u7f51\u7edc\uff0c\u901a\u8fc7\u5229\u7528\u5206\u6563\u7684\u8ba1\u7b97\u8d44\u6e90\u89e3\u51b3\u4e86\u5c0f\u578b\u7ec4\u7ec7\u548c\u4e2a\u4eba\u5728LLM\u90e8\u7f72\u4e2d\u7684\u6269\u5c55\u6027\u95ee\u9898\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u5ef6\u8fdf\u5e76\u4fdd\u6301\u4e86\u5b89\u5168\u6027\u3002", "motivation": "\u89e3\u51b3\u5c0f\u578b\u7ec4\u7ec7\u548c\u4e2a\u4f53\u5728\u90e8\u7f72\u548c\u6d4b\u8bd5LLM\u521b\u65b0\u65f6\u9762\u4e34\u7684\u670d\u52a1\u6269\u5c55\u6027\u6311\u6218\u3002", "method": "\u63d0\u51faGenTorrent\uff0c\u4e00\u79cd\u57fa\u4e8e\u70b9\u5bf9\u70b9\u7f51\u7edc\u7684\u53bb\u4e2d\u5fc3\u5316LLM\u670d\u52a1\u8986\u76d6\u7f51\u7edc\uff0c\u89e3\u51b3\u4e86\u7f51\u7edc\u7ec4\u7ec7\u3001\u9690\u79c1\u3001\u8d44\u6e90\u6548\u7387\u548c\u9a8c\u8bc1\u7b49\u56db\u4e2a\u5173\u952e\u95ee\u9898\u3002", "result": "\u539f\u578b\u6d4b\u8bd5\u663e\u793a\uff0cGenTorrent\u6bd4\u57fa\u7ebf\u8bbe\u8ba1\u964d\u4f4e\u4e8650%\u4ee5\u4e0a\u7684\u5ef6\u8fdf\uff0c\u5b89\u5168\u6027\u529f\u80fd\u5bf9\u6027\u80fd\u5f71\u54cd\u6781\u5c0f\u3002", "conclusion": "GenTorrent\u4e3a\u672a\u6765AI\u670d\u52a1\u7684\u6c11\u4e3b\u5316\u548c\u6269\u5c55\u6027\u5f00\u8f9f\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2504.20682", "pdf": "https://arxiv.org/pdf/2504.20682", "abs": "https://arxiv.org/abs/2504.20682", "authors": ["Long Liu", "Cihui Yang"], "title": "OG-HFYOLO :Orientation gradient guidance and heterogeneous feature fusion for deformation table cell instance segmentation", "categories": ["cs.CV"], "comment": null, "summary": "Table structure recognition is a key task in document analysis. However, the\ngeometric deformation in deformed tables causes a weak correlation between\ncontent information and structure, resulting in downstream tasks not being able\nto obtain accurate content information. To obtain fine-grained spatial\ncoordinates of cells, we propose the OG-HFYOLO model, which enhances the edge\nresponse by Gradient Orientation-aware Extractor, combines a Heterogeneous\nKernel Cross Fusion module and a scale-aware loss function to adapt to\nmulti-scale objective features, and introduces mask-driven non-maximal\nsuppression in the post-processing, which replaces the traditional bounding box\nsuppression mechanism. Furthermore, we also propose a data generator, filling\nthe gap in the dataset for fine-grained deformation table cell spatial\ncoordinate localization, and derive a large-scale dataset named Deformation\nWired Table (DWTAL). Experiments show that our proposed model demonstrates\nexcellent segmentation accuracy on all mainstream instance segmentation models.\nThe dataset and the source code are open source:\nhttps://github.com/justliulong/OGHFYOLO.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faOG-HFYOLO\u6a21\u578b\uff0c\u901a\u8fc7\u68af\u5ea6\u65b9\u5411\u611f\u77e5\u63d0\u53d6\u5668\u548c\u5f02\u6784\u6838\u4ea4\u53c9\u878d\u5408\u6a21\u5757\u89e3\u51b3\u53d8\u5f62\u8868\u683c\u7ed3\u6784\u8bc6\u522b\u95ee\u9898\uff0c\u5e76\u751f\u6210\u6570\u636e\u96c6DWTAL\u3002", "motivation": "\u53d8\u5f62\u8868\u683c\u7684\u51e0\u4f55\u53d8\u5f62\u5bfc\u81f4\u5185\u5bb9\u4e0e\u7ed3\u6784\u5173\u8054\u6027\u5f31\uff0c\u5f71\u54cd\u4e0b\u6e38\u4efb\u52a1\u51c6\u786e\u6027\u3002", "method": "\u7ed3\u5408\u68af\u5ea6\u65b9\u5411\u611f\u77e5\u63d0\u53d6\u5668\u3001\u5f02\u6784\u6838\u4ea4\u53c9\u878d\u5408\u6a21\u5757\u3001\u5c3a\u5ea6\u611f\u77e5\u635f\u5931\u51fd\u6570\u548c\u63a9\u7801\u9a71\u52a8\u7684\u975e\u6781\u5927\u6291\u5236\u3002", "result": "\u6a21\u578b\u5728\u4e3b\u6d41\u5b9e\u4f8b\u5206\u5272\u6a21\u578b\u4e2d\u8868\u73b0\u51fa\u4f18\u5f02\u7684\u5206\u5272\u7cbe\u5ea6\u3002", "conclusion": "\u63d0\u51fa\u7684\u6a21\u578b\u548c\u6570\u636e\u96c6\u586b\u8865\u4e86\u53d8\u5f62\u8868\u683c\u7ec6\u7c92\u5ea6\u7a7a\u95f4\u5750\u6807\u5b9a\u4f4d\u7684\u7a7a\u767d\u3002"}}
{"id": "2504.20102", "pdf": "https://arxiv.org/pdf/2504.20102", "abs": "https://arxiv.org/abs/2504.20102", "authors": ["Qingzhi Yu", "Shuai Yan", "Wenfeng Dai", "Xiang Cheng"], "title": "HyboWaveNet: Hyperbolic Graph Neural Networks with Multi-Scale Wavelet Transform for Protein-Protein Interaction Prediction", "categories": ["cs.LG", "cs.AI", "q-bio.QM"], "comment": "9 pages", "summary": "Protein-protein interactions (PPIs) are fundamental for deciphering cellular\nfunctions,disease pathways,and drug discovery.Although existing neural networks\nand machine learning methods have achieved high accuracy in PPI\nprediction,their black-box nature leads to a lack of causal interpretation of\nthe prediction results and difficulty in capturing hierarchical geometries and\nmulti-scale dynamic interaction patterns among proteins.To address these\nchallenges, we propose HyboWaveNet,a novel deep learning framework that\ncollaborates with hyperbolic graphical neural networks (HGNNs) and multiscale\ngraphical wavelet transform for robust PPI prediction. Mapping protein features\nto Lorentz space simulates hierarchical topological relationships among\nbiomolecules via a hyperbolic distance metric,enabling node feature\nrepresentations that better fit biological a priori.HyboWaveNet inherently\nsimulates hierarchical and scale-free biological relationships, while the\nintegration of wavelet transforms enables adaptive extraction of local and\nglobal interaction features across different resolutions. Our framework\ngenerates node feature representations via a graph neural network under the\nLorenz model and generates pairs of positive samples under multiple different\nviews for comparative learning, followed by further feature extraction via\nmulti-scale graph wavelet transforms to predict potential PPIs. Experiments on\npublic datasets show that HyboWaveNet improves over both existing\nstate-of-the-art methods. We also demonstrate through ablation experimental\nstudies that the multi-scale graph wavelet transform module improves the\npredictive performance and generalization ability of HyboWaveNet. This work\nlinks geometric deep learning and signal processing to advance PPI prediction,\nproviding a principled approach for analyzing complex biological systems", "AI": {"tldr": "HyboWaveNet\u7ed3\u5408\u53cc\u66f2\u56fe\u795e\u7ecf\u7f51\u7edc\u548c\u591a\u5c3a\u5ea6\u56fe\u5c0f\u6ce2\u53d8\u6362\uff0c\u63d0\u5347\u86cb\u767d\u8d28\u76f8\u4e92\u4f5c\u7528\u9884\u6d4b\u7684\u51c6\u786e\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u73b0\u6709\u795e\u7ecf\u7f51\u7edc\u65b9\u6cd5\u5728\u86cb\u767d\u8d28\u76f8\u4e92\u4f5c\u7528\u9884\u6d4b\u4e2d\u7f3a\u4e4f\u56e0\u679c\u89e3\u91ca\uff0c\u96be\u4ee5\u6355\u6349\u591a\u5c3a\u5ea6\u52a8\u6001\u4ea4\u4e92\u6a21\u5f0f\u3002", "method": "\u5229\u7528\u53cc\u66f2\u7a7a\u95f4\u6620\u5c04\u86cb\u767d\u8d28\u7279\u5f81\uff0c\u7ed3\u5408\u591a\u5c3a\u5ea6\u56fe\u5c0f\u6ce2\u53d8\u6362\u63d0\u53d6\u5c40\u90e8\u548c\u5168\u5c40\u7279\u5f81\uff0c\u901a\u8fc7\u5bf9\u6bd4\u5b66\u4e60\u9884\u6d4b\u76f8\u4e92\u4f5c\u7528\u3002", "result": "\u5b9e\u9a8c\u663e\u793aHyboWaveNet\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u591a\u5c3a\u5ea6\u56fe\u5c0f\u6ce2\u53d8\u6362\u6a21\u5757\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002", "conclusion": "HyboWaveNet\u5c06\u51e0\u4f55\u6df1\u5ea6\u5b66\u4e60\u4e0e\u4fe1\u53f7\u5904\u7406\u7ed3\u5408\uff0c\u4e3a\u590d\u6742\u751f\u7269\u7cfb\u7edf\u5206\u6790\u63d0\u4f9b\u65b0\u601d\u8def\u3002"}}
{"id": "2504.20685", "pdf": "https://arxiv.org/pdf/2504.20685", "abs": "https://arxiv.org/abs/2504.20685", "authors": ["Zesheng Wang", "Alexandre Bruckert", "Patrick Le Callet", "Guangtao Zhai"], "title": "Efficient Listener: Dyadic Facial Motion Synthesis via Action Diffusion", "categories": ["cs.CV", "cs.HC"], "comment": null, "summary": "Generating realistic listener facial motions in dyadic conversations remains\nchallenging due to the high-dimensional action space and temporal dependency\nrequirements. Existing approaches usually consider extracting 3D Morphable\nModel (3DMM) coefficients and modeling in the 3DMM space. However, this makes\nthe computational speed of the 3DMM a bottleneck, making it difficult to\nachieve real-time interactive responses. To tackle this problem, we propose\nFacial Action Diffusion (FAD), which introduces the diffusion methods from the\nfield of image generation to achieve efficient facial action generation. We\nfurther build the Efficient Listener Network (ELNet) specially designed to\naccommodate both the visual and audio information of the speaker as input.\nConsidering of FAD and ELNet, the proposed method learns effective listener\nfacial motion representations and leads to improvements of performance over the\nstate-of-the-art methods while reducing 99% computational time.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u65b9\u6cd5\u7684\u9ad8\u6548\u9762\u90e8\u52a8\u4f5c\u751f\u6210\u6280\u672f\uff08FAD\uff09\u548c\u9ad8\u6548\u542c\u4f17\u7f51\u7edc\uff08ELNet\uff09\uff0c\u663e\u8457\u63d0\u5347\u4e86\u542c\u4f17\u9762\u90e8\u52a8\u4f5c\u751f\u6210\u7684\u6027\u80fd\uff0c\u5e76\u51cf\u5c11\u4e8699%\u7684\u8ba1\u7b97\u65f6\u95f4\u3002", "motivation": "\u5728\u5bf9\u8bdd\u4e2d\u751f\u6210\u903c\u771f\u7684\u542c\u4f17\u9762\u90e8\u52a8\u4f5c\u5177\u6709\u6311\u6218\u6027\uff0c\u73b0\u6709\u65b9\u6cd5\u56e03DMM\u8ba1\u7b97\u901f\u5ea6\u9650\u5236\u96be\u4ee5\u5b9e\u73b0\u5b9e\u65f6\u4ea4\u4e92\u3002", "method": "\u7ed3\u5408FAD\uff08\u6269\u6563\u65b9\u6cd5\uff09\u548cELNet\uff08\u89c6\u89c9\u4e0e\u97f3\u9891\u8f93\u5165\u7684\u7f51\u7edc\uff09\uff0c\u5b66\u4e60\u6709\u6548\u7684\u9762\u90e8\u52a8\u4f5c\u8868\u793a\u3002", "result": "\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u8ba1\u7b97\u65f6\u95f4\u51cf\u5c1199%\u3002", "conclusion": "FAD\u548cELNet\u7684\u7ec4\u5408\u4e3a\u5b9e\u65f6\u9762\u90e8\u52a8\u4f5c\u751f\u6210\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2504.20103", "pdf": "https://arxiv.org/pdf/2504.20103", "abs": "https://arxiv.org/abs/2504.20103", "authors": ["Wenfeng Dai", "Yanhong Wang", "Shuai Yan", "Qingzhi Yu", "Xiang Cheng"], "title": "Heterogeneous network drug-target interaction prediction model based on graph wavelet transform and multi-level contrastive learning", "categories": ["q-bio.QM", "cs.AI", "cs.LG"], "comment": null, "summary": "Drug-target interaction (DTI) prediction is a core task in drug development\nand precision medicine in the biomedical field. However, traditional machine\nlearning methods generally have the black box problem, which makes it difficult\nto reveal the deep correlation between the model decision mechanism and the\ninteraction pattern between biological molecules. This study proposes a\nheterogeneous network drug target interaction prediction framework, integrating\ngraph neural network and multi scale signal processing technology to construct\na model with both efficient prediction and multi level interpretability. Its\ntechnical breakthroughs are mainly reflected in the following three\ndimensions:Local global feature collaborative perception module. Based on\nheterogeneous graph convolutional neural network (HGCN), a multi order neighbor\naggregation strategy is designed.Multi scale graph signal decomposition and\nbiological interpretation module. A deep hierarchical node feature transform\n(GWT) architecture is proposed.Contrastive learning combining multi dimensional\nperspectives and hierarchical representations. By comparing the learning\nmodels, the node representations from the two perspectives of HGCN and GWT are\naligned and fused, so that the model can integrate multi dimensional\ninformation and improve the prediction robustness. Experimental results show\nthat our framework shows excellent prediction performance on all datasets. This\nstudy provides a complete solution for drug target discovery from black box\nprediction to mechanism decoding, and its methodology has important reference\nvalue for modeling complex biomolecular interaction systems.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u56fe\u795e\u7ecf\u7f51\u7edc\u548c\u591a\u5c3a\u5ea6\u4fe1\u53f7\u5904\u7406\u7684\u836f\u7269-\u9776\u70b9\u76f8\u4e92\u4f5c\u7528\u9884\u6d4b\u6846\u67b6\uff0c\u5177\u6709\u9ad8\u6548\u9884\u6d4b\u548c\u591a\u5c42\u6b21\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u4f20\u7edf\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u5b58\u5728\u9ed1\u7bb1\u95ee\u9898\uff0c\u96be\u4ee5\u63ed\u793a\u6a21\u578b\u51b3\u7b56\u673a\u5236\u4e0e\u751f\u7269\u5206\u5b50\u76f8\u4e92\u4f5c\u7528\u6a21\u5f0f\u7684\u6df1\u5c42\u5173\u8054\u3002", "method": "\u6574\u5408\u5f02\u6784\u56fe\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08HGCN\uff09\u548c\u591a\u5c3a\u5ea6\u4fe1\u53f7\u5904\u7406\u6280\u672f\uff0c\u8bbe\u8ba1\u4e86\u5c40\u90e8-\u5168\u5c40\u7279\u5f81\u534f\u540c\u611f\u77e5\u6a21\u5757\u3001\u591a\u5c3a\u5ea6\u56fe\u4fe1\u53f7\u5206\u89e3\u4e0e\u751f\u7269\u89e3\u91ca\u6a21\u5757\uff0c\u4ee5\u53ca\u5bf9\u6bd4\u5b66\u4e60\u7b56\u7565\u3002", "result": "\u5728\u6240\u6709\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u4f18\u5f02\u7684\u9884\u6d4b\u6027\u80fd\u3002", "conclusion": "\u4e3a\u836f\u7269\u9776\u70b9\u53d1\u73b0\u63d0\u4f9b\u4e86\u4ece\u9ed1\u7bb1\u9884\u6d4b\u5230\u673a\u5236\u89e3\u7801\u7684\u5b8c\u6574\u89e3\u51b3\u65b9\u6848\uff0c\u5bf9\u590d\u6742\u751f\u7269\u5206\u5b50\u76f8\u4e92\u4f5c\u7528\u7cfb\u7edf\u5efa\u6a21\u5177\u6709\u91cd\u8981\u53c2\u8003\u4ef7\u503c\u3002"}}
{"id": "2504.20690", "pdf": "https://arxiv.org/pdf/2504.20690", "abs": "https://arxiv.org/abs/2504.20690", "authors": ["Zechuan Zhang", "Ji Xie", "Yu Lu", "Zongxin Yang", "Yi Yang"], "title": "In-Context Edit: Enabling Instructional Image Editing with In-Context Generation in Large Scale Diffusion Transformer", "categories": ["cs.CV"], "comment": "Project Page: https://river-zhang.github.io/ICEdit-gh-pages/", "summary": "Instruction-based image editing enables robust image modification via natural\nlanguage prompts, yet current methods face a precision-efficiency tradeoff.\nFine-tuning methods demand significant computational resources and large\ndatasets, while training-free techniques struggle with instruction\ncomprehension and edit quality. We resolve this dilemma by leveraging\nlarge-scale Diffusion Transformer (DiT)' enhanced generation capacity and\nnative contextual awareness. Our solution introduces three contributions: (1)\nan in-context editing framework for zero-shot instruction compliance using\nin-context prompting, avoiding structural changes; (2) a LoRA-MoE hybrid tuning\nstrategy that enhances flexibility with efficient adaptation and dynamic expert\nrouting, without extensive retraining; and (3) an early filter inference-time\nscaling method using vision-language models (VLMs) to select better initial\nnoise early, improving edit quality. Extensive evaluations demonstrate our\nmethod's superiority: it outperforms state-of-the-art approaches while\nrequiring only 0.5% training data and 1% trainable parameters compared to\nconventional baselines. This work establishes a new paradigm that enables\nhigh-precision yet efficient instruction-guided editing. Codes and demos can be\nfound in https://river-zhang.github.io/ICEdit-gh-pages/.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6307\u4ee4\u7684\u56fe\u50cf\u7f16\u8f91\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u5408Diffusion Transformer\uff08DiT\uff09\u7684\u80fd\u529b\u548c\u4e0a\u4e0b\u6587\u611f\u77e5\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u7cbe\u5ea6\u548c\u6548\u7387\u4e0a\u7684\u6743\u8861\u95ee\u9898\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8e\u6307\u4ee4\u7684\u56fe\u50cf\u7f16\u8f91\u65b9\u6cd5\u5b58\u5728\u7cbe\u5ea6\u4e0e\u6548\u7387\u7684\u6743\u8861\u95ee\u9898\uff1a\u5fae\u8c03\u65b9\u6cd5\u9700\u8981\u5927\u91cf\u8ba1\u7b97\u8d44\u6e90\u548c\u6570\u636e\uff0c\u800c\u65e0\u8bad\u7ec3\u65b9\u6cd5\u5219\u96be\u4ee5\u7406\u89e3\u6307\u4ee4\u548c\u4fdd\u8bc1\u7f16\u8f91\u8d28\u91cf\u3002", "method": "\u63d0\u51fa\u4e86\u4e09\u4e2a\u521b\u65b0\u70b9\uff1a(1) \u57fa\u4e8e\u4e0a\u4e0b\u6587\u63d0\u793a\u7684\u96f6\u6837\u672c\u7f16\u8f91\u6846\u67b6\uff1b(2) LoRA-MoE\u6df7\u5408\u8c03\u4f18\u7b56\u7565\uff1b(3) \u4f7f\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u7684\u65e9\u671f\u8fc7\u6ee4\u63a8\u7406\u65f6\u95f4\u7f29\u653e\u65b9\u6cd5\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u4f18\u4e8e\u73b0\u6709\u6280\u672f\uff0c\u4ec5\u97000.5%\u7684\u8bad\u7ec3\u6570\u636e\u548c1%\u7684\u53ef\u8bad\u7ec3\u53c2\u6570\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u5efa\u7acb\u4e86\u4e00\u79cd\u9ad8\u7cbe\u5ea6\u4e14\u9ad8\u6548\u7684\u6307\u4ee4\u5f15\u5bfc\u7f16\u8f91\u65b0\u8303\u5f0f\u3002"}}
{"id": "2504.20105", "pdf": "https://arxiv.org/pdf/2504.20105", "abs": "https://arxiv.org/abs/2504.20105", "authors": ["Shuang Wang", "He Zhang", "Tianxing Wu", "Yueyou Zhang", "Wei Emma Zhang", "Quan Z. Sheng"], "title": "Electricity Cost Minimization for Multi-Workflow Allocation in Geo-Distributed Data Centers", "categories": ["cs.DC", "cs.AI"], "comment": "have been accepted by IEEE Transactions on Services Computing", "summary": "Worldwide, Geo-distributed Data Centers (GDCs) provide computing and storage\nservices for massive workflow applications, resulting in high electricity costs\nthat vary depending on geographical locations and time. How to reduce\nelectricity costs while satisfying the deadline constraints of workflow\napplications is important in GDCs, which is determined by the execution time of\nservers, power, and electricity price. Determining the completion time of\nworkflows with different server frequencies can be challenging, especially in\nscenarios with heterogeneous computing resources in GDCs. Moreover, the\nelectricity price is also different in geographical locations and may change\ndynamically. To address these challenges, we develop a geo-distributed system\narchitecture and propose an Electricity Cost aware Multiple Workflows\nScheduling algorithm (ECMWS) for servers of GDCs with fixed frequency and\npower. ECMWS comprises four stages, namely workflow sequencing, deadline\npartitioning, task sequencing, and resource allocation where two graph\nembedding models and a policy network are constructed to solve the Markov\nDecision Process (MDP). After statistically calibrating parameters and\nalgorithm components over a comprehensive set of workflow instances, the\nproposed algorithms are compared with the state-of-the-art methods over two\ntypes of workflow instances. The experimental results demonstrate that our\nproposed algorithm significantly outperforms other algorithms, achieving an\nimprovement of over 15\\% while maintaining an acceptable computational time.\nThe source codes are available at\nhttps://gitee.com/public-artifacts/ecmws-experiments.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5730\u7406\u5206\u5e03\u5f0f\u6570\u636e\u4e2d\u5fc3\uff08GDCs\uff09\u4e2d\u7535\u529b\u6210\u672c\u611f\u77e5\u7684\u591a\u5de5\u4f5c\u6d41\u8c03\u5ea6\u7b97\u6cd5\uff08ECMWS\uff09\uff0c\u4ee5\u964d\u4f4e\u7535\u529b\u6210\u672c\u5e76\u6ee1\u8db3\u5de5\u4f5c\u6d41\u7684\u622a\u6b62\u65f6\u95f4\u7ea6\u675f\u3002", "motivation": "\u5730\u7406\u5206\u5e03\u5f0f\u6570\u636e\u4e2d\u5fc3\u7684\u7535\u529b\u6210\u672c\u56e0\u5730\u7406\u4f4d\u7f6e\u548c\u65f6\u95f4\u800c\u5f02\uff0c\u5982\u4f55\u5728\u6ee1\u8db3\u5de5\u4f5c\u6d41\u622a\u6b62\u65f6\u95f4\u7684\u540c\u65f6\u964d\u4f4e\u7535\u529b\u6210\u672c\u662f\u4e00\u4e2a\u91cd\u8981\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86ECMWS\u7b97\u6cd5\uff0c\u5305\u62ec\u5de5\u4f5c\u6d41\u6392\u5e8f\u3001\u622a\u6b62\u65f6\u95f4\u5212\u5206\u3001\u4efb\u52a1\u6392\u5e8f\u548c\u8d44\u6e90\u5206\u914d\u56db\u4e2a\u9636\u6bb5\uff0c\u5229\u7528\u56fe\u5d4c\u5165\u6a21\u578b\u548c\u7b56\u7565\u7f51\u7edc\u89e3\u51b3\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\uff08MDP\uff09\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cECMWS\u7b97\u6cd5\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u6027\u80fd\u63d0\u5347\u8d85\u8fc715%\uff0c\u540c\u65f6\u4fdd\u6301\u53ef\u63a5\u53d7\u7684\u8ba1\u7b97\u65f6\u95f4\u3002", "conclusion": "ECMWS\u7b97\u6cd5\u5728\u964d\u4f4e\u7535\u529b\u6210\u672c\u548c\u6ee1\u8db3\u5de5\u4f5c\u6d41\u622a\u6b62\u65f6\u95f4\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4e3a\u5730\u7406\u5206\u5e03\u5f0f\u6570\u636e\u4e2d\u5fc3\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2504.20800", "pdf": "https://arxiv.org/pdf/2504.20800", "abs": "https://arxiv.org/abs/2504.20800", "authors": ["Weizhen He", "Yunfeng Yan", "Shixiang Tang", "Yiheng Deng", "Yangyang Zhong", "Pengxin Luo", "Donglian Qi"], "title": "Adept: Annotation-Denoising Auxiliary Tasks with Discrete Cosine Transform Map and Keypoint for Human-Centric Pretraining", "categories": ["cs.CV"], "comment": null, "summary": "Human-centric perception is the core of diverse computer vision tasks and has\nbeen a long-standing research focus. However, previous research studied these\nhuman-centric tasks individually, whose performance is largely limited to the\nsize of the public task-specific datasets. Recent human-centric methods\nleverage the additional modalities, e.g., depth, to learn fine-grained semantic\ninformation, which limits the benefit of pretraining models due to their\nsensitivity to camera views and the scarcity of RGB-D data on the Internet.\nThis paper improves the data scalability of human-centric pretraining methods\nby discarding depth information and exploring semantic information of RGB\nimages in the frequency space by Discrete Cosine Transform (DCT). We further\npropose new annotation denoising auxiliary tasks with keypoints and DCT maps to\nenforce the RGB image extractor to learn fine-grained semantic information of\nhuman bodies. Our extensive experiments show that when pretrained on\nlarge-scale datasets (COCO and AIC datasets) without depth annotation, our\nmodel achieves better performance than state-of-the-art methods by +0.5 mAP on\nCOCO, +1.4 PCKh on MPII and -0.51 EPE on Human3.6M for pose estimation, by\n+4.50 mIoU on Human3.6M for human parsing, by -3.14 MAE on SHA and -0.07 MAE on\nSHB for crowd counting, by +1.1 F1 score on SHA and +0.8 F1 score on SHA for\ncrowd localization, and by +0.1 mAP on Market1501 and +0.8 mAP on MSMT for\nperson ReID. We also validate the effectiveness of our method on MPII+NTURGBD\ndatasets", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eRGB\u56fe\u50cf\u9891\u7387\u7a7a\u95f4\uff08DCT\uff09\u7684\u4eba\u4f53\u4e2d\u5fc3\u9884\u8bad\u7ec3\u65b9\u6cd5\uff0c\u901a\u8fc7\u4e22\u5f03\u6df1\u5ea6\u4fe1\u606f\u5e76\u5229\u7528\u5173\u952e\u70b9\u548cDCT\u56fe\u7684\u8f85\u52a9\u4efb\u52a1\uff0c\u663e\u8457\u63d0\u5347\u4e86\u591a\u9879\u4efb\u52a1\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u4eba\u4f53\u4e2d\u5fc3\u4efb\u52a1\u7814\u7a76\u901a\u5e38\u4f9d\u8d56\u6df1\u5ea6\u4fe1\u606f\u6216\u4efb\u52a1\u7279\u5b9a\u6570\u636e\u96c6\uff0c\u4f46\u6df1\u5ea6\u4fe1\u606f\u5bf9\u76f8\u673a\u89c6\u89d2\u654f\u611f\u4e14\u6570\u636e\u7a00\u7f3a\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7RGB\u56fe\u50cf\u7684\u9891\u7387\u7a7a\u95f4\u5b66\u4e60\u7ec6\u7c92\u5ea6\u8bed\u4e49\u4fe1\u606f\uff0c\u63d0\u5347\u6570\u636e\u53ef\u6269\u5c55\u6027\u3002", "method": "\u5229\u7528\u79bb\u6563\u4f59\u5f26\u53d8\u6362\uff08DCT\uff09\u4eceRGB\u56fe\u50cf\u4e2d\u63d0\u53d6\u9891\u7387\u7a7a\u95f4\u4fe1\u606f\uff0c\u5e76\u8bbe\u8ba1\u5173\u952e\u70b9\u548cDCT\u56fe\u7684\u53bb\u566a\u8f85\u52a9\u4efb\u52a1\uff0c\u4ee5\u589e\u5f3a\u6a21\u578b\u5bf9\u4eba\u4f53\u7ec6\u7c92\u5ea6\u8bed\u4e49\u7684\u5b66\u4e60\u80fd\u529b\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\uff08\u5982COCO\u3001MPII\u3001Human3.6M\u7b49\uff09\u4e0a\uff0c\u6a21\u578b\u5728\u59ff\u6001\u4f30\u8ba1\u3001\u4eba\u4f53\u89e3\u6790\u3001\u4eba\u7fa4\u8ba1\u6570\u7b49\u4efb\u52a1\u4e2d\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u6027\u80fd\u63d0\u5347\u663e\u8457\u3002", "conclusion": "\u901a\u8fc7\u9891\u7387\u7a7a\u95f4\u5b66\u4e60\u548c\u8f85\u52a9\u4efb\u52a1\u8bbe\u8ba1\uff0c\u672c\u6587\u65b9\u6cd5\u5728\u65e0\u9700\u6df1\u5ea6\u4fe1\u606f\u7684\u60c5\u51b5\u4e0b\uff0c\u5b9e\u73b0\u4e86\u5bf9\u4eba\u4f53\u4e2d\u5fc3\u4efb\u52a1\u7684\u9ad8\u6548\u9884\u8bad\u7ec3\u548c\u6027\u80fd\u63d0\u5347\u3002"}}
{"id": "2504.20106", "pdf": "https://arxiv.org/pdf/2504.20106", "abs": "https://arxiv.org/abs/2504.20106", "authors": ["Ren-Wei Liang", "Chin-Ting Hsu", "Chan-Hung Yu", "Saransh Agrawal", "Shih-Cheng Huang", "Shang-Tse Chen", "Kuan-Hao Huang", "Shao-Hua Sun"], "title": "Adaptive Helpfulness-Harmlessness Alignment with Preference Vectors", "categories": ["cs.LG", "cs.AI"], "comment": "22 pages, 5 figures, 9 tables", "summary": "Ensuring that large language models (LLMs) are both helpful and harmless is a\ncritical challenge, as overly strict constraints can lead to excessive\nrefusals, while permissive models risk generating harmful content. Existing\napproaches, such as reinforcement learning from human feedback (RLHF) and\ndirect preference optimization (DPO), attempt to balance these trade-offs but\nsuffer from performance conflicts, limited controllability, and poor\nextendability. To address these issues, we propose Preference Vector, a novel\nframework inspired by task arithmetic. Instead of optimizing multiple\npreferences within a single objective, we train separate models on individual\npreferences, extract behavior shifts as preference vectors, and dynamically\nmerge them at test time. This modular approach enables fine-grained,\nuser-controllable preference adjustments and facilitates seamless integration\nof new preferences without retraining. Experiments show that our proposed\nPreference Vector framework improves helpfulness without excessive\nconservatism, allows smooth control over preference trade-offs, and supports\nscalable multi-preference alignment.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aPreference Vector\u7684\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u79bb\u8bad\u7ec3\u548c\u52a8\u6001\u5408\u5e76\u504f\u597d\u5411\u91cf\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u6027\u80fd\u51b2\u7a81\u3001\u53ef\u63a7\u6027\u548c\u6269\u5c55\u6027\u4e0a\u7684\u4e0d\u8db3\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u9700\u8981\u5728\u5e2e\u52a9\u6027\u548c\u65e0\u5bb3\u6027\u4e4b\u95f4\u53d6\u5f97\u5e73\u8861\uff0c\u73b0\u6709\u65b9\u6cd5\uff08\u5982RLHF\u548cDPO\uff09\u5b58\u5728\u6027\u80fd\u51b2\u7a81\u3001\u53ef\u63a7\u6027\u5dee\u548c\u6269\u5c55\u6027\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51faPreference Vector\u6846\u67b6\uff0c\u901a\u8fc7\u5355\u72ec\u8bad\u7ec3\u6a21\u578b\u63d0\u53d6\u504f\u597d\u5411\u91cf\uff0c\u5e76\u5728\u6d4b\u8bd5\u65f6\u52a8\u6001\u5408\u5e76\uff0c\u5b9e\u73b0\u7ec6\u7c92\u5ea6\u63a7\u5236\u548c\u6269\u5c55\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u6846\u67b6\u5728\u63d0\u5347\u5e2e\u52a9\u6027\u7684\u540c\u65f6\u907f\u514d\u4e86\u8fc7\u5ea6\u4fdd\u5b88\uff0c\u652f\u6301\u5e73\u6ed1\u7684\u504f\u597d\u6743\u8861\u548c\u53ef\u6269\u5c55\u7684\u591a\u504f\u597d\u5bf9\u9f50\u3002", "conclusion": "Preference Vector\u6846\u67b6\u4e3aLLMs\u7684\u504f\u597d\u8c03\u6574\u63d0\u4f9b\u4e86\u7075\u6d3b\u3001\u53ef\u63a7\u4e14\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2504.20829", "pdf": "https://arxiv.org/pdf/2504.20829", "abs": "https://arxiv.org/abs/2504.20829", "authors": ["Jiaxin Hong", "Sixu Chen", "Shuoyang Sun", "Hongyao Yu", "Hao Fang", "Yuqi Tan", "Bin Chen", "Shuhan Qi", "Jiawei Li"], "title": "GaussTrap: Stealthy Poisoning Attacks on 3D Gaussian Splatting for Targeted Scene Confusion", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "As 3D Gaussian Splatting (3DGS) emerges as a breakthrough in scene\nrepresentation and novel view synthesis, its rapid adoption in safety-critical\ndomains (e.g., autonomous systems, AR/VR) urgently demands scrutiny of\npotential security vulnerabilities. This paper presents the first systematic\nstudy of backdoor threats in 3DGS pipelines. We identify that adversaries may\nimplant backdoor views to induce malicious scene confusion during inference,\npotentially leading to environmental misperception in autonomous navigation or\nspatial distortion in immersive environments. To uncover this risk, we propose\nGuassTrap, a novel poisoning attack method targeting 3DGS models. GuassTrap\ninjects malicious views at specific attack viewpoints while preserving\nhigh-quality rendering in non-target views, ensuring minimal detectability and\nmaximizing potential harm. Specifically, the proposed method consists of a\nthree-stage pipeline (attack, stabilization, and normal training) to implant\nstealthy, viewpoint-consistent poisoned renderings in 3DGS, jointly optimizing\nattack efficacy and perceptual realism to expose security risks in 3D\nrendering. Extensive experiments on both synthetic and real-world datasets\ndemonstrate that GuassTrap can effectively embed imperceptible yet harmful\nbackdoor views while maintaining high-quality rendering in normal views,\nvalidating its robustness, adaptability, and practical applicability.", "AI": {"tldr": "\u672c\u6587\u9996\u6b21\u7cfb\u7edf\u7814\u7a76\u4e863D\u9ad8\u65af\u6cfc\u6e85\uff083DGS\uff09\u4e2d\u7684\u540e\u95e8\u5a01\u80c1\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aGuassTrap\u7684\u65b0\u578b\u653b\u51fb\u65b9\u6cd5\uff0c\u80fd\u591f\u5728\u7279\u5b9a\u89c6\u89d2\u690d\u5165\u6076\u610f\u89c6\u56fe\uff0c\u540c\u65f6\u4fdd\u6301\u975e\u76ee\u6807\u89c6\u56fe\u7684\u9ad8\u8d28\u91cf\u6e32\u67d3\u3002", "motivation": "\u968f\u77403DGS\u5728\u5b89\u5168\u5173\u952e\u9886\u57df\u7684\u5feb\u901f\u5e94\u7528\uff0c\u4e9f\u9700\u7814\u7a76\u5176\u6f5c\u5728\u5b89\u5168\u6f0f\u6d1e\uff0c\u5c24\u5176\u662f\u540e\u95e8\u5a01\u80c1\u53ef\u80fd\u5bfc\u81f4\u7684\u73af\u5883\u8bef\u5224\u6216\u7a7a\u95f4\u626d\u66f2\u3002", "method": "GuassTrap\u901a\u8fc7\u4e09\u9636\u6bb5\u6d41\u7a0b\uff08\u653b\u51fb\u3001\u7a33\u5b9a\u548c\u6b63\u5e38\u8bad\u7ec3\uff09\u690d\u5165\u9690\u853d\u4e14\u89c6\u89d2\u4e00\u81f4\u7684\u6709\u6bd2\u6e32\u67d3\uff0c\u4f18\u5316\u653b\u51fb\u6548\u679c\u548c\u611f\u77e5\u771f\u5b9e\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cGuassTrap\u80fd\u6709\u6548\u5d4c\u5165\u96be\u4ee5\u5bdf\u89c9\u7684\u6709\u5bb3\u540e\u95e8\u89c6\u56fe\uff0c\u540c\u65f6\u4fdd\u6301\u6b63\u5e38\u89c6\u56fe\u7684\u9ad8\u8d28\u91cf\u6e32\u67d3\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e863D\u6e32\u67d3\u4e2d\u7684\u5b89\u5168\u98ce\u9669\uff0cGuassTrap\u5c55\u793a\u4e86\u5176\u9c81\u68d2\u6027\u548c\u5b9e\u9645\u9002\u7528\u6027\u3002"}}
{"id": "2504.20112", "pdf": "https://arxiv.org/pdf/2504.20112", "abs": "https://arxiv.org/abs/2504.20112", "authors": ["Chowdhury Mohammad Abid Rahman", "Aldo H. Romero", "Prashnna K. Gyawali"], "title": "Supervised Pretraining for Material Property Prediction", "categories": ["cs.LG", "cond-mat.mtrl-sci", "cs.AI"], "comment": "21 pages, 7 figures, 2 algorithms, 6 tables", "summary": "Accurate prediction of material properties facilitates the discovery of novel\nmaterials with tailored functionalities. Deep learning models have recently\nshown superior accuracy and flexibility in capturing structure-property\nrelationships. However, these models often rely on supervised learning, which\nrequires large, well-annotated datasets an expensive and time-consuming\nprocess. Self-supervised learning (SSL) offers a promising alternative by\npretraining on large, unlabeled datasets to develop foundation models that can\nbe fine-tuned for material property prediction. In this work, we propose\nsupervised pretraining, where available class information serves as surrogate\nlabels to guide learning, even when downstream tasks involve unrelated material\nproperties. We evaluate this strategy on two state-of-the-art SSL models and\nintroduce a novel framework for supervised pretraining. To further enhance\nrepresentation learning, we propose a graph-based augmentation technique that\ninjects noise to improve robustness without structurally deforming material\ngraphs. The resulting foundation models are fine-tuned for six challenging\nmaterial property predictions, achieving significant performance gains over\nbaselines, ranging from 2% to 6.67% improvement in mean absolute error (MAE)\nand establishing a new benchmark in material property prediction. This study\nrepresents the first exploration of supervised pertaining with surrogate labels\nin material property prediction, advancing methodology and application in the\nfield.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u76d1\u7763\u9884\u8bad\u7ec3\u7684\u81ea\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\uff0c\u7528\u4e8e\u6750\u6599\u5c5e\u6027\u9884\u6d4b\uff0c\u901a\u8fc7\u5f15\u5165\u56fe\u589e\u5f3a\u6280\u672f\u548c\u4ee3\u7406\u6807\u7b7e\uff0c\u663e\u8457\u63d0\u5347\u4e86\u9884\u6d4b\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u4f9d\u8d56\u5927\u91cf\u6807\u6ce8\u6570\u636e\uff0c\u6210\u672c\u9ad8\u4e14\u8017\u65f6\u3002\u81ea\u76d1\u7763\u5b66\u4e60\uff08SSL\uff09\u63d0\u4f9b\u4e86\u4e00\u79cd\u66ff\u4ee3\u65b9\u6848\uff0c\u4f46\u5982\u4f55\u6709\u6548\u5229\u7528\u73b0\u6709\u6807\u7b7e\u4fe1\u606f\u63d0\u5347\u9884\u8bad\u7ec3\u6548\u679c\u5c1a\u5f85\u63a2\u7d22\u3002", "method": "\u63d0\u51fa\u76d1\u7763\u9884\u8bad\u7ec3\u7b56\u7565\uff0c\u5229\u7528\u4ee3\u7406\u6807\u7b7e\u6307\u5bfc\u5b66\u4e60\uff1b\u5f15\u5165\u56fe\u589e\u5f3a\u6280\u672f\u589e\u5f3a\u9c81\u68d2\u6027\uff1b\u5728\u4e24\u79cdSSL\u6a21\u578b\u4e0a\u8bc4\u4f30\uff0c\u5e76\u5f00\u53d1\u65b0\u6846\u67b6\u3002", "result": "\u5728\u516d\u79cd\u6750\u6599\u5c5e\u6027\u9884\u6d4b\u4efb\u52a1\u4e2d\uff0c\u6027\u80fd\u663e\u8457\u63d0\u5347\uff0cMAE\u6539\u55842%\u81f36.67%\uff0c\u786e\u7acb\u4e86\u65b0\u57fa\u51c6\u3002", "conclusion": "\u8be5\u7814\u7a76\u9996\u6b21\u63a2\u7d22\u4e86\u5728\u6750\u6599\u5c5e\u6027\u9884\u6d4b\u4e2d\u4f7f\u7528\u4ee3\u7406\u6807\u7b7e\u7684\u76d1\u7763\u9884\u8bad\u7ec3\uff0c\u63a8\u52a8\u4e86\u8be5\u9886\u57df\u7684\u65b9\u6cd5\u548c\u5e94\u7528\u53d1\u5c55\u3002"}}
{"id": "2504.20830", "pdf": "https://arxiv.org/pdf/2504.20830", "abs": "https://arxiv.org/abs/2504.20830", "authors": ["Jianyu Wu", "Yizhou Wang", "Xiangyu Yue", "Xinzhu Ma", "Jingyang Guo", "Dongzhan Zhou", "Wanli Ouyang", "Shixiang Tang"], "title": "CMT: A Cascade MAR with Topology Predictor for Multimodal Conditional CAD Generation", "categories": ["cs.CV"], "comment": null, "summary": "While accurate and user-friendly Computer-Aided Design (CAD) is crucial for\nindustrial design and manufacturing, existing methods still struggle to achieve\nthis due to their over-simplified representations or architectures incapable of\nsupporting multimodal design requirements. In this paper, we attempt to tackle\nthis problem from both methods and datasets aspects. First, we propose a\ncascade MAR with topology predictor (CMT), the first multimodal framework for\nCAD generation based on Boundary Representation (B-Rep). Specifically, the\ncascade MAR can effectively capture the ``edge-counters-surface'' priors that\nare essential in B-Reps, while the topology predictor directly estimates\ntopology in B-Reps from the compact tokens in MAR. Second, to facilitate\nlarge-scale training, we develop a large-scale multimodal CAD dataset, mmABC,\nwhich includes over 1.3 million B-Rep models with multimodal annotations,\nincluding point clouds, text descriptions, and multi-view images. Extensive\nexperiments show the superior of CMT in both conditional and unconditional CAD\ngeneration tasks. For example, we improve Coverage and Valid ratio by +10.68%\nand +10.3%, respectively, compared to state-of-the-art methods on ABC in\nunconditional generation. CMT also improves +4.01 Chamfer on image conditioned\nCAD generation on mmABC. The dataset, code and pretrained network shall be\nreleased.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u6a21\u6001CAD\u751f\u6210\u6846\u67b6CMT\uff0c\u7ed3\u5408\u4e86\u7ea7\u8054MAR\u548c\u62d3\u6251\u9884\u6d4b\u5668\uff0c\u5e76\u6784\u5efa\u4e86\u5927\u89c4\u6a21\u6570\u636e\u96c6mmABC\uff0c\u663e\u8457\u63d0\u5347\u4e86CAD\u751f\u6210\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709CAD\u65b9\u6cd5\u56e0\u7b80\u5316\u8868\u793a\u6216\u67b6\u6784\u4e0d\u8db3\u96be\u4ee5\u6ee1\u8db3\u591a\u6a21\u6001\u8bbe\u8ba1\u9700\u6c42\uff0c\u9700\u4ece\u65b9\u6cd5\u548c\u6570\u636e\u96c6\u4e24\u65b9\u9762\u89e3\u51b3\u3002", "method": "\u63d0\u51fa\u7ea7\u8054MAR\u4e0e\u62d3\u6251\u9884\u6d4b\u5668\u7ed3\u5408\u7684CMT\u6846\u67b6\uff0c\u6355\u6349B-Rep\u4e2d\u7684\u5148\u9a8c\u77e5\u8bc6\uff1b\u6784\u5efa\u5305\u542b130\u4e07B-Rep\u6a21\u578b\u7684\u591a\u6a21\u6001\u6570\u636e\u96c6mmABC\u3002", "result": "CMT\u5728\u65e0\u6761\u4ef6\u751f\u6210\u4efb\u52a1\u4e2dCoverage\u548cValid ratio\u5206\u522b\u63d0\u534710.68%\u548c10.3%\uff1b\u5728\u56fe\u50cf\u6761\u4ef6\u751f\u6210\u4e2dChamfer\u63d0\u53474.01\u3002", "conclusion": "CMT\u5728\u591a\u6a21\u6001CAD\u751f\u6210\u4e2d\u8868\u73b0\u4f18\u8d8a\uff0c\u6570\u636e\u96c6\u548c\u4ee3\u7801\u5c06\u516c\u5f00\u3002"}}
{"id": "2504.20114", "pdf": "https://arxiv.org/pdf/2504.20114", "abs": "https://arxiv.org/abs/2504.20114", "authors": ["Zhonghao Li", "Kunpeng Zhang", "Jinghuai Ou", "Shuliang Liu", "Xuming Hu"], "title": "TreeHop: Generate and Filter Next Query Embeddings Efficiently for Multi-hop Question Answering", "categories": ["cs.IR", "cs.AI", "cs.HC", "cs.LG"], "comment": "9 pages", "summary": "Retrieval-augmented generation (RAG) systems face significant challenges in\nmulti-hop question answering (MHQA), where complex queries require synthesizing\ninformation across multiple document chunks. Existing approaches typically rely\non iterative LLM-based query rewriting and routing, resulting in high\ncomputational costs due to repeated LLM invocations and multi-stage processes.\nTo address these limitations, we propose TreeHop, an embedding-level framework\nwithout the need for LLMs in query refinement. TreeHop dynamically updates\nquery embeddings by fusing semantic information from prior queries and\nretrieved documents, enabling iterative retrieval through embedding-space\noperations alone. This method replaces the traditional\n\"Retrieve-Rewrite-Vectorize-Retrieve\" cycle with a streamlined\n\"Retrieve-Embed-Retrieve\" loop, significantly reducing computational overhead.\nMoreover, a rule-based stop criterion is introduced to further prune redundant\nretrievals, balancing efficiency and recall rate. Experimental results show\nthat TreeHop rivals advanced RAG methods across three open-domain MHQA\ndatasets, achieving comparable performance with only 5\\%-0.4\\% of the model\nparameter size and reducing the query latency by approximately 99\\% compared to\nconcurrent approaches. This makes TreeHop a faster and more cost-effective\nsolution for deployment in a range of knowledge-intensive applications. For\nreproducibility purposes, codes and data are available here:\nhttps://github.com/allen-li1231/TreeHop.", "AI": {"tldr": "TreeHop\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700LLM\u67e5\u8be2\u91cd\u5199\u7684\u5d4c\u5165\u7ea7\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u66f4\u65b0\u67e5\u8be2\u5d4c\u5165\u548c\u5d4c\u5165\u7a7a\u95f4\u64cd\u4f5c\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u8ba1\u7b97\u5f00\u9500\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u591a\u8df3\u95ee\u7b54\u4efb\u52a1\u7684\u9ad8\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709RAG\u7cfb\u7edf\u5728\u591a\u8df3\u95ee\u7b54\u4e2d\u56e0\u8fed\u4ee3LLM\u67e5\u8be2\u91cd\u5199\u548c\u8def\u7531\u5bfc\u81f4\u7684\u9ad8\u8ba1\u7b97\u6210\u672c\u95ee\u9898\u3002", "method": "\u63d0\u51faTreeHop\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u66f4\u65b0\u67e5\u8be2\u5d4c\u5165\u548c\u5d4c\u5165\u7a7a\u95f4\u64cd\u4f5c\u5b9e\u73b0\u8fed\u4ee3\u68c0\u7d22\uff0c\u5e76\u5f15\u5165\u89c4\u5219\u505c\u6b62\u51c6\u5219\u4ee5\u51cf\u5c11\u5197\u4f59\u68c0\u7d22\u3002", "result": "\u5728\u4e09\u4e2a\u5f00\u653e\u57dfMHQA\u6570\u636e\u96c6\u4e0a\u6027\u80fd\u5ab2\u7f8e\u5148\u8fdbRAG\u65b9\u6cd5\uff0c\u6a21\u578b\u53c2\u6570\u91cf\u4ec55%-0.4%\uff0c\u67e5\u8be2\u5ef6\u8fdf\u964d\u4f4e\u7ea699%\u3002", "conclusion": "TreeHop\u662f\u4e00\u79cd\u66f4\u5feb\u901f\u3001\u6210\u672c\u6548\u76ca\u66f4\u9ad8\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u77e5\u8bc6\u5bc6\u96c6\u578b\u5e94\u7528\u3002"}}
{"id": "2504.20837", "pdf": "https://arxiv.org/pdf/2504.20837", "abs": "https://arxiv.org/abs/2504.20837", "authors": ["Julien Khlaut", "Elodie Ferreres", "Daniel Tordjman", "H\u00e9l\u00e8ne Philippe", "Tom Boeken", "Pierre Manceron", "Corentin Dancette"], "title": "RadSAM: Segmenting 3D radiological images with a 2D promptable model", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Medical image segmentation is a crucial and time-consuming task in clinical\ncare, where mask precision is extremely important. The Segment Anything Model\n(SAM) offers a promising approach, as it provides an interactive interface\nbased on visual prompting and edition to refine an initial segmentation. This\nmodel has strong generalization capabilities, does not rely on predefined\nclasses, and adapts to diverse objects; however, it is pre-trained on natural\nimages and lacks the ability to process medical data effectively. In addition,\nthis model is built for 2D images, whereas a whole medical domain is based on\n3D images, such as CT and MRI. Recent adaptations of SAM for medical imaging\nare based on 2D models, thus requiring one prompt per slice to segment 3D\nobjects, making the segmentation process tedious. They also lack important\nfeatures such as editing. To bridge this gap, we propose RadSAM, a novel method\nfor segmenting 3D objects with a 2D model from a single prompt. In practice, we\ntrain a 2D model using noisy masks as initial prompts, in addition to bounding\nboxes and points. We then use this novel prompt type with an iterative\ninference pipeline to reconstruct the 3D mask slice-by-slice. We introduce a\nbenchmark to evaluate the model's ability to segment 3D objects in CT images\nfrom a single prompt and evaluate the models' out-of-domain transfer and\nedition capabilities. We demonstrate the effectiveness of our approach against\nstate-of-the-art models on this benchmark using the AMOS abdominal organ\nsegmentation dataset.", "AI": {"tldr": "RadSAM\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e2D\u6a21\u578b\u76843D\u533b\u5b66\u56fe\u50cf\u5206\u5272\u65b9\u6cd5\uff0c\u901a\u8fc7\u5355\u6b21\u63d0\u793a\u5b9e\u73b03D\u5bf9\u8c61\u5206\u5272\uff0c\u89e3\u51b3\u4e86\u73b0\u6709SAM\u6a21\u578b\u5728\u533b\u5b66\u56fe\u50cf\u5904\u7406\u4e2d\u7684\u4e0d\u8db3\u3002", "motivation": "\u533b\u5b66\u56fe\u50cf\u5206\u5272\u5728\u4e34\u5e8a\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709SAM\u6a21\u578b\u57fa\u4e8e\u81ea\u7136\u56fe\u50cf\u9884\u8bad\u7ec3\uff0c\u65e0\u6cd5\u6709\u6548\u5904\u74063D\u533b\u5b66\u6570\u636e\uff0c\u4e14\u7f3a\u4e4f\u7f16\u8f91\u529f\u80fd\u3002", "method": "RadSAM\u901a\u8fc7\u8bad\u7ec32D\u6a21\u578b\uff0c\u4f7f\u7528\u566a\u58f0\u63a9\u6a21\u3001\u8fb9\u754c\u6846\u548c\u70b9\u4f5c\u4e3a\u521d\u59cb\u63d0\u793a\uff0c\u7ed3\u5408\u8fed\u4ee3\u63a8\u7406\u7ba1\u9053\u9010\u7247\u91cd\u5efa3D\u63a9\u6a21\u3002", "result": "\u5728AMOS\u8179\u90e8\u5668\u5b98\u5206\u5272\u6570\u636e\u96c6\u4e0a\uff0cRadSAM\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u5148\u8fdb\u6a21\u578b\uff0c\u5c55\u793a\u4e86\u51763D\u5206\u5272\u548c\u7f16\u8f91\u80fd\u529b\u3002", "conclusion": "RadSAM\u586b\u8865\u4e86SAM\u5728\u533b\u5b66\u56fe\u50cf\u5904\u7406\u4e2d\u7684\u7a7a\u767d\uff0c\u4e3a3D\u533b\u5b66\u56fe\u50cf\u5206\u5272\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u7075\u6d3b\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2504.20115", "pdf": "https://arxiv.org/pdf/2504.20115", "abs": "https://arxiv.org/abs/2504.20115", "authors": ["Zijie Lin", "Yiqing Shen", "Qilin Cai", "He Sun", "Jinrui Zhou", "Mingjun Xiao"], "title": "AutoP2C: An LLM-Based Agent Framework for Code Repository Generation from Multimodal Content in Academic Papers", "categories": ["cs.SE", "cs.AI"], "comment": null, "summary": "Machine Learning (ML) research is spread through academic papers featuring\nrich multimodal content, including text, diagrams, and tabular results.\nHowever, translating these multimodal elements into executable code remains a\nchallenging and time-consuming process that requires substantial ML expertise.\nWe introduce ``Paper-to-Code'' (P2C), a novel task that transforms the\nmultimodal content of scientific publications into fully executable code\nrepositories, which extends beyond the existing formulation of code generation\nthat merely converts textual descriptions into isolated code snippets. To\nautomate the P2C process, we propose AutoP2C, a multi-agent framework based on\nlarge language models that processes both textual and visual content from\nresearch papers to generate complete code repositories. Specifically, AutoP2C\ncontains four stages: (1) repository blueprint extraction from established\ncodebases, (2) multimodal content parsing that integrates information from\ntext, equations, and figures, (3) hierarchical task decomposition for\nstructured code generation, and (4) iterative feedback-driven debugging to\nensure functionality and performance. Evaluation on a benchmark of eight\nresearch papers demonstrates the effectiveness of AutoP2C, which can\nsuccessfully generate executable code repositories for all eight papers, while\nOpenAI-o1 or DeepSeek-R1 can only produce runnable code for one paper. The code\nis available at https://github.com/shoushouyu/Automated-Paper-to-Code.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u201cPaper-to-Code\u201d\uff08P2C\uff09\u7684\u65b0\u4efb\u52a1\uff0c\u65e8\u5728\u5c06\u79d1\u5b66\u8bba\u6587\u4e2d\u7684\u591a\u6a21\u6001\u5185\u5bb9\u8f6c\u5316\u4e3a\u53ef\u6267\u884c\u7684\u4ee3\u7801\u4ed3\u5e93\u3002\u4e3a\u6b64\uff0c\u4f5c\u8005\u5f00\u53d1\u4e86AutoP2C\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u9636\u6bb5\u5904\u7406\u751f\u6210\u5b8c\u6574\u4ee3\u7801\u3002", "motivation": "\u5f53\u524d\u5c06\u8bba\u6587\u4e2d\u7684\u591a\u6a21\u6001\u5185\u5bb9\u8f6c\u5316\u4e3a\u53ef\u6267\u884c\u4ee3\u7801\u7684\u8fc7\u7a0b\u8017\u65f6\u4e14\u9700\u8981\u4e13\u4e1a\u77e5\u8bc6\uff0c\u56e0\u6b64\u9700\u8981\u81ea\u52a8\u5316\u5de5\u5177\u6765\u7b80\u5316\u8fd9\u4e00\u8fc7\u7a0b\u3002", "method": "AutoP2C\u6846\u67b6\u5305\u542b\u56db\u4e2a\u9636\u6bb5\uff1a1) \u4ece\u73b0\u6709\u4ee3\u7801\u5e93\u4e2d\u63d0\u53d6\u4ed3\u5e93\u84dd\u56fe\uff1b2) \u89e3\u6790\u6587\u672c\u3001\u516c\u5f0f\u548c\u56fe\u8868\u7684\u591a\u6a21\u6001\u5185\u5bb9\uff1b3) \u5206\u5c42\u4efb\u52a1\u5206\u89e3\u4ee5\u751f\u6210\u7ed3\u6784\u5316\u4ee3\u7801\uff1b4) \u8fed\u4ee3\u53cd\u9988\u9a71\u52a8\u7684\u8c03\u8bd5\u4ee5\u786e\u4fdd\u529f\u80fd\u6027\u548c\u6027\u80fd\u3002", "result": "\u5728\u516b\u7bc7\u8bba\u6587\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cAutoP2C\u6210\u529f\u751f\u6210\u4e86\u6240\u6709\u8bba\u6587\u7684\u53ef\u6267\u884c\u4ee3\u7801\u4ed3\u5e93\uff0c\u800c\u5176\u4ed6\u6a21\u578b\uff08\u5982OpenAI-o1\u6216DeepSeek-R1\uff09\u4ec5\u80fd\u4e3a\u4e00\u7bc7\u8bba\u6587\u751f\u6210\u53ef\u8fd0\u884c\u4ee3\u7801\u3002", "conclusion": "AutoP2C\u6846\u67b6\u5728\u81ea\u52a8\u5316\u751f\u6210\u53ef\u6267\u884c\u4ee3\u7801\u4ed3\u5e93\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2504.20860", "pdf": "https://arxiv.org/pdf/2504.20860", "abs": "https://arxiv.org/abs/2504.20860", "authors": ["Mainak Singha", "Subhankar Roy", "Sarthak Mehrotra", "Ankit Jha", "Moloud Abdar", "Biplab Banerjee", "Elisa Ricci"], "title": "FedMVP: Federated Multi-modal Visual Prompt Tuning for Vision-Language Models", "categories": ["cs.CV"], "comment": null, "summary": "Textual prompt tuning adapts Vision-Language Models (e.g., CLIP) in federated\nlearning by tuning lightweight input tokens (or prompts) on local client data,\nwhile keeping network weights frozen. Post training, only the prompts are\nshared by the clients with the central server for aggregation. However, textual\nprompt tuning often struggles with overfitting to known concepts and may be\noverly reliant on memorized text features, limiting its adaptability to unseen\nconcepts. To address this limitation, we propose Federated Multimodal Visual\nPrompt Tuning (FedMVP) that conditions the prompts on comprehensive contextual\ninformation -- image-conditioned features and textual attribute features of a\nclass -- that is multimodal in nature. At the core of FedMVP is a PromptFormer\nmodule that synergistically aligns textual and visual features through\ncross-attention, enabling richer contexual integration. The dynamically\ngenerated multimodal visual prompts are then input to the frozen vision encoder\nof CLIP, and trained with a combination of CLIP similarity loss and a\nconsistency loss. Extensive evaluation on 20 datasets spanning three\ngeneralization settings demonstrates that FedMVP not only preserves performance\non in-distribution classes and domains, but also displays higher\ngeneralizability to unseen classes and domains when compared to\nstate-of-the-art methods. Codes will be released upon acceptance.", "AI": {"tldr": "FedMVP\u63d0\u51fa\u4e86\u4e00\u79cd\u8054\u90a6\u5b66\u4e60\u4e2d\u57fa\u4e8e\u591a\u6a21\u6001\u89c6\u89c9\u63d0\u793a\u8c03\u4f18\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u5408\u56fe\u50cf\u548c\u6587\u672c\u7279\u5f81\u751f\u6210\u52a8\u6001\u63d0\u793a\uff0c\u63d0\u5347\u4e86\u6a21\u578b\u5bf9\u672a\u89c1\u6982\u5ff5\u7684\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u4f20\u7edf\u6587\u672c\u63d0\u793a\u8c03\u4f18\u5728\u8054\u90a6\u5b66\u4e60\u4e2d\u5bb9\u6613\u8fc7\u62df\u5408\u5df2\u77e5\u6982\u5ff5\uff0c\u4e14\u4f9d\u8d56\u8bb0\u5fc6\u7684\u6587\u672c\u7279\u5f81\uff0c\u9650\u5236\u4e86\u5176\u5bf9\u672a\u89c1\u6982\u5ff5\u7684\u9002\u5e94\u6027\u3002", "method": "FedMVP\u5229\u7528PromptFormer\u6a21\u5757\u901a\u8fc7\u4ea4\u53c9\u6ce8\u610f\u529b\u5bf9\u9f50\u6587\u672c\u548c\u89c6\u89c9\u7279\u5f81\uff0c\u751f\u6210\u591a\u6a21\u6001\u89c6\u89c9\u63d0\u793a\uff0c\u5e76\u7ed3\u5408CLIP\u76f8\u4f3c\u6027\u635f\u5931\u548c\u4e00\u81f4\u6027\u635f\u5931\u8fdb\u884c\u8bad\u7ec3\u3002", "result": "\u572820\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cFedMVP\u5728\u4fdd\u6301\u5df2\u77e5\u5206\u5e03\u6027\u80fd\u7684\u540c\u65f6\uff0c\u5bf9\u672a\u89c1\u7c7b\u548c\u57df\u7684\u6cdb\u5316\u80fd\u529b\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "FedMVP\u901a\u8fc7\u591a\u6a21\u6001\u63d0\u793a\u8c03\u4f18\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u5728\u8054\u90a6\u5b66\u4e60\u4e2d\u7684\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2504.20117", "pdf": "https://arxiv.org/pdf/2504.20117", "abs": "https://arxiv.org/abs/2504.20117", "authors": ["Shubham Gandhi", "Dhruv Shah", "Manasi Patwardhan", "Lovekesh Vig", "Gautam Shroff"], "title": "ResearchCodeAgent: An LLM Multi-Agent System for Automated Codification of Research Methodologies", "categories": ["cs.SE", "cs.AI", "cs.CL", "cs.MA"], "comment": null, "summary": "In this paper we introduce ResearchCodeAgent, a novel multi-agent system\nleveraging large language models (LLMs) agents to automate the codification of\nresearch methodologies described in machine learning literature. The system\nbridges the gap between high-level research concepts and their practical\nimplementation, allowing researchers auto-generating code of existing research\npapers for benchmarking or building on top-of existing methods specified in the\nliterature with availability of partial or complete starter code.\nResearchCodeAgent employs a flexible agent architecture with a comprehensive\naction suite, enabling context-aware interactions with the research\nenvironment. The system incorporates a dynamic planning mechanism, utilizing\nboth short and long-term memory to adapt its approach iteratively. We evaluate\nResearchCodeAgent on three distinct machine learning tasks with distinct task\ncomplexity and representing different parts of the ML pipeline: data\naugmentation, optimization, and data batching. Our results demonstrate the\nsystem's effectiveness and generalizability, with 46.9% of generated code being\nhigh-quality and error-free, and 25% showing performance improvements over\nbaseline implementations. Empirical analysis shows an average reduction of\n57.9% in coding time compared to manual implementation. We observe higher gains\nfor more complex tasks. ResearchCodeAgent represents a significant step towards\nautomating the research implementation process, potentially accelerating the\npace of machine learning research.", "AI": {"tldr": "ResearchCodeAgent\u662f\u4e00\u4e2a\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u7528\u4e8e\u81ea\u52a8\u5316\u751f\u6210\u673a\u5668\u5b66\u4e60\u7814\u7a76\u8bba\u6587\u4e2d\u7684\u4ee3\u7801\u5b9e\u73b0\uff0c\u663e\u8457\u51cf\u5c11\u7f16\u7801\u65f6\u95f4\u5e76\u63d0\u9ad8\u4ee3\u7801\u8d28\u91cf\u3002", "motivation": "\u89e3\u51b3\u7814\u7a76\u6982\u5ff5\u4e0e\u5b9e\u9645\u4ee3\u7801\u5b9e\u73b0\u4e4b\u95f4\u7684\u9e3f\u6c9f\uff0c\u5e2e\u52a9\u7814\u7a76\u4eba\u5458\u5feb\u901f\u751f\u6210\u57fa\u51c6\u4ee3\u7801\u6216\u6269\u5c55\u73b0\u6709\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u7075\u6d3b\u7684\u667a\u80fd\u4f53\u67b6\u6784\u548c\u52a8\u6001\u89c4\u5212\u673a\u5236\uff0c\u7ed3\u5408\u77ed\u671f\u548c\u957f\u671f\u8bb0\u5fc6\uff0c\u652f\u6301\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u7814\u7a76\u73af\u5883\u4ea4\u4e92\u3002", "result": "\u5728\u4e09\u4e2a\u673a\u5668\u5b66\u4e60\u4efb\u52a1\u4e2d\uff0c46.9%\u7684\u751f\u6210\u4ee3\u7801\u9ad8\u8d28\u91cf\u4e14\u65e0\u9519\u8bef\uff0c25%\u4f18\u4e8e\u57fa\u7ebf\u5b9e\u73b0\uff0c\u7f16\u7801\u65f6\u95f4\u5e73\u5747\u51cf\u5c1157.9%\u3002", "conclusion": "ResearchCodeAgent\u4e3a\u7814\u7a76\u5b9e\u73b0\u81ea\u52a8\u5316\u8fc8\u51fa\u91cd\u8981\u4e00\u6b65\uff0c\u6709\u671b\u52a0\u901f\u673a\u5668\u5b66\u4e60\u7814\u7a76\u8fdb\u7a0b\u3002"}}
{"id": "2504.20865", "pdf": "https://arxiv.org/pdf/2504.20865", "abs": "https://arxiv.org/abs/2504.20865", "authors": ["Lorenzo Pellegrini", "Davide Cozzolino", "Serafino Pandolfini", "Davide Maltoni", "Matteo Ferrara", "Luisa Verdoliva", "Marco Prati", "Marco Ramilli"], "title": "AI-GenBench: A New Ongoing Benchmark for AI-Generated Image Detection", "categories": ["cs.CV"], "comment": "9 pages, 6 figures, 4 tables, code available:\n  https://github.com/MI-BioLab/AI-GenBench", "summary": "The rapid advancement of generative AI has revolutionized image creation,\nenabling high-quality synthesis from text prompts while raising critical\nchallenges for media authenticity. We present Ai-GenBench, a novel benchmark\ndesigned to address the urgent need for robust detection of AI-generated images\nin real-world scenarios. Unlike existing solutions that evaluate models on\nstatic datasets, Ai-GenBench introduces a temporal evaluation framework where\ndetection methods are incrementally trained on synthetic images, historically\nordered by their generative models, to test their ability to generalize to new\ngenerative models, such as the transition from GANs to diffusion models. Our\nbenchmark focuses on high-quality, diverse visual content and overcomes key\nlimitations of current approaches, including arbitrary dataset splits, unfair\ncomparisons, and excessive computational demands. Ai-GenBench provides a\ncomprehensive dataset, a standardized evaluation protocol, and accessible tools\nfor both researchers and non-experts (e.g., journalists, fact-checkers),\nensuring reproducibility while maintaining practical training requirements. By\nestablishing clear evaluation rules and controlled augmentation strategies,\nAi-GenBench enables meaningful comparison of detection methods and scalable\nsolutions. Code and data are publicly available to ensure reproducibility and\nto support the development of robust forensic detectors to keep pace with the\nrise of new synthetic generators.", "AI": {"tldr": "Ai-GenBench\u662f\u4e00\u4e2a\u65b0\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u68c0\u6d4bAI\u751f\u6210\u7684\u56fe\u50cf\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5982\u9759\u6001\u6570\u636e\u96c6\u548c\u4e0d\u516c\u5e73\u6bd4\u8f83\u3002", "motivation": "\u751f\u6210AI\u7684\u5feb\u901f\u53d1\u5c55\u5e26\u6765\u4e86\u9ad8\u8d28\u91cf\u56fe\u50cf\u5408\u6210\u7684\u80fd\u529b\uff0c\u4f46\u4e5f\u5f15\u53d1\u4e86\u5a92\u4f53\u771f\u5b9e\u6027\u7684\u6311\u6218\uff0c\u9700\u8981\u66f4\u5f3a\u5927\u7684\u68c0\u6d4b\u65b9\u6cd5\u3002", "method": "Ai-GenBench\u91c7\u7528\u65f6\u95f4\u8bc4\u4f30\u6846\u67b6\uff0c\u9010\u6b65\u8bad\u7ec3\u68c0\u6d4b\u6a21\u578b\uff0c\u6d4b\u8bd5\u5176\u5bf9\u65b0\u578b\u751f\u6210\u6a21\u578b\uff08\u5982\u4eceGAN\u5230\u6269\u6563\u6a21\u578b\uff09\u7684\u6cdb\u5316\u80fd\u529b\u3002", "result": "\u8be5\u57fa\u51c6\u63d0\u4f9b\u4e86\u9ad8\u8d28\u91cf\u3001\u591a\u6837\u5316\u7684\u89c6\u89c9\u5185\u5bb9\uff0c\u514b\u670d\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5e76\u63d0\u4f9b\u4e86\u6807\u51c6\u5316\u8bc4\u4f30\u5de5\u5177\u3002", "conclusion": "Ai-GenBench\u901a\u8fc7\u6e05\u6670\u7684\u8bc4\u4f30\u89c4\u5219\u548c\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u652f\u6301\u5f00\u53d1\u5f3a\u5927\u7684\u68c0\u6d4b\u5668\u4ee5\u5e94\u5bf9\u65b0\u578b\u751f\u6210\u6a21\u578b\u7684\u6311\u6218\u3002"}}
{"id": "2504.20118", "pdf": "https://arxiv.org/pdf/2504.20118", "abs": "https://arxiv.org/abs/2504.20118", "authors": ["Jinglin He", "Yunqi Guo", "Lai Kwan Lam", "Waikei Leung", "Lixing He", "Yuanan Jiang", "Chi Chiu Wang", "Guoliang Xing", "Hongkai Chen"], "title": "OpenTCM: A GraphRAG-Empowered LLM-based System for Traditional Chinese Medicine Knowledge Retrieval and Diagnosis", "categories": ["cs.IR", "cs.AI"], "comment": "8 pages, 4 figures", "summary": "Traditional Chinese Medicine (TCM) represents a rich repository of ancient\nmedical knowledge that continues to play an important role in modern\nhealthcare. Due to the complexity and breadth of the TCM literature, the\nintegration of AI technologies is critical for its modernization and broader\naccessibility. However, this integration poses considerable challenges,\nincluding the interpretation of obscure classical Chinese texts and the\nmodeling of intricate semantic relationships among TCM concepts. In this paper,\nwe develop OpenTCM, an LLM-based system that combines a domain-specific TCM\nknowledge graph and Graph-based Retrieval-Augmented Generation (GraphRAG).\nFirst, we extract more than 3.73 million classical Chinese characters from 68\ngynecological books in the Chinese Medical Classics Database, with the help of\nTCM and gynecology experts. Second, we construct a comprehensive\nmulti-relational knowledge graph comprising more than 48,000 entities and\n152,000 interrelationships, using customized prompts and Chinese-oriented LLMs\nsuch as DeepSeek and Kimi to ensure high-fidelity semantic understanding. Last,\nwe integrate OpenTCM with this knowledge graph, enabling high-fidelity\ningredient knowledge retrieval and diagnostic question-answering without model\nfine-tuning. Experimental evaluations demonstrate that our prompt design and\nmodel selection significantly improve knowledge graph quality, achieving a\nprecision of 98. 55% and an F1 score of 99. 55%. In addition, OpenTCM achieves\nmean expert scores of 4.5 in ingredient information retrieval and 3.8 in\ndiagnostic question-answering tasks, outperforming state-of-the-art solutions\nin real-world TCM use cases.", "AI": {"tldr": "OpenTCM\u662f\u4e00\u4e2a\u57fa\u4e8eLLM\u7684\u7cfb\u7edf\uff0c\u7ed3\u5408\u4e86\u4e2d\u533b\u77e5\u8bc6\u56fe\u8c31\u548c\u56fe\u589e\u5f3a\u68c0\u7d22\u751f\u6210\u6280\u672f\uff0c\u65e8\u5728\u89e3\u51b3\u4e2d\u533b\u6587\u732e\u7684\u590d\u6742\u6027\u548c\u8bed\u4e49\u5173\u7cfb\u5efa\u6a21\u95ee\u9898\uff0c\u53d6\u5f97\u4e86\u9ad8\u7cbe\u5ea6\u548c\u4e13\u5bb6\u8bc4\u5206\u3002", "motivation": "\u4e2d\u533b\u6587\u732e\u590d\u6742\u4e14\u96be\u4ee5\u89e3\u8bfb\uff0cAI\u6280\u672f\u7684\u6574\u5408\u5bf9\u4e2d\u533b\u73b0\u4ee3\u5316\u548c\u666e\u53ca\u81f3\u5173\u91cd\u8981\u3002", "method": "\u4ece\u4e2d\u533b\u7ecf\u5178\u6570\u636e\u5e93\u4e2d\u63d0\u53d6\u6570\u636e\uff0c\u6784\u5efa\u591a\u5173\u7cfb\u77e5\u8bc6\u56fe\u8c31\uff0c\u5e76\u7ed3\u5408LLM\u6280\u672f\u5f00\u53d1OpenTCM\u7cfb\u7edf\u3002", "result": "\u77e5\u8bc6\u56fe\u8c31\u7cbe\u5ea6\u8fbe98.55%\uff0cF1\u5206\u657099.55%\uff1bOpenTCM\u5728\u6210\u5206\u68c0\u7d22\u548c\u8bca\u65ad\u95ee\u7b54\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "OpenTCM\u4e3a\u4e2d\u533b\u73b0\u4ee3\u5316\u63d0\u4f9b\u4e86\u9ad8\u6548\u5de5\u5177\uff0c\u663e\u8457\u63d0\u5347\u4e86\u77e5\u8bc6\u68c0\u7d22\u548c\u8bca\u65ad\u95ee\u7b54\u7684\u51c6\u786e\u6027\u3002"}}
{"id": "2504.20872", "pdf": "https://arxiv.org/pdf/2504.20872", "abs": "https://arxiv.org/abs/2504.20872", "authors": ["Gilson Junior Soares", "Matheus Abrantes Cerqueira", "Jancarlo F. Gomes", "Laurent Najman", "Silvio Jamil F. Guimar\u00e3es", "Alexandre Xavier Falc\u00e3o"], "title": "FLIM-based Salient Object Detection Networks with Adaptive Decoders", "categories": ["cs.CV"], "comment": "This work has been submitted to the Journal of the Brazilian Computer\n  Society (JBCS)", "summary": "Salient Object Detection (SOD) methods can locate objects that stand out in\nan image, assign higher values to their pixels in a saliency map, and binarize\nthe map outputting a predicted segmentation mask. A recent tendency is to\ninvestigate pre-trained lightweight models rather than deep neural networks in\nSOD tasks, coping with applications under limited computational resources. In\nthis context, we have investigated lightweight networks using a methodology\nnamed Feature Learning from Image Markers (FLIM), which assumes that the\nencoder's kernels can be estimated from marker pixels on discriminative regions\nof a few representative images. This work proposes flyweight networks, hundreds\nof times lighter than lightweight models, for SOD by combining a FLIM encoder\nwith an adaptive decoder, whose weights are estimated for each input image by a\ngiven heuristic function. Such FLIM networks are trained from three to four\nrepresentative images only and without backpropagation, making the models\nsuitable for applications under labeled data constraints as well. We study five\nadaptive decoders; two of them are introduced here. Differently from the\nprevious ones that rely on one neuron per pixel with shared weights, the\nheuristic functions of the new adaptive decoders estimate the weights of each\nneuron per pixel. We compare FLIM models with adaptive decoders for two\nchallenging SOD tasks with three lightweight networks from the\nstate-of-the-art, two FLIM networks with decoders trained by backpropagation,\nand one FLIM network whose labeled markers define the decoder's weights. The\nexperiments demonstrate the advantages of the proposed networks over the\nbaselines, revealing the importance of further investigating such methods in\nnew applications.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8d85\u8f7b\u91cf\u7ea7\u7684\u663e\u8457\u76ee\u6807\u68c0\u6d4b\uff08SOD\uff09\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u5408FLIM\u7f16\u7801\u5668\u548c\u81ea\u9002\u5e94\u89e3\u7801\u5668\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u6a21\u578b\u7684\u8ba1\u7b97\u8d44\u6e90\u9700\u6c42\uff0c\u5e76\u5728\u5c11\u91cf\u4ee3\u8868\u6027\u56fe\u50cf\u4e0a\u8bad\u7ec3\uff0c\u65e0\u9700\u53cd\u5411\u4f20\u64ad\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u89e3\u51b3\u5728\u6709\u9650\u8ba1\u7b97\u8d44\u6e90\u4e0b\u8fdb\u884c\u663e\u8457\u76ee\u6807\u68c0\u6d4b\u7684\u95ee\u9898\uff0c\u540c\u65f6\u51cf\u5c11\u5bf9\u5927\u91cf\u6807\u8bb0\u6570\u636e\u7684\u4f9d\u8d56\u3002", "method": "\u65b9\u6cd5\u5305\u62ec\u4f7f\u7528FLIM\u7f16\u7801\u5668\u4ece\u5c11\u91cf\u4ee3\u8868\u6027\u56fe\u50cf\u4e2d\u5b66\u4e60\u7279\u5f81\uff0c\u5e76\u7ed3\u5408\u81ea\u9002\u5e94\u89e3\u7801\u5668\uff0c\u5176\u6743\u91cd\u901a\u8fc7\u542f\u53d1\u5f0f\u51fd\u6570\u4e3a\u6bcf\u4e2a\u8f93\u5165\u56fe\u50cf\u52a8\u6001\u4f30\u8ba1\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u63d0\u51fa\u7684FLIM\u7f51\u7edc\u5728\u6027\u80fd\u4e0a\u4f18\u4e8e\u73b0\u6709\u8f7b\u91cf\u7ea7\u6a21\u578b\uff0c\u9a8c\u8bc1\u4e86\u5176\u9ad8\u6548\u6027\u548c\u9002\u7528\u6027\u3002", "conclusion": "\u7ed3\u8bba\u5f3a\u8c03\u4e86\u8fdb\u4e00\u6b65\u7814\u7a76\u6b64\u7c7b\u65b9\u6cd5\u5728\u65b0\u5e94\u7528\u4e2d\u7684\u6f5c\u529b\uff0c\u5c24\u5176\u662f\u5728\u8d44\u6e90\u53d7\u9650\u548c\u6807\u8bb0\u6570\u636e\u6709\u9650\u7684\u573a\u666f\u4e0b\u3002"}}
{"id": "2504.20119", "pdf": "https://arxiv.org/pdf/2504.20119", "abs": "https://arxiv.org/abs/2504.20119", "authors": ["Lorenz Brehme", "Thomas Str\u00f6hle", "Ruth Breu"], "title": "Can LLMs Be Trusted for Evaluating RAG Systems? A Survey of Methods and Datasets", "categories": ["cs.IR", "cs.AI"], "comment": "8 Pages. This paper has been accepted for presentation at the IEEE\n  Swiss Conference on Data Science (SDS25)", "summary": "Retrieval-Augmented Generation (RAG) has advanced significantly in recent\nyears. The complexity of RAG systems, which involve multiple components-such as\nindexing, retrieval, and generation-along with numerous other parameters, poses\nsubstantial challenges for systematic evaluation and quality enhancement.\nPrevious research highlights that evaluating RAG systems is essential for\ndocumenting advancements, comparing configurations, and identifying effective\napproaches for domain-specific applications. This study systematically reviews\n63 academic articles to provide a comprehensive overview of state-of-the-art\nRAG evaluation methodologies, focusing on four key areas: datasets, retrievers,\nindexing and databases, and the generator component. We observe the feasibility\nof an automated evaluation approach for each component of a RAG system,\nleveraging an LLM capable of both generating evaluation datasets and conducting\nevaluations. In addition, we found that further practical research is essential\nto provide companies with clear guidance on the do's and don'ts of implementing\nand evaluating RAG systems. By synthesizing evaluation approaches for key RAG\ncomponents and emphasizing the creation and adaptation of domain-specific\ndatasets for benchmarking, we contribute to the advancement of systematic\nevaluation methods and the improvement of evaluation rigor for RAG systems.\nFurthermore, by examining the interplay between automated approaches leveraging\nLLMs and human judgment, we contribute to the ongoing discourse on balancing\nautomation and human input, clarifying their respective contributions,\nlimitations, and challenges in achieving robust and reliable evaluations.", "AI": {"tldr": "\u672c\u6587\u7cfb\u7edf\u7efc\u8ff0\u4e8663\u7bc7\u5b66\u672f\u6587\u7ae0\uff0c\u5168\u9762\u6982\u8ff0\u4e86RAG\u7cfb\u7edf\u7684\u6700\u65b0\u8bc4\u4f30\u65b9\u6cd5\uff0c\u91cd\u70b9\u5173\u6ce8\u6570\u636e\u96c6\u3001\u68c0\u7d22\u5668\u3001\u7d22\u5f15\u4e0e\u6570\u636e\u5e93\u4ee5\u53ca\u751f\u6210\u5668\u7ec4\u4ef6\uff0c\u5e76\u63d0\u51fa\u81ea\u52a8\u5316\u8bc4\u4f30\u7684\u53ef\u884c\u6027\u3002", "motivation": "RAG\u7cfb\u7edf\u7684\u590d\u6742\u6027\u4f7f\u5176\u8bc4\u4f30\u548c\u8d28\u91cf\u63d0\u5347\u9762\u4e34\u6311\u6218\uff0c\u9700\u7cfb\u7edf\u5316\u65b9\u6cd5\u4ee5\u8bb0\u5f55\u8fdb\u5c55\u3001\u6bd4\u8f83\u914d\u7f6e\u5e76\u4e3a\u9886\u57df\u5e94\u7528\u63d0\u4f9b\u6307\u5bfc\u3002", "method": "\u901a\u8fc7\u7cfb\u7edf\u7efc\u8ff063\u7bc7\u5b66\u672f\u6587\u7ae0\uff0c\u5206\u6790RAG\u7cfb\u7edf\u7684\u56db\u4e2a\u5173\u952e\u7ec4\u4ef6\uff0c\u5e76\u63a2\u8ba8\u5229\u7528LLM\u8fdb\u884c\u81ea\u52a8\u5316\u8bc4\u4f30\u7684\u53ef\u884c\u6027\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u81ea\u52a8\u5316\u8bc4\u4f30\u65b9\u6cd5\u53ef\u884c\uff0c\u4f46\u9700\u8fdb\u4e00\u6b65\u5b9e\u8df5\u7814\u7a76\u4ee5\u6307\u5bfc\u4f01\u4e1a\u5b9e\u65bd\u548c\u8bc4\u4f30RAG\u7cfb\u7edf\u3002", "conclusion": "\u672c\u6587\u901a\u8fc7\u7efc\u5408RAG\u7ec4\u4ef6\u8bc4\u4f30\u65b9\u6cd5\uff0c\u5f3a\u8c03\u9886\u57df\u7279\u5b9a\u6570\u636e\u96c6\u7684\u521b\u5efa\u4e0e\u9002\u5e94\uff0c\u63a8\u52a8\u4e86\u7cfb\u7edf\u5316\u8bc4\u4f30\u65b9\u6cd5\u7684\u8fdb\u6b65\uff0c\u5e76\u63a2\u8ba8\u4e86\u81ea\u52a8\u5316\u4e0e\u4eba\u5de5\u8bc4\u4f30\u7684\u5e73\u8861\u3002"}}
{"id": "2504.20902", "pdf": "https://arxiv.org/pdf/2504.20902", "abs": "https://arxiv.org/abs/2504.20902", "authors": ["Quentin Guimard", "Moreno D'Inc\u00e0", "Massimiliano Mancini", "Elisa Ricci"], "title": "Classifier-to-Bias: Toward Unsupervised Automatic Bias Detection for Visual Classifiers", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "CVPR 2025. Code: https://github.com/mardgui/C2B", "summary": "A person downloading a pre-trained model from the web should be aware of its\nbiases. Existing approaches for bias identification rely on datasets containing\nlabels for the task of interest, something that a non-expert may not have\naccess to, or may not have the necessary resources to collect: this greatly\nlimits the number of tasks where model biases can be identified. In this work,\nwe present Classifier-to-Bias (C2B), the first bias discovery framework that\nworks without access to any labeled data: it only relies on a textual\ndescription of the classification task to identify biases in the target\nclassification model. This description is fed to a large language model to\ngenerate bias proposals and corresponding captions depicting biases together\nwith task-specific target labels. A retrieval model collects images for those\ncaptions, which are then used to assess the accuracy of the model w.r.t. the\ngiven biases. C2B is training-free, does not require any annotations, has no\nconstraints on the list of biases, and can be applied to any pre-trained model\non any classification task. Experiments on two publicly available datasets show\nthat C2B discovers biases beyond those of the original datasets and outperforms\na recent state-of-the-art bias detection baseline that relies on task-specific\nannotations, being a promising first step toward addressing task-agnostic\nunsupervised bias detection.", "AI": {"tldr": "C2B\u662f\u4e00\u4e2a\u65e0\u9700\u6807\u6ce8\u6570\u636e\u7684\u504f\u5dee\u53d1\u73b0\u6846\u67b6\uff0c\u901a\u8fc7\u4efb\u52a1\u63cf\u8ff0\u751f\u6210\u504f\u5dee\u5efa\u8bae\u5e76\u8bc4\u4f30\u6a21\u578b\u504f\u5dee\u3002", "motivation": "\u73b0\u6709\u504f\u5dee\u8bc6\u522b\u65b9\u6cd5\u4f9d\u8d56\u6807\u6ce8\u6570\u636e\uff0c\u9650\u5236\u4e86\u5e94\u7528\u8303\u56f4\uff1bC2B\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u751f\u6210\u504f\u5dee\u5efa\u8bae\u548c\u63cf\u8ff0\uff0c\u901a\u8fc7\u68c0\u7d22\u6a21\u578b\u6536\u96c6\u56fe\u50cf\u5e76\u8bc4\u4f30\u6a21\u578b\u504f\u5dee\u3002", "result": "C2B\u5728\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u4f9d\u8d56\u6807\u6ce8\u7684\u57fa\u7ebf\u65b9\u6cd5\uff0c\u80fd\u53d1\u73b0\u66f4\u591a\u504f\u5dee\u3002", "conclusion": "C2B\u662f\u4efb\u52a1\u65e0\u5173\u3001\u65e0\u76d1\u7763\u504f\u5dee\u68c0\u6d4b\u7684\u6709\u524d\u666f\u7684\u7b2c\u4e00\u6b65\u3002"}}
{"id": "2504.20124", "pdf": "https://arxiv.org/pdf/2504.20124", "abs": "https://arxiv.org/abs/2504.20124", "authors": ["Abul Ehtesham", "Saket Kumar", "Aditi Singh", "Tala Talaei Khoei"], "title": "Pediatric Asthma Detection with Googles HeAR Model: An AI-Driven Respiratory Sound Classifier", "categories": ["cs.SD", "cs.AI"], "comment": null, "summary": "Early detection of asthma in children is crucial to prevent long-term\nrespiratory complications and reduce emergency interventions. This work\npresents an AI-powered diagnostic pipeline that leverages Googles Health\nAcoustic Representations (HeAR) model to detect early signs of asthma from\npediatric respiratory sounds. The SPRSound dataset, the first open-access\ncollection of annotated respiratory sounds in children aged 1 month to 18\nyears, is used to extract 2-second audio segments labeled as wheeze, crackle,\nrhonchi, stridor, or normal. Each segment is embedded into a 512-dimensional\nrepresentation using HeAR, a foundation model pretrained on 300 million\nhealth-related audio clips, including 100 million cough sounds. Multiple\nclassifiers, including SVM, Random Forest, and MLP, are trained on these\nembeddings to distinguish between asthma-indicative and normal sounds. The\nsystem achieves over 91\\% accuracy, with strong performance on precision-recall\nmetrics for positive cases. In addition to classification, learned embeddings\nare visualized using PCA, misclassifications are analyzed through waveform\nplayback, and ROC and confusion matrix insights are provided. This method\ndemonstrates that short, low-resource pediatric recordings, when powered by\nfoundation audio models, can enable fast, noninvasive asthma screening. The\napproach is especially promising for digital diagnostics in remote or\nunderserved healthcare settings.", "AI": {"tldr": "AI\u5229\u7528HeAR\u6a21\u578b\u5206\u6790\u513f\u7ae5\u547c\u5438\u97f3\uff0c\u5b9e\u73b0\u54ee\u5598\u65e9\u671f\u68c0\u6d4b\uff0c\u51c6\u786e\u7387\u8d8591%\u3002", "motivation": "\u65e9\u671f\u53d1\u73b0\u513f\u7ae5\u54ee\u5598\uff0c\u51cf\u5c11\u957f\u671f\u547c\u5438\u9053\u5e76\u53d1\u75c7\u548c\u7d27\u6025\u5e72\u9884\u3002", "method": "\u4f7f\u7528HeAR\u6a21\u578b\u63d0\u53d6\u547c\u5438\u97f3\u7279\u5f81\uff0c\u8bad\u7ec3\u591a\u79cd\u5206\u7c7b\u5668\u533a\u5206\u54ee\u5598\u97f3\u548c\u6b63\u5e38\u97f3\u3002", "result": "\u7cfb\u7edf\u51c6\u786e\u7387\u8d85\u8fc791%\uff0c\u5728\u9633\u6027\u75c5\u4f8b\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u9002\u7528\u4e8e\u8fdc\u7a0b\u533b\u7597\uff0c\u4e3a\u975e\u4fb5\u5165\u6027\u54ee\u5598\u7b5b\u67e5\u63d0\u4f9b\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2504.20948", "pdf": "https://arxiv.org/pdf/2504.20948", "abs": "https://arxiv.org/abs/2504.20948", "authors": ["Yanghui Song", "Chengfu Yang"], "title": "DS_FusionNet: Dynamic Dual-Stream Fusion with Bidirectional Knowledge Distillation for Plant Disease Recognition", "categories": ["cs.CV"], "comment": null, "summary": "Given the severe challenges confronting the global growth security of\neconomic crops, precise identification and prevention of plant diseases has\nemerged as a critical issue in artificial intelligence-enabled agricultural\ntechnology. To address the technical challenges in plant disease recognition,\nincluding small-sample learning, leaf occlusion, illumination variations, and\nhigh inter-class similarity, this study innovatively proposes a Dynamic\nDual-Stream Fusion Network (DS_FusionNet). The network integrates a\ndual-backbone architecture, deformable dynamic fusion modules, and\nbidirectional knowledge distillation strategy, significantly enhancing\nrecognition accuracy. Experimental results demonstrate that DS_FusionNet\nachieves classification accuracies exceeding 90% using only 10% of the\nPlantDisease and CIFAR-10 datasets, while maintaining 85% accuracy on the\ncomplex PlantWild dataset, exhibiting exceptional generalization capabilities.\nThis research not only provides novel technical insights for fine-grained image\nclassification but also establishes a robust foundation for precise\nidentification and management of agricultural diseases.", "AI": {"tldr": "\u63d0\u51fa\u52a8\u6001\u53cc\u6d41\u878d\u5408\u7f51\u7edc\uff08DS_FusionNet\uff09\uff0c\u89e3\u51b3\u690d\u7269\u75c5\u5bb3\u8bc6\u522b\u4e2d\u7684\u5c0f\u6837\u672c\u5b66\u4e60\u3001\u53f6\u7247\u906e\u6321\u7b49\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u8bc6\u522b\u7cbe\u5ea6\u3002", "motivation": "\u5168\u7403\u7ecf\u6d4e\u4f5c\u7269\u751f\u957f\u5b89\u5168\u9762\u4e34\u4e25\u5cfb\u6311\u6218\uff0c\u7cbe\u786e\u8bc6\u522b\u548c\u9884\u9632\u690d\u7269\u75c5\u5bb3\u6210\u4e3a\u519c\u4e1a\u6280\u672f\u4e2d\u7684\u5173\u952e\u95ee\u9898\u3002", "method": "\u91c7\u7528\u53cc\u4e3b\u5e72\u67b6\u6784\u3001\u53ef\u53d8\u5f62\u52a8\u6001\u878d\u5408\u6a21\u5757\u548c\u53cc\u5411\u77e5\u8bc6\u84b8\u998f\u7b56\u7565\u3002", "result": "\u5728PlantDisease\u548cCIFAR-10\u6570\u636e\u96c6\u4e0a\u4ec5\u752810%\u6570\u636e\u8fbe\u523090%\u4ee5\u4e0a\u5206\u7c7b\u51c6\u786e\u7387\uff0c\u5728\u590d\u6742PlantWild\u6570\u636e\u96c6\u4e0a\u4fdd\u630185%\u51c6\u786e\u7387\u3002", "conclusion": "\u7814\u7a76\u4e3a\u7ec6\u7c92\u5ea6\u56fe\u50cf\u5206\u7c7b\u63d0\u4f9b\u65b0\u6280\u672f\u601d\u8def\uff0c\u5e76\u4e3a\u519c\u4e1a\u75c5\u5bb3\u7cbe\u786e\u8bc6\u522b\u4e0e\u7ba1\u7406\u5960\u5b9a\u57fa\u7840\u3002"}}
{"id": "2504.20125", "pdf": "https://arxiv.org/pdf/2504.20125", "abs": "https://arxiv.org/abs/2504.20125", "authors": ["Michael Pekala", "Gregory Canal", "Samuel Barham", "Milena B. Graziano", "Morgan Trexler", "Leslie Hamilton", "Elizabeth Reilly", "Christopher D. Stiles"], "title": "Towards Large Language Models for Lunar Mission Planning and In Situ Resource Utilization", "categories": ["cs.DL", "cs.AI"], "comment": null, "summary": "A key factor for lunar mission planning is the ability to assess the local\navailability of raw materials. However, many potentially relevant measurements\nare scattered across a variety of scientific publications. In this paper we\nconsider the viability of obtaining lunar composition data by leveraging LLMs\nto rapidly process a corpus of scientific publications. While leveraging LLMs\nto obtain knowledge from scientific documents is not new, this particular\napplication presents interesting challenges due to the heterogeneity of lunar\nsamples and the nuances involved in their characterization. Accuracy and\nuncertainty quantification are particularly crucial since many materials\nproperties can be sensitive to small variations in composition. Our findings\nindicate that off-the-shelf LLMs are generally effective at extracting data\nfrom tables commonly found in these documents. However, there remains\nopportunity to further refine the data we extract in this initial approach; in\nparticular, to capture fine-grained mineralogy information and to improve\nperformance on more subtle/complex pieces of information.", "AI": {"tldr": "\u5229\u7528LLM\u4ece\u79d1\u5b66\u6587\u732e\u4e2d\u5feb\u901f\u63d0\u53d6\u6708\u7403\u6210\u5206\u6570\u636e\u7684\u53ef\u884c\u6027\u7814\u7a76\uff0c\u53d1\u73b0\u73b0\u6210LLM\u5bf9\u8868\u683c\u6570\u636e\u63d0\u53d6\u6709\u6548\uff0c\u4f46\u9700\u8fdb\u4e00\u6b65\u4f18\u5316\u4ee5\u6355\u83b7\u66f4\u7cbe\u7ec6\u7684\u4fe1\u606f\u3002", "motivation": "\u6708\u7403\u4efb\u52a1\u89c4\u5212\u9700\u8bc4\u4f30\u539f\u6750\u6599\u53ef\u7528\u6027\uff0c\u4f46\u76f8\u5173\u6570\u636e\u5206\u6563\u4e8e\u5927\u91cf\u6587\u732e\u4e2d\uff0cLLM\u53ef\u52a0\u901f\u6570\u636e\u5904\u7406\u3002", "method": "\u5229\u7528LLM\u5904\u7406\u79d1\u5b66\u6587\u732e\uff0c\u63d0\u53d6\u6708\u7403\u6210\u5206\u6570\u636e\uff0c\u5173\u6ce8\u51c6\u786e\u6027\u548c\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u3002", "result": "\u73b0\u6210LLM\u5bf9\u8868\u683c\u6570\u636e\u63d0\u53d6\u6709\u6548\uff0c\u4f46\u5bf9\u590d\u6742\u6216\u7ec6\u5fae\u4fe1\u606f\u4ecd\u9700\u6539\u8fdb\u3002", "conclusion": "LLM\u5728\u6708\u7403\u6210\u5206\u6570\u636e\u63d0\u53d6\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u4f46\u9700\u8fdb\u4e00\u6b65\u4f18\u5316\u4ee5\u63d0\u5347\u6027\u80fd\u3002"}}
{"id": "2504.20970", "pdf": "https://arxiv.org/pdf/2504.20970", "abs": "https://arxiv.org/abs/2504.20970", "authors": ["Mete Erdogan", "Sebnem Demirtas"], "title": "SVD Based Least Squares for X-Ray Pneumonia Classification Using Deep Features", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "Preprint submitted to IEEE International Workshop on Machine Learning\n  for Signal Processing (MLSP), 2025", "summary": "Accurate and early diagnosis of pneumonia through X-ray imaging is essential\nfor effective treatment and improved patient outcomes. Recent advancements in\nmachine learning have enabled automated diagnostic tools that assist\nradiologists in making more reliable and efficient decisions. In this work, we\npropose a Singular Value Decomposition-based Least Squares (SVD-LS) framework\nfor multi-class pneumonia classification, leveraging powerful feature\nrepresentations from state-of-the-art self-supervised and transfer learning\nmodels. Rather than relying on computationally expensive gradient based\nfine-tuning, we employ a closed-form, non-iterative classification approach\nthat ensures efficiency without compromising accuracy. Experimental results\ndemonstrate that SVD-LS achieves competitive performance while offering\nsignificantly reduced computational costs, making it a viable alternative for\nreal-time medical imaging applications.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eSVD-LS\u7684\u591a\u7c7b\u80ba\u708e\u5206\u7c7b\u6846\u67b6\uff0c\u7ed3\u5408\u81ea\u76d1\u7763\u548c\u8fc1\u79fb\u5b66\u4e60\u6a21\u578b\uff0c\u5b9e\u73b0\u9ad8\u6548\u4e14\u51c6\u786e\u7684\u8bca\u65ad\u3002", "motivation": "\u901a\u8fc7X\u5149\u5f71\u50cf\u5b9e\u73b0\u80ba\u708e\u7684\u65e9\u671f\u51c6\u786e\u8bca\u65ad\u5bf9\u6cbb\u7597\u81f3\u5173\u91cd\u8981\uff0c\u673a\u5668\u5b66\u4e60\u5de5\u5177\u53ef\u8f85\u52a9\u653e\u5c04\u79d1\u533b\u751f\u63d0\u9ad8\u6548\u7387\u548c\u53ef\u9760\u6027\u3002", "method": "\u91c7\u7528SVD-LS\u6846\u67b6\uff0c\u5229\u7528\u81ea\u76d1\u7763\u548c\u8fc1\u79fb\u5b66\u4e60\u6a21\u578b\u63d0\u53d6\u7279\u5f81\uff0c\u907f\u514d\u8ba1\u7b97\u91cf\u5927\u7684\u68af\u5ea6\u5fae\u8c03\uff0c\u91c7\u7528\u95ed\u5f0f\u975e\u8fed\u4ee3\u5206\u7c7b\u65b9\u6cd5\u3002", "result": "\u5b9e\u9a8c\u8868\u660eSVD-LS\u5728\u4fdd\u6301\u7ade\u4e89\u529b\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\uff0c\u9002\u7528\u4e8e\u5b9e\u65f6\u533b\u7597\u5f71\u50cf\u5e94\u7528\u3002", "conclusion": "SVD-LS\u662f\u4e00\u79cd\u9ad8\u6548\u4e14\u51c6\u786e\u7684\u80ba\u708e\u5206\u7c7b\u65b9\u6cd5\uff0c\u9002\u5408\u5b9e\u9645\u533b\u7597\u573a\u666f\u3002"}}
{"id": "2504.20131", "pdf": "https://arxiv.org/pdf/2504.20131", "abs": "https://arxiv.org/abs/2504.20131", "authors": ["Antonio A. Ginart", "Naveen Kodali", "Jason Lee", "Caiming Xiong", "Silvio Savarese", "John R. Emmons"], "title": "LZ Penalty: An information-theoretic repetition penalty for autoregressive language models", "categories": ["cs.LG", "cs.AI", "cs.IT", "math.IT"], "comment": "Preprint (draft)", "summary": "We introduce the LZ penalty, a penalty specialized for reducing degenerate\nrepetitions in autoregressive language models without loss of capability. The\npenalty is based on the codelengths in the LZ77 universal lossless compression\nalgorithm. Through the lens of the prediction-compression duality, decoding the\nLZ penalty has the interpretation of sampling from the residual distribution\nafter removing the information that is highly compressible. We demonstrate the\nLZ penalty enables state-of-the-art open-source reasoning models to operate\nwith greedy (temperature zero) decoding without loss of capability and without\ninstances of degenerate repetition. Both the industry-standard frequency\npenalty and repetition penalty are ineffective, incurring degenerate repetition\nrates of up to 4%.", "AI": {"tldr": "LZ\u60e9\u7f5a\u662f\u4e00\u79cd\u7528\u4e8e\u51cf\u5c11\u81ea\u56de\u5f52\u8bed\u8a00\u6a21\u578b\u4e2d\u9000\u5316\u91cd\u590d\u7684\u4e13\u7528\u60e9\u7f5a\u65b9\u6cd5\uff0c\u57fa\u4e8eLZ77\u65e0\u635f\u538b\u7f29\u7b97\u6cd5\u7684\u7f16\u7801\u957f\u5ea6\uff0c\u901a\u8fc7\u9884\u6d4b-\u538b\u7f29\u5bf9\u5076\u6027\u5b9e\u73b0\u3002", "motivation": "\u89e3\u51b3\u81ea\u56de\u5f52\u8bed\u8a00\u6a21\u578b\u4e2d\u9000\u5316\u91cd\u590d\u95ee\u9898\uff0c\u540c\u65f6\u4fdd\u6301\u6a21\u578b\u80fd\u529b\u3002", "method": "\u57fa\u4e8eLZ77\u7b97\u6cd5\u7684\u7f16\u7801\u957f\u5ea6\u8bbe\u8ba1\u60e9\u7f5a\u673a\u5236\uff0c\u901a\u8fc7\u9884\u6d4b-\u538b\u7f29\u5bf9\u5076\u6027\u79fb\u9664\u9ad8\u5ea6\u53ef\u538b\u7f29\u4fe1\u606f\u3002", "result": "LZ\u60e9\u7f5a\u4f7f\u5f00\u6e90\u63a8\u7406\u6a21\u578b\u5728\u8d2a\u5a6a\u89e3\u7801\u4e0b\u65e0\u9000\u5316\u91cd\u590d\uff0c\u4f18\u4e8e\u884c\u4e1a\u6807\u51c6\u7684\u9891\u7387\u548c\u91cd\u590d\u60e9\u7f5a\u65b9\u6cd5\u3002", "conclusion": "LZ\u60e9\u7f5a\u6709\u6548\u89e3\u51b3\u4e86\u9000\u5316\u91cd\u590d\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u6a21\u578b\u89e3\u7801\u7684\u7a33\u5b9a\u6027\u3002"}}
{"id": "2504.20995", "pdf": "https://arxiv.org/pdf/2504.20995", "abs": "https://arxiv.org/abs/2504.20995", "authors": ["Haoyu Zhen", "Qiao Sun", "Hongxin Zhang", "Junyan Li", "Siyuan Zhou", "Yilun Du", "Chuang Gan"], "title": "TesserAct: Learning 4D Embodied World Models", "categories": ["cs.CV", "cs.RO"], "comment": "Project Page: https://tesseractworld.github.io/", "summary": "This paper presents an effective approach for learning novel 4D embodied\nworld models, which predict the dynamic evolution of 3D scenes over time in\nresponse to an embodied agent's actions, providing both spatial and temporal\nconsistency. We propose to learn a 4D world model by training on RGB-DN (RGB,\nDepth, and Normal) videos. This not only surpasses traditional 2D models by\nincorporating detailed shape, configuration, and temporal changes into their\npredictions, but also allows us to effectively learn accurate inverse dynamic\nmodels for an embodied agent. Specifically, we first extend existing robotic\nmanipulation video datasets with depth and normal information leveraging\noff-the-shelf models. Next, we fine-tune a video generation model on this\nannotated dataset, which jointly predicts RGB-DN (RGB, Depth, and Normal) for\neach frame. We then present an algorithm to directly convert generated RGB,\nDepth, and Normal videos into a high-quality 4D scene of the world. Our method\nensures temporal and spatial coherence in 4D scene predictions from embodied\nscenarios, enables novel view synthesis for embodied environments, and\nfacilitates policy learning that significantly outperforms those derived from\nprior video-based world models.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5b66\u4e604D\u4e16\u754c\u6a21\u578b\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7RGB-DN\u89c6\u9891\u9884\u6d4b\u52a8\u60013D\u573a\u666f\u7684\u65f6\u7a7a\u6f14\u5316\uff0c\u5e76\u652f\u6301\u667a\u80fd\u4f53\u7684\u52a8\u4f5c\u54cd\u5e94\u3002", "motivation": "\u4f20\u7edf2D\u6a21\u578b\u65e0\u6cd5\u6355\u6349\u573a\u666f\u7684\u8be6\u7ec6\u5f62\u72b6\u3001\u914d\u7f6e\u548c\u65f6\u95f4\u53d8\u5316\uff0c\u9650\u5236\u4e86\u667a\u80fd\u4f53\u7684\u52a8\u6001\u6a21\u578b\u5b66\u4e60\u3002", "method": "\u6269\u5c55\u673a\u5668\u4eba\u64cd\u4f5c\u89c6\u9891\u6570\u636e\u96c6\u4e3aRGB-DN\u683c\u5f0f\uff0c\u5fae\u8c03\u89c6\u9891\u751f\u6210\u6a21\u578b\u4ee5\u9884\u6d4bRGB-DN\u5e27\uff0c\u5e76\u8bbe\u8ba1\u7b97\u6cd5\u5c06\u751f\u6210\u7684\u89c6\u9891\u8f6c\u6362\u4e3a\u9ad8\u8d28\u91cf4D\u573a\u666f\u3002", "result": "\u65b9\u6cd5\u5b9e\u73b0\u4e86\u65f6\u7a7a\u4e00\u81f4\u76844D\u573a\u666f\u9884\u6d4b\uff0c\u652f\u6301\u65b0\u89c6\u89d2\u5408\u6210\uff0c\u5e76\u5728\u7b56\u7565\u5b66\u4e60\u4e0a\u4f18\u4e8e\u73b0\u6709\u89c6\u9891\u4e16\u754c\u6a21\u578b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u667a\u80fd\u4f53\u63d0\u4f9b\u4e86\u66f4\u51c6\u786e\u7684\u52a8\u6001\u4e16\u754c\u6a21\u578b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7b56\u7565\u5b66\u4e60\u6548\u679c\u3002"}}
{"id": "2504.20168", "pdf": "https://arxiv.org/pdf/2504.20168", "abs": "https://arxiv.org/abs/2504.20168", "authors": ["Nishant Subramani", "Jason Eisner", "Justin Svegliato", "Benjamin Van Durme", "Yu Su", "Sam Thomson"], "title": "MICE for CATs: Model-Internal Confidence Estimation for Calibrating Agents with Tools", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Accepted at NAACL 2025. Code:\n  https://github.com/microsoft/mice_for_cats", "summary": "Tool-using agents that act in the world need to be both useful and safe.\nWell-calibrated model confidences can be used to weigh the risk versus reward\nof potential actions, but prior work shows that many models are poorly\ncalibrated. Inspired by interpretability literature exploring the internals of\nmodels, we propose a novel class of model-internal confidence estimators (MICE)\nto better assess confidence when calling tools. MICE first decodes from each\nintermediate layer of the language model using logitLens and then computes\nsimilarity scores between each layer's generation and the final output. These\nfeatures are fed into a learned probabilistic classifier to assess confidence\nin the decoded output. On the simulated trial and error (STE) tool-calling\ndataset using Llama3 models, we find that MICE beats or matches the baselines\non smoothed expected calibration error. Using MICE confidences to determine\nwhether to call a tool significantly improves over strong baselines on a new\nmetric, expected tool-calling utility. Further experiments show that MICE is\nsample-efficient, can generalize zero-shot to unseen APIs, and results in\nhigher tool-calling utility in scenarios with varying risk levels. Our code is\nopen source, available at https://github.com/microsoft/mice_for_cats.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u6a21\u578b\u5185\u90e8\u7f6e\u4fe1\u5ea6\u4f30\u8ba1\u5668\uff08MICE\uff09\uff0c\u901a\u8fc7\u89e3\u7801\u8bed\u8a00\u6a21\u578b\u5404\u4e2d\u95f4\u5c42\u5e76\u8ba1\u7b97\u76f8\u4f3c\u5ea6\u5206\u6570\uff0c\u7ed3\u5408\u6982\u7387\u5206\u7c7b\u5668\u8bc4\u4f30\u7f6e\u4fe1\u5ea6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5de5\u5177\u8c03\u7528\u7684\u5b9e\u7528\u6027\u548c\u5b89\u5168\u6027\u3002", "motivation": "\u5de5\u5177\u4f7f\u7528\u4ee3\u7406\u9700\u8981\u5728\u5b9e\u7528\u6027\u548c\u5b89\u5168\u6027\u4e4b\u95f4\u6743\u8861\uff0c\u800c\u73b0\u6709\u6a21\u578b\u7684\u7f6e\u4fe1\u5ea6\u6821\u51c6\u8f83\u5dee\uff0c\u56e0\u6b64\u9700\u8981\u66f4\u51c6\u786e\u7684\u7f6e\u4fe1\u5ea6\u8bc4\u4f30\u65b9\u6cd5\u3002", "method": "MICE\u901a\u8fc7\u89e3\u7801\u8bed\u8a00\u6a21\u578b\u5404\u4e2d\u95f4\u5c42\uff08\u4f7f\u7528logitLens\uff09\uff0c\u8ba1\u7b97\u5404\u5c42\u751f\u6210\u4e0e\u6700\u7ec8\u8f93\u51fa\u7684\u76f8\u4f3c\u5ea6\uff0c\u8f93\u5165\u6982\u7387\u5206\u7c7b\u5668\u8bc4\u4f30\u7f6e\u4fe1\u5ea6\u3002", "result": "\u5728STE\u6570\u636e\u96c6\u4e0a\uff0cMICE\u5728\u5e73\u6ed1\u9884\u671f\u6821\u51c6\u8bef\u5dee\u4e0a\u4f18\u4e8e\u57fa\u7ebf\uff1b\u4f7f\u7528MICE\u7f6e\u4fe1\u5ea6\u663e\u8457\u63d0\u5347\u4e86\u5de5\u5177\u8c03\u7528\u7684\u9884\u671f\u6548\u7528\u3002", "conclusion": "MICE\u9ad8\u6548\u3001\u53ef\u96f6\u6837\u672c\u6cdb\u5316\u81f3\u65b0API\uff0c\u5e76\u5728\u4e0d\u540c\u98ce\u9669\u573a\u666f\u4e2d\u63d0\u5347\u5de5\u5177\u8c03\u7528\u6548\u7528\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90\u3002"}}
{"id": "2504.20996", "pdf": "https://arxiv.org/pdf/2504.20996", "abs": "https://arxiv.org/abs/2504.20996", "authors": ["Sicheng Mo", "Thao Nguyen", "Xun Huang", "Siddharth Srinivasan Iyer", "Yijun Li", "Yuchen Liu", "Abhishek Tandon", "Eli Shechtman", "Krishna Kumar Singh", "Yong Jae Lee", "Bolei Zhou", "Yuheng Li"], "title": "X-Fusion: Introducing New Modality to Frozen Large Language Models", "categories": ["cs.CV"], "comment": "Project Page: https://sichengmo.github.io/XFusion/", "summary": "We propose X-Fusion, a framework that extends pretrained Large Language\nModels (LLMs) for multimodal tasks while preserving their language\ncapabilities. X-Fusion employs a dual-tower design with modality-specific\nweights, keeping the LLM's parameters frozen while integrating vision-specific\ninformation for both understanding and generation. Our experiments demonstrate\nthat X-Fusion consistently outperforms alternative architectures on both\nimage-to-text and text-to-image tasks. We find that incorporating\nunderstanding-focused data improves generation quality, reducing image data\nnoise enhances overall performance, and feature alignment accelerates\nconvergence for smaller models but has minimal impact on larger ones. Our\nfindings provide valuable insights into building efficient unified multimodal\nmodels.", "AI": {"tldr": "X-Fusion\u662f\u4e00\u4e2a\u6269\u5c55\u9884\u8bad\u7ec3\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7528\u4e8e\u591a\u6a21\u6001\u4efb\u52a1\u7684\u6846\u67b6\uff0c\u4fdd\u6301\u5176\u8bed\u8a00\u80fd\u529b\u7684\u540c\u65f6\u6574\u5408\u89c6\u89c9\u4fe1\u606f\u3002", "motivation": "\u89e3\u51b3\u5982\u4f55\u5728\u4fdd\u7559LLM\u8bed\u8a00\u80fd\u529b\u7684\u540c\u65f6\u6269\u5c55\u5176\u591a\u6a21\u6001\u4efb\u52a1\u80fd\u529b\u7684\u95ee\u9898\u3002", "method": "\u91c7\u7528\u53cc\u5854\u8bbe\u8ba1\uff0c\u51bb\u7ed3LLM\u53c2\u6570\uff0c\u6574\u5408\u6a21\u6001\u7279\u5b9a\u6743\u91cd\uff0c\u7ed3\u5408\u89c6\u89c9\u4fe1\u606f\u3002", "result": "\u5728\u56fe\u50cf\u5230\u6587\u672c\u548c\u6587\u672c\u5230\u56fe\u50cf\u4efb\u52a1\u4e0a\u8868\u73b0\u4f18\u4e8e\u5176\u4ed6\u67b6\u6784\uff0c\u7406\u89e3\u6570\u636e\u63d0\u5347\u751f\u6210\u8d28\u91cf\uff0c\u51cf\u5c11\u56fe\u50cf\u566a\u58f0\u63d0\u5347\u6027\u80fd\u3002", "conclusion": "X-Fusion\u4e3a\u6784\u5efa\u9ad8\u6548\u7edf\u4e00\u591a\u6a21\u6001\u6a21\u578b\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u89c1\u89e3\u3002"}}
{"id": "2504.20172", "pdf": "https://arxiv.org/pdf/2504.20172", "abs": "https://arxiv.org/abs/2504.20172", "authors": ["Erik Jahn", "Karthik Karnik", "Leonard J. Schulman"], "title": "Causal Identification in Time Series Models", "categories": ["cs.LG", "cs.AI", "stat.ME", "stat.ML"], "comment": null, "summary": "In this paper, we analyze the applicability of the Causal Identification\nalgorithm to causal time series graphs with latent confounders. Since these\ngraphs extend over infinitely many time steps, deciding whether causal effects\nacross arbitrary time intervals are identifiable appears to require computation\non graph segments of unbounded size. Even for deciding the identifiability of\nintervention effects on variables that are close in time, no bound is known on\nhow many time steps in the past need to be considered. We give a first bound of\nthis kind that only depends on the number of variables per time step and the\nmaximum time lag of any direct or latent causal effect. More generally, we show\nthat applying the Causal Identification algorithm to a constant-size segment of\nthe time series graph is sufficient to decide identifiability of causal\neffects, even across unbounded time intervals.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u56e0\u679c\u8bc6\u522b\u7b97\u6cd5\u5728\u542b\u6f5c\u5728\u6df7\u6742\u56e0\u5b50\u7684\u56e0\u679c\u65f6\u95f4\u5e8f\u5217\u56fe\u4e2d\u7684\u5e94\u7528\uff0c\u63d0\u51fa\u4e86\u4e00\u4e2a\u4ec5\u4f9d\u8d56\u4e8e\u6bcf\u65f6\u95f4\u6b65\u53d8\u91cf\u6570\u91cf\u548c\u6700\u5927\u65f6\u95f4\u6ede\u540e\u7684\u8fb9\u754c\u6761\u4ef6\u3002", "motivation": "\u7531\u4e8e\u65f6\u95f4\u5e8f\u5217\u56fe\u6d89\u53ca\u65e0\u9650\u65f6\u95f4\u6b65\uff0c\u786e\u5b9a\u4efb\u610f\u65f6\u95f4\u95f4\u9694\u7684\u56e0\u679c\u6548\u5e94\u662f\u5426\u53ef\u8bc6\u522b\u9700\u8981\u5904\u7406\u65e0\u754c\u5927\u5c0f\u7684\u56fe\u6bb5\uff0c\u76ee\u524d\u7f3a\u4e4f\u76f8\u5173\u8fb9\u754c\u6761\u4ef6\u3002", "method": "\u901a\u8fc7\u5206\u6790\u56e0\u679c\u65f6\u95f4\u5e8f\u5217\u56fe\u7684\u7279\u6027\uff0c\u63d0\u51fa\u4e00\u4e2a\u4ec5\u4f9d\u8d56\u4e8e\u6bcf\u65f6\u95f4\u6b65\u53d8\u91cf\u6570\u91cf\u548c\u6700\u5927\u65f6\u95f4\u6ede\u540e\u7684\u8fb9\u754c\u6761\u4ef6\uff0c\u5e76\u8bc1\u660e\u6052\u5b9a\u5927\u5c0f\u56fe\u6bb5\u8db3\u4ee5\u5224\u65ad\u56e0\u679c\u6548\u5e94\u7684\u53ef\u8bc6\u522b\u6027\u3002", "result": "\u9996\u6b21\u63d0\u51fa\u4e86\u4e00\u4e2a\u8fb9\u754c\u6761\u4ef6\uff0c\u8bc1\u660e\u6052\u5b9a\u5927\u5c0f\u56fe\u6bb5\u8db3\u4ee5\u5224\u65ad\u56e0\u679c\u6548\u5e94\u7684\u53ef\u8bc6\u522b\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u56e0\u679c\u65f6\u95f4\u5e8f\u5217\u56fe\u7684\u53ef\u8bc6\u522b\u6027\u95ee\u9898\u63d0\u4f9b\u4e86\u7406\u8bba\u652f\u6301\uff0c\u7b80\u5316\u4e86\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u8ba1\u7b97\u590d\u6742\u5ea6\u3002"}}
{"id": "2504.20998", "pdf": "https://arxiv.org/pdf/2504.20998", "abs": "https://arxiv.org/abs/2504.20998", "authors": ["Thao Nguyen", "Krishna Kumar Singh", "Jing Shi", "Trung Bui", "Yong Jae Lee", "Yuheng Li"], "title": "YoChameleon: Personalized Vision and Language Generation", "categories": ["cs.CV", "cs.AI"], "comment": "CVPR 2025; Project page: https://thaoshibe.github.io/YoChameleon", "summary": "Large Multimodal Models (e.g., GPT-4, Gemini, Chameleon) have evolved into\npowerful tools with millions of users. However, they remain generic models and\nlack personalized knowledge of specific user concepts. Previous work has\nexplored personalization for text generation, yet it remains unclear how these\nmethods can be adapted to new modalities, such as image generation. In this\npaper, we introduce Yo'Chameleon, the first attempt to study personalization\nfor large multimodal models. Given 3-5 images of a particular concept,\nYo'Chameleon leverages soft-prompt tuning to embed subject-specific information\nto (i) answer questions about the subject and (ii) recreate pixel-level details\nto produce images of the subject in new contexts. Yo'Chameleon is trained with\n(i) a self-prompting optimization mechanism to balance performance across\nmultiple modalities, and (ii) a ``soft-positive\" image generation approach to\nenhance image quality in a few-shot setting.", "AI": {"tldr": "Yo'Chameleon\u662f\u9996\u4e2a\u7814\u7a76\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\u4e2a\u6027\u5316\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u8f6f\u63d0\u793a\u8c03\u4f18\u5b9e\u73b0\u7279\u5b9a\u4e3b\u9898\u7684\u77e5\u8bc6\u5d4c\u5165\uff0c\u652f\u6301\u95ee\u7b54\u548c\u56fe\u50cf\u751f\u6210\u3002", "motivation": "\u73b0\u6709\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\u7f3a\u4e4f\u4e2a\u6027\u5316\u77e5\u8bc6\uff0c\u5c24\u5176\u5728\u56fe\u50cf\u751f\u6210\u9886\u57df\u3002", "method": "\u4f7f\u75283-5\u5f20\u56fe\u50cf\u8f93\u5165\uff0c\u901a\u8fc7\u8f6f\u63d0\u793a\u8c03\u4f18\u5d4c\u5165\u4e3b\u9898\u4fe1\u606f\uff0c\u7ed3\u5408\u81ea\u63d0\u793a\u4f18\u5316\u548c\u8f6f\u6b63\u56fe\u50cf\u751f\u6210\u6280\u672f\u3002", "result": "\u80fd\u591f\u5728\u5c11\u91cf\u6837\u672c\u4e0b\u5b9e\u73b0\u9ad8\u8d28\u91cf\u7684\u4e2a\u6027\u5316\u95ee\u7b54\u548c\u56fe\u50cf\u751f\u6210\u3002", "conclusion": "Yo'Chameleon\u4e3a\u591a\u6a21\u6001\u6a21\u578b\u4e2a\u6027\u5316\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u5c24\u5176\u5728\u56fe\u50cf\u751f\u6210\u9886\u57df\u8868\u73b0\u7a81\u51fa\u3002"}}
{"id": "2504.20220", "pdf": "https://arxiv.org/pdf/2504.20220", "abs": "https://arxiv.org/abs/2504.20220", "authors": ["Henning Sch\u00e4fer", "Cynthia S. Schmidt", "Johannes Wutzkowsky", "Kamil Lorek", "Lea Reinartz", "Johannes R\u00fcckert", "Christian Temme", "Britta B\u00f6ckmann", "Peter A. Horn", "Christoph M. Friedrich"], "title": "A Multimodal Pipeline for Clinical Data Extraction: Applying Vision-Language Models to Scans of Transfusion Reaction Reports", "categories": ["cs.CL", "cs.CV", "68T07", "I.7.5; I.4.7; I.2.7; H.3.3; J.3"], "comment": null, "summary": "Despite the growing adoption of electronic health records, many processes\nstill rely on paper documents, reflecting the heterogeneous real-world\nconditions in which healthcare is delivered. The manual transcription process\nis time-consuming and prone to errors when transferring paper-based data to\ndigital formats. To streamline this workflow, this study presents an\nopen-source pipeline that extracts and categorizes checkbox data from scanned\ndocuments. Demonstrated on transfusion reaction reports, the design supports\nadaptation to other checkbox-rich document types. The proposed method\nintegrates checkbox detection, multilingual optical character recognition (OCR)\nand multilingual vision-language models (VLMs). The pipeline achieves high\nprecision and recall compared against annually compiled gold-standards from\n2017 to 2024. The result is a reduction in administrative workload and accurate\nregulatory reporting. The open-source availability of this pipeline encourages\nself-hosted parsing of checkbox forms.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u5f00\u6e90\u6d41\u7a0b\uff0c\u4ece\u626b\u63cf\u6587\u6863\u4e2d\u63d0\u53d6\u548c\u5206\u7c7b\u590d\u9009\u6846\u6570\u636e\uff0c\u4ee5\u51cf\u5c11\u4eba\u5de5\u8f6c\u5f55\u9519\u8bef\u5e76\u63d0\u9ad8\u6548\u7387\u3002", "motivation": "\u5c3d\u7ba1\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\u666e\u53ca\uff0c\u8bb8\u591a\u6d41\u7a0b\u4ecd\u4f9d\u8d56\u7eb8\u8d28\u6587\u6863\uff0c\u624b\u52a8\u8f6c\u5f55\u8017\u65f6\u4e14\u6613\u51fa\u9519\u3002", "method": "\u6574\u5408\u590d\u9009\u6846\u68c0\u6d4b\u3001\u591a\u8bed\u8a00OCR\u548c\u591a\u8bed\u8a00\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u3002", "result": "\u4e0e2017-2024\u5e74\u7684\u9ec4\u91d1\u6807\u51c6\u76f8\u6bd4\uff0c\u5b9e\u73b0\u4e86\u9ad8\u7cbe\u5ea6\u548c\u53ec\u56de\u7387\uff0c\u51cf\u5c11\u4e86\u884c\u653f\u8d1f\u62c5\u3002", "conclusion": "\u5f00\u6e90\u6d41\u7a0b\u53ef\u81ea\u6258\u7ba1\u89e3\u6790\u590d\u9009\u6846\u8868\u5355\uff0c\u63d0\u5347\u51c6\u786e\u6027\u548c\u6548\u7387\u3002"}}
{"id": "2504.20183", "pdf": "https://arxiv.org/pdf/2504.20183", "abs": "https://arxiv.org/abs/2504.20183", "authors": ["Niki van Stein", "Anna V. Kononova", "Haoran Yin", "Thomas B\u00e4ck"], "title": "BLADE: Benchmark suite for LLM-driven Automated Design and Evolution of iterative optimisation heuristics", "categories": ["cs.SE", "cs.AI", "cs.NE"], "comment": "9 pages, accepted at GECCO Workshop 2025", "summary": "The application of Large Language Models (LLMs) for Automated Algorithm\nDiscovery (AAD), particularly for optimisation heuristics, is an emerging field\nof research. This emergence necessitates robust, standardised benchmarking\npractices to rigorously evaluate the capabilities and limitations of LLM-driven\nAAD methods and the resulting generated algorithms, especially given the\nopacity of their design process and known issues with existing benchmarks. To\naddress this need, we introduce BLADE (Benchmark suite for LLM-driven Automated\nDesign and Evolution), a modular and extensible framework specifically designed\nfor benchmarking LLM-driven AAD methods in a continuous black-box optimisation\ncontext. BLADE integrates collections of benchmark problems (including MA-BBOB\nand SBOX-COST among others) with instance generators and textual descriptions\naimed at capability-focused testing, such as generalisation, specialisation and\ninformation exploitation. It offers flexible experimental setup options,\nstandardised logging for reproducibility and fair comparison, incorporates\nmethods for analysing the AAD process (e.g., Code Evolution Graphs and various\nvisualisation approaches) and facilitates comparison against human-designed\nbaselines through integration with established tools like IOHanalyser and\nIOHexplainer. BLADE provides an `out-of-the-box' solution to systematically\nevaluate LLM-driven AAD approaches. The framework is demonstrated through two\ndistinct use cases exploring mutation prompt strategies and function\nspecialisation.", "AI": {"tldr": "BLADE\u662f\u4e00\u4e2a\u6a21\u5757\u5316\u3001\u53ef\u6269\u5c55\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u8bc4\u4f30LLM\u9a71\u52a8\u7684\u81ea\u52a8\u5316\u7b97\u6cd5\u8bbe\u8ba1\u65b9\u6cd5\uff0c\u63d0\u4f9b\u6807\u51c6\u5316\u57fa\u51c6\u6d4b\u8bd5\u548c\u5206\u6790\u5de5\u5177\u3002", "motivation": "\u7531\u4e8eLLM\u9a71\u52a8\u7684\u81ea\u52a8\u5316\u7b97\u6cd5\u8bbe\u8ba1\u65b9\u6cd5\u7f3a\u4e4f\u900f\u660e\u6027\u4e14\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u5b58\u5728\u95ee\u9898\uff0c\u9700\u8981\u4e00\u79cd\u6807\u51c6\u5316\u7684\u8bc4\u4f30\u6846\u67b6\u3002", "method": "BLADE\u6574\u5408\u4e86\u591a\u79cd\u57fa\u51c6\u95ee\u9898\u3001\u5b9e\u4f8b\u751f\u6210\u5668\u548c\u6587\u672c\u63cf\u8ff0\uff0c\u652f\u6301\u7075\u6d3b\u7684\u5b9e\u9a8c\u8bbe\u7f6e\u548c\u6807\u51c6\u5316\u65e5\u5fd7\u8bb0\u5f55\uff0c\u5e76\u63d0\u4f9b\u5206\u6790\u5de5\u5177\u3002", "result": "BLADE\u901a\u8fc7\u4e24\u4e2a\u7528\u4f8b\u5c55\u793a\u4e86\u5176\u5728\u63a2\u7d22\u7a81\u53d8\u63d0\u793a\u7b56\u7565\u548c\u51fd\u6570\u7279\u5316\u65b9\u9762\u7684\u5b9e\u7528\u6027\u3002", "conclusion": "BLADE\u4e3a\u7cfb\u7edf\u8bc4\u4f30LLM\u9a71\u52a8\u7684\u81ea\u52a8\u5316\u7b97\u6cd5\u8bbe\u8ba1\u65b9\u6cd5\u63d0\u4f9b\u4e86\u5f00\u7bb1\u5373\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2504.20339", "pdf": "https://arxiv.org/pdf/2504.20339", "abs": "https://arxiv.org/abs/2504.20339", "authors": ["Cedric Le Gentil", "Leonardo Brizi", "Daniil Lisus", "Xinyuan Qiao", "Giorgio Grisetti", "Timothy D. Barfoot"], "title": "DRO: Doppler-Aware Direct Radar Odometry", "categories": ["cs.RO", "cs.CV"], "comment": "Accepted for presentation at RSS 2025", "summary": "A renaissance in radar-based sensing for mobile robotic applications is\nunderway. Compared to cameras or lidars, millimetre-wave radars have the\nability to `see' through thin walls, vegetation, and adversarial weather\nconditions such as heavy rain, fog, snow, and dust. In this paper, we propose a\nnovel SE(2) odometry approach for spinning frequency-modulated continuous-wave\nradars. Our method performs scan-to-local-map registration of the incoming\nradar data in a direct manner using all the radar intensity information without\nthe need for feature or point cloud extraction. The method performs locally\ncontinuous trajectory estimation and accounts for both motion and Doppler\ndistortion of the radar scans. If the radar possesses a specific frequency\nmodulation pattern that makes radial Doppler velocities observable, an\nadditional Doppler-based constraint is formulated to improve the velocity\nestimate and enable odometry in geometrically feature-deprived scenarios (e.g.,\nfeatureless tunnels). Our method has been validated on over 250km of on-road\ndata sourced from public datasets (Boreas and MulRan) and collected using our\nautomotive platform. With the aid of a gyroscope, it outperforms\nstate-of-the-art methods and achieves an average relative translation error of\n0.26% on the Boreas leaderboard. When using data with the appropriate\nDoppler-enabling frequency modulation pattern, the translation error is reduced\nto 0.18% in similar environments. We also benchmarked our algorithm using 1.5\nhours of data collected with a mobile robot in off-road environments with\nvarious levels of structure to demonstrate its versatility. Our real-time\nimplementation is publicly available: https://github.com/utiasASRL/dro.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u7684SE(2)\u91cc\u7a0b\u8ba1\u65b9\u6cd5\uff0c\u7528\u4e8e\u65cb\u8f6c\u8c03\u9891\u8fde\u7eed\u6ce2\u96f7\u8fbe\uff0c\u901a\u8fc7\u76f4\u63a5\u626b\u63cf\u5230\u5c40\u90e8\u5730\u56fe\u7684\u914d\u51c6\uff0c\u65e0\u9700\u7279\u5f81\u63d0\u53d6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5728\u6076\u52a3\u5929\u6c14\u548c\u7279\u5f81\u532e\u4e4f\u73af\u5883\u4e2d\u7684\u6027\u80fd\u3002", "motivation": "\u6beb\u7c73\u6ce2\u96f7\u8fbe\u80fd\u591f\u7a7f\u900f\u8584\u58c1\u3001\u690d\u88ab\u548c\u6076\u52a3\u5929\u6c14\u6761\u4ef6\uff08\u5982\u5927\u96e8\u3001\u96fe\u3001\u96ea\u548c\u7070\u5c18\uff09\uff0c\u5728\u79fb\u52a8\u673a\u5668\u4eba\u5e94\u7528\u4e2d\u5177\u6709\u72ec\u7279\u4f18\u52bf\u3002", "method": "\u91c7\u7528\u76f4\u63a5\u626b\u63cf\u5230\u5c40\u90e8\u5730\u56fe\u7684\u914d\u51c6\u65b9\u6cd5\uff0c\u5229\u7528\u6240\u6709\u96f7\u8fbe\u5f3a\u5ea6\u4fe1\u606f\uff0c\u65e0\u9700\u7279\u5f81\u6216\u70b9\u4e91\u63d0\u53d6\uff0c\u5e76\u5f15\u5165\u591a\u666e\u52d2\u7ea6\u675f\u4ee5\u63d0\u9ad8\u901f\u5ea6\u4f30\u8ba1\u3002", "result": "\u5728\u8d85\u8fc7250\u516c\u91cc\u7684\u9053\u8def\u6570\u636e\u4e0a\u9a8c\u8bc1\uff0c\u76f8\u5bf9\u5e73\u79fb\u8bef\u5dee\u4ec5\u4e3a0.26%\uff08\u4f7f\u7528\u9640\u87ba\u4eea\u8f85\u52a9\u65f6\uff09\uff0c\u5728\u652f\u6301\u591a\u666e\u52d2\u8c03\u5236\u7684\u573a\u666f\u4e2d\u964d\u81f30.18%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u6076\u52a3\u5929\u6c14\u548c\u7279\u5f81\u532e\u4e4f\u73af\u5883\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5b9e\u65f6\u5b9e\u73b0\u5df2\u5f00\u6e90\u3002"}}
{"id": "2504.20187", "pdf": "https://arxiv.org/pdf/2504.20187", "abs": "https://arxiv.org/abs/2504.20187", "authors": ["Weihao Sun", "Heeseung Bang", "Andreas A. Malikopoulos"], "title": "AI Recommendation Systems for Lane-Changing Using Adherence-Aware Reinforcement Learning", "categories": ["cs.LG", "cs.AI", "cs.SY", "eess.SY"], "comment": "6 pages, 5 figures, conference", "summary": "In this paper, we present an adherence-aware reinforcement learning (RL)\napproach aimed at seeking optimal lane-changing recommendations within a\nsemi-autonomous driving environment to enhance a single vehicle's travel\nefficiency. The problem is framed within a Markov decision process setting and\nis addressed through an adherence-aware deep Q network, which takes into\naccount the partial compliance of human drivers with the recommended actions.\nThis approach is evaluated within CARLA's driving environment under realistic\nscenarios.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u8f66\u9053\u53d8\u6362\u63a8\u8350\u65b9\u6cd5\uff0c\u8003\u8651\u4eba\u7c7b\u9a7e\u9a76\u5458\u7684\u90e8\u5206\u9075\u4ece\u6027\uff0c\u4ee5\u63d0\u9ad8\u534a\u81ea\u52a8\u9a7e\u9a76\u73af\u5883\u4e2d\u7684\u884c\u9a76\u6548\u7387\u3002", "motivation": "\u5728\u534a\u81ea\u52a8\u9a7e\u9a76\u73af\u5883\u4e2d\uff0c\u4eba\u7c7b\u9a7e\u9a76\u5458\u5bf9\u63a8\u8350\u52a8\u4f5c\u7684\u90e8\u5206\u9075\u4ece\u6027\u53ef\u80fd\u5f71\u54cd\u884c\u9a76\u6548\u7387\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u80fd\u591f\u9002\u5e94\u8fd9\u79cd\u884c\u4e3a\u7684\u4f18\u5316\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\u6846\u67b6\uff0c\u8bbe\u8ba1\u4e86\u4e00\u79cd\u8003\u8651\u9a7e\u9a76\u5458\u9075\u4ece\u6027\u7684\u6df1\u5ea6Q\u7f51\u7edc\uff08DQN\uff09\uff0c\u5e76\u5728CARLA\u9a7e\u9a76\u73af\u5883\u4e2d\u8fdb\u884c\u6d4b\u8bd5\u3002", "result": "\u5728CARLA\u7684\u6a21\u62df\u73af\u5883\u4e2d\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u63d0\u5347\u534a\u81ea\u52a8\u9a7e\u9a76\u73af\u5883\u4e2d\u7684\u884c\u9a76\u6548\u7387\uff0c\u540c\u65f6\u8003\u8651\u4e86\u4eba\u7c7b\u9a7e\u9a76\u5458\u7684\u884c\u4e3a\u7279\u70b9\u3002"}}
{"id": "2504.20196", "pdf": "https://arxiv.org/pdf/2504.20196", "abs": "https://arxiv.org/abs/2504.20196", "authors": ["Daye Nam", "Ahmed Omran", "Ambar Murillo", "Saksham Thakur", "Abner Araujo", "Marcel Blistein", "Alexander Fr\u00f6mmgen", "Vincent Hellendoorn", "Satish Chandra"], "title": "Prompting LLMs for Code Editing: Struggles and Remedies", "categories": ["cs.SE", "cs.AI", "cs.HC"], "comment": null, "summary": "Large Language Models (LLMs) are rapidly transforming software engineering,\nwith coding assistants embedded in an IDE becoming increasingly prevalent.\nWhile research has focused on improving the tools and understanding developer\nperceptions, a critical gap exists in understanding how developers actually use\nthese tools in their daily workflows, and, crucially, where they struggle. This\npaper addresses part of this gap through a multi-phased investigation of\ndeveloper interactions with an LLM-powered code editing and transformation\nfeature, Transform Code, in an IDE widely used at Google. First, we analyze\ntelemetry logs of the feature usage, revealing that frequent re-prompting can\nbe an indicator of developer struggles with using Transform Code. Second, we\nconduct a qualitative analysis of unsatisfactory requests, identifying five key\ncategories of information often missing from developer prompts. Finally, based\non these findings, we propose and evaluate a tool, AutoPrompter, for\nautomatically improving prompts by inferring missing information from the\nsurrounding code context, leading to a 27% improvement in edit correctness on\nour test set.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u5f00\u53d1\u8005\u5982\u4f55\u5728\u5b9e\u9645\u5de5\u4f5c\u6d41\u7a0b\u4e2d\u4f7f\u7528LLM\u9a71\u52a8\u7684\u4ee3\u7801\u7f16\u8f91\u5de5\u5177\uff0c\u53d1\u73b0\u9891\u7e41\u91cd\u65b0\u63d0\u793a\u53ef\u80fd\u8868\u660e\u4f7f\u7528\u56f0\u96be\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u52a8\u6539\u8fdb\u63d0\u793a\u7684\u5de5\u5177AutoPrompter\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u7f16\u8f91\u6b63\u786e\u6027\u3002", "motivation": "\u5c3d\u7ba1LLM\u5728\u8f6f\u4ef6\u5de5\u7a0b\u4e2d\u7684\u5e94\u7528\u65e5\u76ca\u5e7f\u6cdb\uff0c\u4f46\u5f00\u53d1\u8005\u5982\u4f55\u5b9e\u9645\u4f7f\u7528\u8fd9\u4e9b\u5de5\u5177\u53ca\u5176\u9762\u4e34\u7684\u56f0\u96be\u5c1a\u672a\u88ab\u5145\u5206\u7406\u89e3\u3002", "method": "\u901a\u8fc7\u5206\u6790IDE\u4e2dTransform Code\u529f\u80fd\u7684\u9065\u6d4b\u65e5\u5fd7\u548c\u5b9a\u6027\u5206\u6790\u4e0d\u6ee1\u610f\u7684\u8bf7\u6c42\uff0c\u8bc6\u522b\u5f00\u53d1\u8005\u63d0\u793a\u4e2d\u7f3a\u5931\u7684\u5173\u952e\u4fe1\u606f\u7c7b\u522b\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u9891\u7e41\u91cd\u65b0\u63d0\u793a\u4e0e\u4f7f\u7528\u56f0\u96be\u76f8\u5173\uff0c\u5e76\u5f00\u53d1\u4e86AutoPrompter\u5de5\u5177\uff0c\u4f7f\u7f16\u8f91\u6b63\u786e\u6027\u63d0\u9ad8\u4e8627%\u3002", "conclusion": "\u8be5\u7814\u7a76\u586b\u8865\u4e86\u5f00\u53d1\u8005\u5b9e\u9645\u4f7f\u7528LLM\u5de5\u5177\u7684\u7a7a\u767d\uff0c\u5e76\u63d0\u51fa\u4e86\u6539\u8fdb\u63d0\u793a\u7684\u65b9\u6cd5\uff0c\u4e3a\u672a\u6765\u5de5\u5177\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u53c2\u8003\u3002"}}
{"id": "2504.20403", "pdf": "https://arxiv.org/pdf/2504.20403", "abs": "https://arxiv.org/abs/2504.20403", "authors": ["Hanxi Liu", "Yifang Men", "Zhouhui Lian"], "title": "Creating Your Editable 3D Photorealistic Avatar with Tetrahedron-constrained Gaussian Splatting", "categories": ["cs.GR", "cs.CV"], "comment": null, "summary": "Personalized 3D avatar editing holds significant promise due to its\nuser-friendliness and availability to applications such as AR/VR and virtual\ntry-ons. Previous studies have explored the feasibility of 3D editing, but\noften struggle to generate visually pleasing results, possibly due to the\nunstable representation learning under mixed optimization of geometry and\ntexture in complicated reconstructed scenarios. In this paper, we aim to\nprovide an accessible solution for ordinary users to create their editable 3D\navatars with precise region localization, geometric adaptability, and\nphotorealistic renderings. To tackle this challenge, we introduce a\nmeticulously designed framework that decouples the editing process into local\nspatial adaptation and realistic appearance learning, utilizing a hybrid\nTetrahedron-constrained Gaussian Splatting (TetGS) as the underlying\nrepresentation. TetGS combines the controllable explicit structure of\ntetrahedral grids with the high-precision rendering capabilities of 3D Gaussian\nSplatting and is optimized in a progressive manner comprising three stages: 3D\navatar instantiation from real-world monocular videos to provide accurate\npriors for TetGS initialization; localized spatial adaptation with explicitly\npartitioned tetrahedrons to guide the redistribution of Gaussian kernels; and\ngeometry-based appearance generation with a coarse-to-fine activation strategy.\nBoth qualitative and quantitative experiments demonstrate the effectiveness and\nsuperiority of our approach in generating photorealistic 3D editable avatars.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eTetrahedron-constrained Gaussian Splatting (TetGS)\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u751f\u6210\u53ef\u7f16\u8f91\u76843D\u5934\u50cf\uff0c\u5177\u6709\u5c40\u90e8\u7a7a\u95f4\u9002\u5e94\u6027\u548c\u771f\u5b9e\u611f\u6e32\u67d3\u80fd\u529b\u3002", "motivation": "\u4e2a\u6027\u53163D\u5934\u50cf\u7f16\u8f91\u5728AR/VR\u548c\u865a\u62df\u8bd5\u7a7f\u7b49\u5e94\u7528\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5728\u590d\u6742\u91cd\u5efa\u573a\u666f\u4e2d\u96be\u4ee5\u751f\u6210\u89c6\u89c9\u4e0a\u4ee4\u4eba\u6109\u60a6\u7684\u7ed3\u679c\u3002", "method": "\u91c7\u7528TetGS\u4f5c\u4e3a\u5e95\u5c42\u8868\u793a\uff0c\u5c06\u7f16\u8f91\u8fc7\u7a0b\u5206\u89e3\u4e3a\u5c40\u90e8\u7a7a\u95f4\u9002\u5e94\u548c\u771f\u5b9e\u611f\u5916\u89c2\u5b66\u4e60\uff0c\u5e76\u901a\u8fc7\u4e09\u9636\u6bb5\u4f18\u5316\u5b9e\u73b0\uff1a3D\u5934\u50cf\u5b9e\u4f8b\u5316\u3001\u5c40\u90e8\u7a7a\u95f4\u9002\u5e94\u548c\u51e0\u4f55\u5916\u89c2\u751f\u6210\u3002", "result": "\u5b9a\u6027\u548c\u5b9a\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u751f\u6210\u771f\u5b9e\u611f3D\u53ef\u7f16\u8f91\u5934\u50cf\u65b9\u9762\u5177\u6709\u4f18\u8d8a\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u666e\u901a\u7528\u6237\u63d0\u4f9b\u4e86\u4e00\u79cd\u751f\u6210\u9ad8\u8d28\u91cf\u53ef\u7f16\u8f913D\u5934\u50cf\u7684\u53ef\u884c\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2504.20197", "pdf": "https://arxiv.org/pdf/2504.20197", "abs": "https://arxiv.org/abs/2504.20197", "authors": ["Aryeh Brill"], "title": "Representation Learning on a Random Lattice", "categories": ["cs.LG", "cond-mat.dis-nn", "cs.AI"], "comment": "Published in Proceedings of ILIAD (2024),\n  https://www.iliadconference.com/proceedings", "summary": "Decomposing a deep neural network's learned representations into\ninterpretable features could greatly enhance its safety and reliability. To\nbetter understand features, we adopt a geometric perspective, viewing them as a\nlearned coordinate system for mapping an embedded data distribution. We\nmotivate a model of a generic data distribution as a random lattice and analyze\nits properties using percolation theory. Learned features are categorized into\ncontext, component, and surface features. The model is qualitatively consistent\nwith recent findings in mechanistic interpretability and suggests directions\nfor future research.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u51e0\u4f55\u89c6\u89d2\uff0c\u5c06\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u7684\u8868\u793a\u5206\u89e3\u4e3a\u53ef\u89e3\u91ca\u7279\u5f81\uff0c\u4ee5\u63d0\u5347\u5176\u5b89\u5168\u6027\u548c\u53ef\u9760\u6027\u3002", "motivation": "\u901a\u8fc7\u7406\u89e3\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u7684\u7279\u5f81\u8868\u793a\uff0c\u63d0\u5347\u6a21\u578b\u7684\u5b89\u5168\u6027\u548c\u53ef\u9760\u6027\u3002", "method": "\u91c7\u7528\u51e0\u4f55\u89c6\u89d2\uff0c\u5c06\u7279\u5f81\u89c6\u4e3a\u5d4c\u5165\u6570\u636e\u5206\u5e03\u7684\u5750\u6807\u7cfb\uff0c\u5e76\u57fa\u4e8e\u968f\u673a\u683c\u70b9\u6a21\u578b\u5206\u6790\u5176\u6027\u8d28\u3002", "result": "\u7279\u5f81\u88ab\u5206\u7c7b\u4e3a\u4e0a\u4e0b\u6587\u3001\u7ec4\u4ef6\u548c\u8868\u9762\u7279\u5f81\uff0c\u4e0e\u73b0\u6709\u673a\u5236\u89e3\u91ca\u6027\u7814\u7a76\u4e00\u81f4\u3002", "conclusion": "\u6a21\u578b\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u65b9\u5411\uff0c\u652f\u6301\u5bf9\u795e\u7ecf\u7f51\u7edc\u7279\u5f81\u7684\u8fdb\u4e00\u6b65\u7406\u89e3\u3002"}}
{"id": "2504.20405", "pdf": "https://arxiv.org/pdf/2504.20405", "abs": "https://arxiv.org/abs/2504.20405", "authors": ["Sahil Sethi", "Sai Reddy", "Mansi Sakarvadia", "Jordan Serotte", "Darlington Nwaudo", "Nicholas Maassen", "Lewis Shi"], "title": "SCOPE-MRI: Bankart Lesion Detection as a Case Study in Data Curation and Deep Learning for Challenging Diagnoses", "categories": ["eess.IV", "cs.AI", "cs.CV", "cs.LG"], "comment": null, "summary": "While deep learning has shown strong performance in musculoskeletal imaging,\nexisting work has largely focused on pathologies where diagnosis is not a\nclinical challenge, leaving more difficult problems underexplored, such as\ndetecting Bankart lesions (anterior-inferior glenoid labral tears) on standard\nMRIs. Diagnosing these lesions is challenging due to their subtle imaging\nfeatures, often leading to reliance on invasive MRI arthrograms (MRAs). This\nstudy introduces ScopeMRI, the first publicly available, expert-annotated\ndataset for shoulder pathologies, and presents a deep learning (DL) framework\nfor detecting Bankart lesions on both standard MRIs and MRAs. ScopeMRI includes\n586 shoulder MRIs (335 standard, 251 MRAs) from 558 patients who underwent\narthroscopy. Ground truth labels were derived from intraoperative findings, the\ngold standard for diagnosis. Separate DL models for MRAs and standard MRIs were\ntrained using a combination of CNNs and transformers. Predictions from\nsagittal, axial, and coronal views were ensembled to optimize performance. The\nmodels were evaluated on a 20% hold-out test set (117 MRIs: 46 MRAs, 71\nstandard MRIs). The models achieved an AUC of 0.91 and 0.93, sensitivity of 83%\nand 94%, and specificity of 91% and 86% for standard MRIs and MRAs,\nrespectively. Notably, model performance on non-invasive standard MRIs matched\nor surpassed radiologists interpreting MRAs. External validation demonstrated\ninitial generalizability across imaging protocols. This study demonstrates that\nDL models can achieve radiologist-level diagnostic performance on standard\nMRIs, reducing the need for invasive MRAs. By releasing ScopeMRI and a modular\ncodebase for training and evaluating deep learning models on 3D medical imaging\ndata, we aim to accelerate research in musculoskeletal imaging and support the\ndevelopment of new datasets for clinically challenging diagnostic tasks.", "AI": {"tldr": "ScopeMRI\u662f\u9996\u4e2a\u516c\u5f00\u7684\u80a9\u90e8\u75c5\u7406\u6570\u636e\u96c6\uff0c\u7ed3\u5408\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u68c0\u6d4bBankart\u75c5\u53d8\uff0c\u6027\u80fd\u8fbe\u5230\u6216\u8d85\u8fc7\u653e\u5c04\u79d1\u533b\u751f\u6c34\u5e73\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u591a\u5173\u6ce8\u6613\u4e8e\u8bca\u65ad\u7684\u75c5\u7406\uff0c\u800cBankart\u75c5\u53d8\u56e0\u5f71\u50cf\u7279\u5f81\u7ec6\u5fae\u4e14\u4f9d\u8d56\u4fb5\u5165\u6027\u68c0\u67e5\uff08MRA\uff09\u800c\u672a\u88ab\u5145\u5206\u7814\u7a76\u3002", "method": "\u4f7f\u7528CNN\u548cTransformer\u7ed3\u5408\u7684\u65b9\u6cd5\uff0c\u8bad\u7ec3\u5206\u522b\u9488\u5bf9\u6807\u51c6MRI\u548cMRA\u7684\u6a21\u578b\uff0c\u5e76\u901a\u8fc7\u591a\u89c6\u89d2\u96c6\u6210\u4f18\u5316\u6027\u80fd\u3002", "result": "\u6a21\u578b\u5728\u6807\u51c6MRI\u548cMRA\u4e0a\u7684AUC\u5206\u522b\u4e3a0.91\u548c0.93\uff0c\u654f\u611f\u6027\u548c\u7279\u5f02\u6027\u8868\u73b0\u4f18\u5f02\uff0c\u6027\u80fd\u5ab2\u7f8e\u653e\u5c04\u79d1\u533b\u751f\u3002", "conclusion": "\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u53ef\u5728\u6807\u51c6MRI\u4e0a\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u8bca\u65ad\uff0c\u51cf\u5c11\u5bf9\u4fb5\u5165\u6027MRA\u7684\u4f9d\u8d56\uff0cScopeMRI\u7684\u53d1\u5e03\u5c06\u63a8\u52a8\u76f8\u5173\u7814\u7a76\u3002"}}
{"id": "2504.20454", "pdf": "https://arxiv.org/pdf/2504.20454", "abs": "https://arxiv.org/abs/2504.20454", "authors": ["Jiajun Ding", "Beiyao Zhu", "Xiaosheng Liu", "Lishen Zhang", "Zhao Liu"], "title": "LymphAtlas- A Unified Multimodal Lymphoma Imaging Repository Delivering AI-Enhanced Diagnostic Insight", "categories": ["eess.IV", "cs.CV"], "comment": "17pages,4 figures", "summary": "This study integrates PET metabolic information with CT anatomical structures\nto establish a 3D multimodal segmentation dataset for lymphoma based on\nwhole-body FDG PET/CT examinations, which bridges the gap of the lack of\nstandardised multimodal segmentation datasets in the field of haematological\nmalignancies. We retrospectively collected 483 examination datasets acquired\nbetween March 2011 and May 2024, involving 220 patients (106 non-Hodgkin\nlymphoma, 42 Hodgkin lymphoma); all data underwent ethical review and were\nrigorously de-identified. Complete 3D structural information was preserved\nduring data acquisition, preprocessing and annotation, and a high-quality\ndataset was constructed based on the nnUNet format. By systematic technical\nvalidation and evaluation of the preprocessing process, annotation quality and\nautomatic segmentation algorithm, the deep learning model trained based on this\ndataset is verified to achieve accurate segmentation of lymphoma lesions in\nPET/CT images with high accuracy, good robustness and reproducibility, which\nproves the applicability and stability of this dataset in accurate segmentation\nand quantitative analysis. The deep fusion of PET/CT images achieved with this\ndataset not only significantly improves the accurate portrayal of the\nmorphology, location and metabolic features of tumour lesions, but also\nprovides solid data support for early diagnosis, clinical staging and\npersonalized treatment, and promotes the development of automated image\nsegmentation and precision medicine based on deep learning. The dataset and\nrelated resources are available at https://github.com/SuperD0122/LymphAtlas-.", "AI": {"tldr": "\u8be5\u7814\u7a76\u6574\u5408PET\u4ee3\u8c22\u4fe1\u606f\u4e0eCT\u89e3\u5256\u7ed3\u6784\uff0c\u6784\u5efa\u4e86\u57fa\u4e8e\u5168\u8eabFDG PET/CT\u68c0\u67e5\u7684\u6dcb\u5df4\u76243D\u591a\u6a21\u6001\u5206\u5272\u6570\u636e\u96c6\uff0c\u586b\u8865\u4e86\u8840\u6db2\u6076\u6027\u80bf\u7624\u9886\u57df\u6807\u51c6\u5316\u591a\u6a21\u6001\u5206\u5272\u6570\u636e\u96c6\u7684\u7a7a\u767d\u3002", "motivation": "\u89e3\u51b3\u8840\u6db2\u6076\u6027\u80bf\u7624\u9886\u57df\u7f3a\u4e4f\u6807\u51c6\u5316\u591a\u6a21\u6001\u5206\u5272\u6570\u636e\u96c6\u7684\u95ee\u9898\uff0c\u652f\u6301\u6dcb\u5df4\u7624\u7684\u7cbe\u786e\u5206\u5272\u4e0e\u5b9a\u91cf\u5206\u6790\u3002", "method": "\u56de\u987e\u6027\u6536\u96c6483\u4f8b\u68c0\u67e5\u6570\u636e\uff08220\u540d\u60a3\u8005\uff09\uff0c\u4fdd\u7559\u5b8c\u65743D\u7ed3\u6784\u4fe1\u606f\uff0c\u57fa\u4e8ennUNet\u683c\u5f0f\u6784\u5efa\u9ad8\u8d28\u91cf\u6570\u636e\u96c6\uff0c\u5e76\u901a\u8fc7\u6280\u672f\u9a8c\u8bc1\u548c\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u8bc4\u4f30\u3002", "result": "\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u8be5\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u9ad8\u7cbe\u5ea6\u3001\u5f3a\u9c81\u68d2\u6027\u548c\u53ef\u91cd\u590d\u6027\u7684\u6dcb\u5df4\u7624\u75c5\u7076\u5206\u5272\uff0c\u8bc1\u660e\u4e86\u6570\u636e\u96c6\u7684\u9002\u7528\u6027\u548c\u7a33\u5b9a\u6027\u3002", "conclusion": "\u8be5\u6570\u636e\u96c6\u663e\u8457\u63d0\u5347\u4e86\u80bf\u7624\u75c5\u7076\u5f62\u6001\u3001\u4f4d\u7f6e\u53ca\u4ee3\u8c22\u7279\u5f81\u7684\u7cbe\u786e\u63cf\u7ed8\uff0c\u4e3a\u65e9\u671f\u8bca\u65ad\u3001\u4e34\u5e8a\u5206\u671f\u548c\u4e2a\u6027\u5316\u6cbb\u7597\u63d0\u4f9b\u4e86\u6570\u636e\u652f\u6301\uff0c\u63a8\u52a8\u4e86\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u81ea\u52a8\u5316\u56fe\u50cf\u5206\u5272\u548c\u7cbe\u51c6\u533b\u5b66\u53d1\u5c55\u3002"}}
{"id": "2504.20213", "pdf": "https://arxiv.org/pdf/2504.20213", "abs": "https://arxiv.org/abs/2504.20213", "authors": ["Yuan Xia", "Akanksha Atrey", "Fadoua Khmaissia", "Kedar S. Namjoshi"], "title": "Can Large Language Models Learn Formal Logic? A Data-Driven Training and Evaluation Framework", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "This paper investigates the logical reasoning capabilities of large language\nmodels (LLMs). For a precisely defined yet tractable formulation, we choose the\nconceptually simple but technically complex task of constructing proofs in\nBoolean logic. A trained LLM receives as input a set of assumptions and a goal,\nand produces as output a proof that formally derives the goal from the\nassumptions. Incorrect proofs are caught by an automated proof checker. A\ncritical obstacle for training is the scarcity of real-world proofs. We propose\nan efficient, randomized procedure for synthesizing valid proofs and introduce\nTemplate Transformation, a data augmentation technique that enhances the\nmodel's ability to handle complex logical expressions. The central evaluation\nquestion is whether an LLM has indeed learned to reason. We propose tests to\nmeasure the reasoning ability of a black-box LLM. By these measures,\nexperiments demonstrate strong reasoning capabilities for assertions with short\nproofs, which decline with proof complexity. Notably, template transformation\nimproves accuracy even for smaller models, suggesting its effectiveness across\nmodel scales.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u903b\u8f91\u63a8\u7406\u80fd\u529b\uff0c\u901a\u8fc7\u5e03\u5c14\u903b\u8f91\u4e2d\u7684\u8bc1\u660e\u6784\u5efa\u4efb\u52a1\u8bc4\u4f30\u5176\u8868\u73b0\u3002\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u6570\u636e\u589e\u5f3a\u65b9\u6cd5\uff08\u6a21\u677f\u53d8\u6362\uff09\uff0c\u5b9e\u9a8c\u663e\u793a\u6a21\u578b\u5728\u77ed\u8bc1\u660e\u4efb\u52a1\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u4f46\u968f\u7740\u8bc1\u660e\u590d\u6742\u5ea6\u589e\u52a0\uff0c\u80fd\u529b\u4e0b\u964d\u3002", "motivation": "\u63a2\u7d22LLMs\u662f\u5426\u5177\u5907\u771f\u6b63\u7684\u903b\u8f91\u63a8\u7406\u80fd\u529b\uff0c\u800c\u975e\u4ec5\u4f9d\u8d56\u6a21\u5f0f\u5339\u914d\u3002", "method": "\u4f7f\u7528\u5e03\u5c14\u903b\u8f91\u8bc1\u660e\u6784\u5efa\u4efb\u52a1\uff0c\u7ed3\u5408\u81ea\u52a8\u8bc1\u660e\u68c0\u67e5\u5668\u9a8c\u8bc1\u6b63\u786e\u6027\u3002\u63d0\u51fa\u6a21\u677f\u53d8\u6362\u6280\u672f\u589e\u5f3a\u6570\u636e\u591a\u6837\u6027\u3002", "result": "\u6a21\u578b\u5728\u77ed\u8bc1\u660e\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u79c0\uff0c\u4f46\u968f\u7740\u8bc1\u660e\u590d\u6742\u5ea6\u589e\u52a0\uff0c\u80fd\u529b\u4e0b\u964d\u3002\u6a21\u677f\u53d8\u6362\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\u3002", "conclusion": "LLMs\u5728\u7b80\u5355\u903b\u8f91\u63a8\u7406\u4efb\u52a1\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u4f46\u590d\u6742\u63a8\u7406\u4ecd\u9700\u6539\u8fdb\u3002\u6a21\u677f\u53d8\u6362\u662f\u4e00\u79cd\u6709\u6548\u7684\u6570\u636e\u589e\u5f3a\u65b9\u6cd5\u3002"}}
{"id": "2504.20501", "pdf": "https://arxiv.org/pdf/2504.20501", "abs": "https://arxiv.org/abs/2504.20501", "authors": ["Jia Wang", "Yunan Mei", "Jiarui Liu", "Xin Fan"], "title": "SAM-Guided Robust Representation Learning for One-Shot 3D Medical Image Segmentation", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "One-shot medical image segmentation (MIS) is crucial for medical analysis due\nto the burden of medical experts on manual annotation. The recent emergence of\nthe segment anything model (SAM) has demonstrated remarkable adaptation in MIS\nbut cannot be directly applied to one-shot medical image segmentation (MIS) due\nto its reliance on labor-intensive user interactions and the high computational\ncost. To cope with these limitations, we propose a novel SAM-guided robust\nrepresentation learning framework, named RRL-MedSAM, to adapt SAM to one-shot\n3D MIS, which exploits the strong generalization capabilities of the SAM\nencoder to learn better feature representation. We devise a dual-stage\nknowledge distillation (DSKD) strategy to distill general knowledge between\nnatural and medical images from the foundation model to train a lightweight\nencoder, and then adopt a mutual exponential moving average (mutual-EMA) to\nupdate the weights of the general lightweight encoder and medical-specific\nencoder. Specifically, pseudo labels from the registration network are used to\nperform mutual supervision for such two encoders. Moreover, we introduce an\nauto-prompting (AP) segmentation decoder which adopts the mask generated from\nthe general lightweight model as a prompt to assist the medical-specific model\nin boosting the final segmentation performance. Extensive experiments conducted\non three public datasets, i.e., OASIS, CT-lung demonstrate that the proposed\nRRL-MedSAM outperforms state-of-the-art one-shot MIS methods for both\nsegmentation and registration tasks. Especially, our lightweight encoder uses\nonly 3\\% of the parameters compared to the encoder of SAM-Base.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aRRL-MedSAM\u7684\u65b0\u578b\u6846\u67b6\uff0c\u901a\u8fc7\u77e5\u8bc6\u84b8\u998f\u548c\u4e92\u66f4\u65b0\u7b56\u7565\uff0c\u5c06SAM\u9002\u914d\u5230\u4e00\u6b21\u60273D\u533b\u5b66\u56fe\u50cf\u5206\u5272\u4efb\u52a1\u4e2d\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u8ba1\u7b97\u6210\u672c\u5e76\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3SAM\u5728\u4e00\u6b21\u6027\u533b\u5b66\u56fe\u50cf\u5206\u5272\u4e2d\u4f9d\u8d56\u4eba\u5de5\u4ea4\u4e92\u548c\u9ad8\u8ba1\u7b97\u6210\u672c\u7684\u95ee\u9898\u3002", "method": "\u91c7\u7528\u53cc\u9636\u6bb5\u77e5\u8bc6\u84b8\u998f\u7b56\u7565\u548c\u4e92\u6307\u6570\u79fb\u52a8\u5e73\u5747\u66f4\u65b0\u6743\u91cd\uff0c\u7ed3\u5408\u81ea\u52a8\u63d0\u793a\u5206\u5272\u89e3\u7801\u5668\u63d0\u5347\u6027\u80fd\u3002", "result": "\u5728OASIS\u548cCT-lung\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u8f7b\u91cf\u7ea7\u7f16\u7801\u5668\u53c2\u6570\u4ec5\u4e3aSAM-Base\u76843%\u3002", "conclusion": "RRL-MedSAM\u5728\u4e00\u6b21\u6027\u533b\u5b66\u56fe\u50cf\u5206\u5272\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u540c\u65f6\u663e\u8457\u964d\u4f4e\u4e86\u8ba1\u7b97\u8d44\u6e90\u9700\u6c42\u3002"}}
{"id": "2504.20251", "pdf": "https://arxiv.org/pdf/2504.20251", "abs": "https://arxiv.org/abs/2504.20251", "authors": ["Aiala Ros\u00e1", "Santiago G\u00f3ngora", "Juan Pablo Filevich", "Ignacio Sastre", "Laura Musto", "Brian Carpenter", "Luis Chiruzzo"], "title": "A Platform for Generating Educational Activities to Teach English as a Second Language", "categories": ["cs.CL", "cs.AI", "cs.CY"], "comment": "Unpublished report written in 2023", "summary": "We present a platform for the generation of educational activities oriented\nto teaching English as a foreign language. The different activities -- games\nand language practice exercises -- are strongly based on Natural Language\nProcessing techniques. The platform offers the possibility of playing\nout-of-the-box games, generated from resources created semi-automatically and\nthen manually curated. It can also generate games or exercises of greater\ncomplexity from texts entered by teachers, providing a stage of review and\nedition of the generated content before use. As a way of expanding the variety\nof activities in the platform, we are currently experimenting with image and\ntext generation. In order to integrate them and improve the performance of\nother neural tools already integrated, we are working on migrating the platform\nto a more powerful server. In this paper we describe the development of our\nplatform and its deployment for end users, discussing the challenges faced and\nhow we overcame them, and also detail our future work plans.", "AI": {"tldr": "\u4ecb\u7ecd\u4e86\u4e00\u4e2a\u57fa\u4e8e\u81ea\u7136\u8bed\u8a00\u5904\u7406\u6280\u672f\u7684\u82f1\u8bed\u6559\u80b2\u5e73\u53f0\uff0c\u652f\u6301\u751f\u6210\u548c\u5b9a\u5236\u6e38\u620f\u4e0e\u7ec3\u4e60\uff0c\u5e76\u8ba1\u5212\u6269\u5c55\u529f\u80fd\u548c\u8fc1\u79fb\u670d\u52a1\u5668\u3002", "motivation": "\u4e3a\u82f1\u8bed\u4f5c\u4e3a\u5916\u8bed\u7684\u6559\u5b66\u63d0\u4f9b\u591a\u6837\u5316\u7684\u6559\u80b2\u6d3b\u52a8\uff0c\u7ed3\u5408\u81ea\u7136\u8bed\u8a00\u5904\u7406\u6280\u672f\u63d0\u5347\u6559\u5b66\u6548\u679c\u3002", "method": "\u5e73\u53f0\u5229\u7528\u534a\u81ea\u52a8\u751f\u6210\u548c\u4eba\u5de5\u5ba1\u6838\u7684\u8d44\u6e90\u63d0\u4f9b\u5373\u7528\u6e38\u620f\uff0c\u5e76\u652f\u6301\u6559\u5e08\u8f93\u5165\u6587\u672c\u751f\u6210\u590d\u6742\u5185\u5bb9\uff0c\u540c\u65f6\u63a2\u7d22\u56fe\u50cf\u548c\u6587\u672c\u751f\u6210\u6280\u672f\u3002", "result": "\u5e73\u53f0\u5df2\u90e8\u7f72\u5e76\u4f9b\u7ec8\u7aef\u7528\u6237\u4f7f\u7528\uff0c\u89e3\u51b3\u4e86\u5f00\u53d1\u4e2d\u7684\u6311\u6218\uff0c\u672a\u6765\u8ba1\u5212\u8fc1\u79fb\u81f3\u66f4\u5f3a\u5927\u7684\u670d\u52a1\u5668\u3002", "conclusion": "\u5e73\u53f0\u5c55\u793a\u4e86\u7ed3\u5408\u81ea\u7136\u8bed\u8a00\u5904\u7406\u6280\u672f\u7684\u6559\u80b2\u5de5\u5177\u6f5c\u529b\uff0c\u672a\u6765\u5c06\u8fdb\u4e00\u6b65\u6269\u5c55\u529f\u80fd\u548c\u4f18\u5316\u6027\u80fd\u3002"}}
{"id": "2504.20584", "pdf": "https://arxiv.org/pdf/2504.20584", "abs": "https://arxiv.org/abs/2504.20584", "authors": ["Martin Huber", "Huanyu Tian", "Christopher E. Mower", "Lucas-Raphael M\u00fcller", "S\u00e9bastien Ourselin", "Christos Bergeles", "Tom Vercauteren"], "title": "Hydra: Marker-Free RGB-D Hand-Eye Calibration", "categories": ["cs.RO", "cs.CV"], "comment": null, "summary": "This work presents an RGB-D imaging-based approach to marker-free hand-eye\ncalibration using a novel implementation of the iterative closest point (ICP)\nalgorithm with a robust point-to-plane (PTP) objective formulated on a Lie\nalgebra. Its applicability is demonstrated through comprehensive experiments\nusing three well known serial manipulators and two RGB-D cameras. With only\nthree randomly chosen robot configurations, our approach achieves approximately\n90% successful calibrations, demonstrating 2-3x higher convergence rates to the\nglobal optimum compared to both marker-based and marker-free baselines. We also\nreport 2 orders of magnitude faster convergence time (0.8 +/- 0.4 s) for 9\nrobot configurations over other marker-free methods. Our method exhibits\nsignificantly improved accuracy (5 mm in task space) over classical approaches\n(7 mm in task space) whilst being marker-free. The benchmarking dataset and\ncode are open sourced under Apache 2.0 License, and a ROS 2 integration with\nrobot abstraction is provided to facilitate deployment.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8eRGB-D\u6210\u50cf\u7684\u65e0\u6807\u8bb0\u624b\u773c\u6807\u5b9a\u65b9\u6cd5\uff0c\u91c7\u7528\u6539\u8fdb\u7684ICP\u7b97\u6cd5\u548c\u9c81\u68d2\u70b9\u5bf9\u9762\u76ee\u6807\u51fd\u6570\uff0c\u5728\u5b9e\u9a8c\u4e2d\u8868\u73b0\u51fa\u66f4\u9ad8\u7684\u6536\u655b\u901f\u5ea6\u548c\u7cbe\u5ea6\u3002", "motivation": "\u4f20\u7edf\u624b\u773c\u6807\u5b9a\u65b9\u6cd5\u4f9d\u8d56\u6807\u8bb0\u7269\uff0c\u9650\u5236\u4e86\u5e94\u7528\u573a\u666f\u3002\u672c\u6587\u65e8\u5728\u5f00\u53d1\u4e00\u79cd\u65e0\u6807\u8bb0\u3001\u9ad8\u6548\u4e14\u9ad8\u7cbe\u5ea6\u7684\u6807\u5b9a\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528\u6539\u8fdb\u7684ICP\u7b97\u6cd5\uff0c\u7ed3\u5408\u9c81\u68d2\u70b9\u5bf9\u9762\u76ee\u6807\u51fd\u6570\u548cLie\u4ee3\u6570\uff0c\u901a\u8fc7RGB-D\u76f8\u673a\u548c\u673a\u5668\u4eba\u914d\u7f6e\u5b9e\u73b0\u6807\u5b9a\u3002", "result": "\u4ec5\u97003\u4e2a\u968f\u673a\u673a\u5668\u4eba\u914d\u7f6e\u5373\u53ef\u5b9e\u73b0\u7ea690%\u7684\u6210\u529f\u7387\uff0c\u6536\u655b\u901f\u5ea6\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u5feb2-3\u500d\uff0c\u7cbe\u5ea6\u8fbe5 mm\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u65e0\u6807\u8bb0\u6761\u4ef6\u4e0b\u663e\u8457\u63d0\u5347\u4e86\u6807\u5b9a\u7684\u6548\u7387\u548c\u7cbe\u5ea6\uff0c\u9002\u7528\u4e8e\u5b9e\u9645\u90e8\u7f72\u3002"}}
{"id": "2504.20275", "pdf": "https://arxiv.org/pdf/2504.20275", "abs": "https://arxiv.org/abs/2504.20275", "authors": ["Mohammadhossein Homaei", "Victor Gonzalez Morales", "Oscar Mogollon Gutierrez", "Ruben Molano Gomez", "Andres Caro"], "title": "Smart Water Security with AI and Blockchain-Enhanced Digital Twins", "categories": ["cs.CR", "cs.AI", "cs.LG"], "comment": "8 Pages, 9 Figures", "summary": "Water distribution systems in rural areas face serious challenges such as a\nlack of real-time monitoring, vulnerability to cyberattacks, and unreliable\ndata handling. This paper presents an integrated framework that combines\nLoRaWAN-based data acquisition, a machine learning-driven Intrusion Detection\nSystem (IDS), and a blockchain-enabled Digital Twin (BC-DT) platform for secure\nand transparent water management. The IDS filters anomalous or spoofed data\nusing a Long Short-Term Memory (LSTM) Autoencoder and Isolation Forest before\nvalidated data is logged via smart contracts on a private Ethereum blockchain\nusing Proof of Authority (PoA) consensus. The verified data feeds into a\nreal-time DT model supporting leak detection, consumption forecasting, and\npredictive maintenance. Experimental results demonstrate that the system\nachieves over 80 transactions per second (TPS) with under 2 seconds of latency\nwhile remaining cost-effective and scalable for up to 1,000 smart meters. This\nwork demonstrates a practical and secure architecture for decentralized water\ninfrastructure in under-connected rural environments.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408LoRaWAN\u3001\u673a\u5668\u5b66\u4e60\u9a71\u52a8\u7684\u5165\u4fb5\u68c0\u6d4b\u7cfb\u7edf\uff08IDS\uff09\u548c\u533a\u5757\u94fe\u6570\u5b57\u5b6a\u751f\uff08BC-DT\uff09\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u519c\u6751\u6c34\u7cfb\u7edf\u7684\u5b89\u5168\u900f\u660e\u7ba1\u7406\u3002", "motivation": "\u519c\u6751\u6c34\u7cfb\u7edf\u9762\u4e34\u5b9e\u65f6\u76d1\u6d4b\u4e0d\u8db3\u3001\u6613\u53d7\u7f51\u7edc\u653b\u51fb\u548c\u6570\u636e\u4e0d\u53ef\u9760\u7b49\u95ee\u9898\uff0c\u4e9f\u9700\u4e00\u79cd\u5b89\u5168\u4e14\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u91c7\u7528LoRaWAN\u91c7\u96c6\u6570\u636e\uff0c\u7ed3\u5408LSTM\u81ea\u7f16\u7801\u5668\u548c\u9694\u79bb\u68ee\u6797\u7684IDS\u8fc7\u6ee4\u5f02\u5e38\u6570\u636e\uff0c\u901a\u8fc7\u79c1\u6709\u4ee5\u592a\u574a\u533a\u5757\u94fe\uff08PoA\u5171\u8bc6\uff09\u8bb0\u5f55\u9a8c\u8bc1\u6570\u636e\uff0c\u5e76\u6784\u5efa\u5b9e\u65f6\u6570\u5b57\u5b6a\u751f\u6a21\u578b\u3002", "result": "\u7cfb\u7edf\u5b9e\u73b0\u6bcf\u79d280+\u4ea4\u6613\uff08TPS\uff09\uff0c\u5ef6\u8fdf\u4f4e\u4e8e2\u79d2\uff0c\u652f\u63011000\u4e2a\u667a\u80fd\u6c34\u8868\uff0c\u5177\u6709\u6210\u672c\u6548\u76ca\u548c\u53ef\u6269\u5c55\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u519c\u6751\u6c34\u57fa\u7840\u8bbe\u65bd\u63d0\u4f9b\u4e86\u4e00\u79cd\u5b9e\u7528\u4e14\u5b89\u5168\u7684\u53bb\u4e2d\u5fc3\u5316\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2504.20658", "pdf": "https://arxiv.org/pdf/2504.20658", "abs": "https://arxiv.org/abs/2504.20658", "authors": ["Stefano Dell'Anna", "Andrea Montibeller", "Giulia Boato"], "title": "TrueFake: A Real World Case Dataset of Last Generation Fake Images also Shared on Social Networks", "categories": ["cs.MM", "cs.AI", "cs.CV"], "comment": null, "summary": "AI-generated synthetic media are increasingly used in real-world scenarios,\noften with the purpose of spreading misinformation and propaganda through\nsocial media platforms, where compression and other processing can degrade fake\ndetection cues. Currently, many forensic tools fail to account for these\nin-the-wild challenges. In this work, we introduce TrueFake, a large-scale\nbenchmarking dataset of 600,000 images including top notch generative\ntechniques and sharing via three different social networks. This dataset allows\nfor rigorous evaluation of state-of-the-art fake image detectors under very\nrealistic and challenging conditions. Through extensive experimentation, we\nanalyze how social media sharing impacts detection performance, and identify\ncurrent most effective detection and training strategies. Our findings\nhighlight the need for evaluating forensic models in conditions that mirror\nreal-world use.", "AI": {"tldr": "TrueFake\u662f\u4e00\u4e2a\u5305\u542b60\u4e07\u5f20\u56fe\u50cf\u7684\u5927\u89c4\u6a21\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u793e\u4ea4\u5a92\u4f53\u73af\u5883\u4e0bAI\u751f\u6210\u56fe\u50cf\u7684\u68c0\u6d4b\u5de5\u5177\uff0c\u63ed\u793a\u4e86\u5f53\u524d\u68c0\u6d4b\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002", "motivation": "AI\u751f\u6210\u7684\u5408\u6210\u5a92\u4f53\u5728\u793e\u4ea4\u5a92\u4f53\u4e2d\u5e7f\u6cdb\u4f20\u64ad\uff0c\u4f46\u73b0\u6709\u68c0\u6d4b\u5de5\u5177\u672a\u80fd\u5145\u5206\u5e94\u5bf9\u538b\u7f29\u548c\u5904\u7406\u5e26\u6765\u7684\u6311\u6218\u3002", "method": "\u6784\u5efaTrueFake\u6570\u636e\u96c6\uff0c\u5305\u542b\u591a\u79cd\u751f\u6210\u6280\u672f\u548c\u793e\u4ea4\u5a92\u4f53\u5171\u4eab\u7684\u56fe\u50cf\uff0c\u901a\u8fc7\u5b9e\u9a8c\u5206\u6790\u68c0\u6d4b\u6027\u80fd\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u793e\u4ea4\u5a92\u4f53\u5171\u4eab\u4f1a\u663e\u8457\u5f71\u54cd\u68c0\u6d4b\u6027\u80fd\uff0c\u5e76\u63d0\u51fa\u4e86\u5f53\u524d\u6700\u6709\u6548\u7684\u68c0\u6d4b\u548c\u8bad\u7ec3\u7b56\u7565\u3002", "conclusion": "\u5f3a\u8c03\u9700\u8981\u5728\u771f\u5b9e\u573a\u666f\u4e0b\u8bc4\u4f30\u68c0\u6d4b\u6a21\u578b\uff0c\u4ee5\u63d0\u5347\u5176\u5b9e\u9645\u5e94\u7528\u6548\u679c\u3002"}}
{"id": "2504.20295", "pdf": "https://arxiv.org/pdf/2504.20295", "abs": "https://arxiv.org/abs/2504.20295", "authors": ["Mohammadhossein Homaei", "Victor Gonzalez Morales", "Oscar Mogollon-Gutierrez", "Andres Caro"], "title": "The Dark Side of Digital Twins: Adversarial Attacks on AI-Driven Water Forecasting", "categories": ["cs.LG", "cs.AI", "cs.CR"], "comment": "7 Pages, 7 Figures", "summary": "Digital twins (DTs) are improving water distribution systems by using\nreal-time data, analytics, and prediction models to optimize operations. This\npaper presents a DT platform designed for a Spanish water supply network,\nutilizing Long Short-Term Memory (LSTM) networks to predict water consumption.\nHowever, machine learning models are vulnerable to adversarial attacks, such as\nthe Fast Gradient Sign Method (FGSM) and Projected Gradient Descent (PGD).\nThese attacks manipulate critical model parameters, injecting subtle\ndistortions that degrade forecasting accuracy. To further exploit these\nvulnerabilities, we introduce a Learning Automata (LA) and Random LA-based\napproach that dynamically adjusts perturbations, making adversarial attacks\nmore difficult to detect. Experimental results show that this approach\nsignificantly impacts prediction reliability, causing the Mean Absolute\nPercentage Error (MAPE) to rise from 26% to over 35%. Moreover, adaptive attack\nstrategies amplify this effect, highlighting cybersecurity risks in AI-driven\nDTs. These findings emphasize the urgent need for robust defenses, including\nadversarial training, anomaly detection, and secure data pipelines.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u897f\u73ed\u7259\u4f9b\u6c34\u7f51\u7edc\u7684\u6570\u5b57\u5b6a\u751f\u5e73\u53f0\uff0c\u5229\u7528LSTM\u9884\u6d4b\u7528\u6c34\u91cf\uff0c\u4f46\u673a\u5668\u5b66\u4e60\u6a21\u578b\u6613\u53d7\u5bf9\u6297\u653b\u51fb\u3002\u4f5c\u8005\u5f15\u5165\u4e86\u4e00\u79cd\u57fa\u4e8e\u5b66\u4e60\u81ea\u52a8\u673a\u7684\u65b9\u6cd5\u52a8\u6001\u8c03\u6574\u6270\u52a8\uff0c\u5b9e\u9a8c\u663e\u793a\u9884\u6d4b\u8bef\u5dee\u663e\u8457\u589e\u52a0\uff0c\u5f3a\u8c03\u4e86AI\u9a71\u52a8\u7684\u6570\u5b57\u5b6a\u751f\u4e2d\u7684\u7f51\u7edc\u5b89\u5168\u98ce\u9669\u3002", "motivation": "\u6570\u5b57\u5b6a\u751f\u6280\u672f\u901a\u8fc7\u5b9e\u65f6\u6570\u636e\u548c\u9884\u6d4b\u6a21\u578b\u4f18\u5316\u4f9b\u6c34\u7cfb\u7edf\uff0c\u4f46\u673a\u5668\u5b66\u4e60\u6a21\u578b\u6613\u53d7\u5bf9\u6297\u653b\u51fb\uff0c\u5f71\u54cd\u9884\u6d4b\u51c6\u786e\u6027\u3002", "method": "\u5229\u7528LSTM\u9884\u6d4b\u7528\u6c34\u91cf\uff0c\u5e76\u5f15\u5165\u5b66\u4e60\u81ea\u52a8\u673a\uff08LA\uff09\u548c\u968f\u673aLA\u65b9\u6cd5\u52a8\u6001\u8c03\u6574\u5bf9\u6297\u653b\u51fb\u7684\u6270\u52a8\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u5bf9\u6297\u653b\u51fb\u4f7f\u9884\u6d4b\u8bef\u5dee\uff08MAPE\uff09\u4ece26%\u5347\u81f335%\u4ee5\u4e0a\uff0c\u81ea\u9002\u5e94\u653b\u51fb\u7b56\u7565\u8fdb\u4e00\u6b65\u653e\u5927\u4e86\u8fd9\u4e00\u5f71\u54cd\u3002", "conclusion": "\u7814\u7a76\u5f3a\u8c03\u4e86AI\u9a71\u52a8\u7684\u6570\u5b57\u5b6a\u751f\u4e2d\u7684\u7f51\u7edc\u5b89\u5168\u98ce\u9669\uff0c\u5e76\u547c\u5401\u91c7\u53d6\u5bf9\u6297\u8bad\u7ec3\u3001\u5f02\u5e38\u68c0\u6d4b\u7b49\u9632\u5fa1\u63aa\u65bd\u3002"}}
{"id": "2504.20720", "pdf": "https://arxiv.org/pdf/2504.20720", "abs": "https://arxiv.org/abs/2504.20720", "authors": ["Yiming Liu", "Lijun Han", "Enlin Gu", "Hesheng Wang"], "title": "Learning a General Model: Folding Clothing with Topological Dynamics", "categories": ["cs.RO", "cs.CV"], "comment": null, "summary": "The high degrees of freedom and complex structure of garments present\nsignificant challenges for clothing manipulation. In this paper, we propose a\ngeneral topological dynamics model to fold complex clothing. By utilizing the\nvisible folding structure as the topological skeleton, we design a novel\ntopological graph to represent the clothing state. This topological graph is\nlow-dimensional and applied for complex clothing in various folding states. It\nindicates the constraints of clothing and enables predictions regarding\nclothing movement. To extract graphs from self-occlusion, we apply semantic\nsegmentation to analyze the occlusion relationships and decompose the clothing\nstructure. The decomposed structure is then combined with keypoint detection to\ngenerate the topological graph. To analyze the behavior of the topological\ngraph, we employ an improved Graph Neural Network (GNN) to learn the general\ndynamics. The GNN model can predict the deformation of clothing and is employed\nto calculate the deformation Jacobi matrix for control. Experiments using\njackets validate the algorithm's effectiveness to recognize and fold complex\nclothing with self-occlusion.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u62d3\u6251\u52a8\u529b\u5b66\u6a21\u578b\u7684\u8863\u7269\u6298\u53e0\u65b9\u6cd5\uff0c\u5229\u7528\u62d3\u6251\u56fe\u8868\u793a\u8863\u7269\u72b6\u6001\uff0c\u7ed3\u5408\u8bed\u4e49\u5206\u5272\u548c\u56fe\u795e\u7ecf\u7f51\u7edc\uff08GNN\uff09\u9884\u6d4b\u8863\u7269\u53d8\u5f62\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u8863\u7269\u81ea\u7531\u5ea6\u591a\u3001\u7ed3\u6784\u590d\u6742\uff0c\u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u5904\u7406\u5176\u6298\u53e0\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u8bed\u4e49\u5206\u5272\u5206\u6790\u906e\u6321\u5173\u7cfb\uff0c\u7ed3\u5408\u5173\u952e\u70b9\u68c0\u6d4b\u751f\u6210\u62d3\u6251\u56fe\uff0c\u6539\u8fdb\u7684GNN\u5b66\u4e60\u52a8\u529b\u5b66\u5e76\u9884\u6d4b\u53d8\u5f62\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u8bc6\u522b\u548c\u6298\u53e0\u5177\u6709\u81ea\u906e\u6321\u7684\u590d\u6742\u8863\u7269\uff08\u5982\u5939\u514b\uff09\u3002", "conclusion": "\u62d3\u6251\u52a8\u529b\u5b66\u6a21\u578b\u4e3a\u590d\u6742\u8863\u7269\u6298\u53e0\u63d0\u4f9b\u4e86\u901a\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2504.20304", "pdf": "https://arxiv.org/pdf/2504.20304", "abs": "https://arxiv.org/abs/2504.20304", "authors": ["Xiulin Yang", "Zhuoxuan Ju", "Lanni Bu", "Zoey Liu", "Nathan Schneider"], "title": "UD-English-CHILDES: A Collected Resource of Gold and Silver Universal Dependencies Trees for Child Language Interactions", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "CHILDES is a widely used resource of transcribed child and child-directed\nspeech. This paper introduces UD-English-CHILDES, the first officially released\nUniversal Dependencies (UD) treebank derived from previously\ndependency-annotated CHILDES data with consistent and unified annotation\nguidelines. Our corpus harmonizes annotations from 11 children and their\ncaregivers, totaling over 48k sentences. We validate existing gold-standard\nannotations under the UD v2 framework and provide an additional 1M\nsilver-standard sentences, offering a consistent resource for computational and\nlinguistic research.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86UD-English-CHILDES\uff0c\u8fd9\u662f\u9996\u4e2a\u57fa\u4e8eCHILDES\u6570\u636e\u7684\u5b98\u65b9Universal Dependencies\u6811\u5e93\uff0c\u7edf\u4e00\u4e8611\u540d\u513f\u7ae5\u53ca\u5176\u7167\u987e\u8005\u768448k\u53e5\u5b50\u6807\u6ce8\uff0c\u5e76\u63d0\u4f9b\u4e861M\u94f6\u6807\u51c6\u53e5\u5b50\u3002", "motivation": "CHILDES\u662f\u5e7f\u6cdb\u4f7f\u7528\u7684\u513f\u7ae5\u8bed\u8a00\u8d44\u6e90\uff0c\u4f46\u7f3a\u4e4f\u4e00\u81f4\u7684UD\u6807\u6ce8\u6807\u51c6\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u901a\u8fc7\u7edf\u4e00\u548c\u9a8c\u8bc1\u73b0\u6709\u4f9d\u8d56\u6807\u6ce8\u6570\u636e\uff0c\u521b\u5efa\u7b26\u5408UD v2\u6807\u51c6\u7684\u6811\u5e93\u3002", "result": "\u751f\u6210\u4e86\u5305\u542b48k\u91d1\u6807\u51c6\u53e5\u5b50\u548c1M\u94f6\u6807\u51c6\u53e5\u5b50\u7684\u8d44\u6e90\uff0c\u4e3a\u8ba1\u7b97\u548c\u8bed\u8a00\u5b66\u7814\u7a76\u63d0\u4f9b\u652f\u6301\u3002", "conclusion": "UD-English-CHILDES\u4e3a\u513f\u7ae5\u8bed\u8a00\u7814\u7a76\u63d0\u4f9b\u4e86\u6807\u51c6\u5316\u548c\u4e00\u81f4\u6027\u7684\u6570\u636e\u8d44\u6e90\u3002"}}
{"id": "2504.20734", "pdf": "https://arxiv.org/pdf/2504.20734", "abs": "https://arxiv.org/abs/2504.20734", "authors": ["Woongyeong Yeo", "Kangsan Kim", "Soyeong Jeong", "Jinheon Baek", "Sung Ju Hwang"], "title": "UniversalRAG: Retrieval-Augmented Generation over Multiple Corpora with Diverse Modalities and Granularities", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.IR", "cs.LG"], "comment": "Project page : https://universalrag.github.io", "summary": "Retrieval-Augmented Generation (RAG) has shown substantial promise in\nimproving factual accuracy by grounding model responses with external knowledge\nrelevant to queries. However, most existing RAG approaches are limited to a\ntext-only corpus, and while recent efforts have extended RAG to other\nmodalities such as images and videos, they typically operate over a single\nmodality-specific corpus. In contrast, real-world queries vary widely in the\ntype of knowledge they require, which a single type of knowledge source cannot\naddress. To address this, we introduce UniversalRAG, a novel RAG framework\ndesigned to retrieve and integrate knowledge from heterogeneous sources with\ndiverse modalities and granularities. Specifically, motivated by the\nobservation that forcing all modalities into a unified representation space\nderived from a single combined corpus causes a modality gap, where the\nretrieval tends to favor items from the same modality as the query, we propose\na modality-aware routing mechanism that dynamically identifies the most\nappropriate modality-specific corpus and performs targeted retrieval within it.\nAlso, beyond modality, we organize each modality into multiple granularity\nlevels, enabling fine-tuned retrieval tailored to the complexity and scope of\nthe query. We validate UniversalRAG on 8 benchmarks spanning multiple\nmodalities, showing its superiority over modality-specific and unified\nbaselines.", "AI": {"tldr": "UniversalRAG\u662f\u4e00\u4e2a\u65b0\u7684RAG\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u8def\u7531\u673a\u5236\u4ece\u591a\u6a21\u6001\u3001\u591a\u7c92\u5ea6\u7684\u5f02\u6784\u77e5\u8bc6\u6e90\u4e2d\u68c0\u7d22\u4fe1\u606f\uff0c\u89e3\u51b3\u4e86\u73b0\u6709RAG\u65b9\u6cd5\u5c40\u9650\u4e8e\u5355\u4e00\u6a21\u6001\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709RAG\u65b9\u6cd5\u901a\u5e38\u4ec5\u9488\u5bf9\u5355\u4e00\u6a21\u6001\uff08\u5982\u6587\u672c\uff09\uff0c\u65e0\u6cd5\u6ee1\u8db3\u73b0\u5b9e\u67e5\u8be2\u5bf9\u591a\u6837\u5316\u77e5\u8bc6\u7684\u9700\u6c42\u3002", "method": "\u63d0\u51fa\u6a21\u6001\u611f\u77e5\u8def\u7531\u673a\u5236\uff0c\u52a8\u6001\u9009\u62e9\u6700\u5408\u9002\u7684\u6a21\u6001\u7279\u5b9a\u8bed\u6599\u5e93\u8fdb\u884c\u68c0\u7d22\uff0c\u5e76\u7ec4\u7ec7\u591a\u7c92\u5ea6\u7ea7\u522b\u4ee5\u9002\u5e94\u67e5\u8be2\u590d\u6742\u5ea6\u3002", "result": "\u57288\u4e2a\u591a\u6a21\u6001\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cUniversalRAG\u4f18\u4e8e\u5355\u4e00\u6a21\u6001\u548c\u7edf\u4e00\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "UniversalRAG\u901a\u8fc7\u591a\u6a21\u6001\u548c\u591a\u7c92\u5ea6\u68c0\u7d22\uff0c\u663e\u8457\u63d0\u5347\u4e86RAG\u7684\u9002\u5e94\u6027\u548c\u51c6\u786e\u6027\u3002"}}
{"id": "2504.20310", "pdf": "https://arxiv.org/pdf/2504.20310", "abs": "https://arxiv.org/abs/2504.20310", "authors": ["Greg Gluch", "Shafi Goldwasser"], "title": "A Cryptographic Perspective on Mitigation vs. Detection in Machine Learning", "categories": ["cs.LG", "cs.AI", "cs.CR"], "comment": "29 pages", "summary": "In this paper, we initiate a cryptographically inspired theoretical study of\ndetection versus mitigation of adversarial inputs produced by attackers of\nMachine Learning algorithms during inference time.\n  We formally define defense by detection (DbD) and defense by mitigation\n(DbM). Our definitions come in the form of a 3-round protocol between two\nresource-bounded parties: a trainer/defender and an attacker. The attacker aims\nto produce inference-time inputs that fool the training algorithm. We define\ncorrectness, completeness, and soundness properties to capture successful\ndefense at inference time while not degrading (too much) the performance of the\nalgorithm on inputs from the training distribution.\n  We first show that achieving DbD and achieving DbM are equivalent for ML\nclassification tasks. Surprisingly, this is not the case for ML generative\nlearning tasks, where there are many possible correct outputs that can be\ngenerated for each input. We show a separation between DbD and DbM by\nexhibiting a generative learning task for which is possible to defend by\nmitigation but is provably impossible to defend by detection under the\nassumption that the Identity-Based Fully Homomorphic Encryption (IB-FHE),\npublicly-verifiable zero-knowledge Succinct Non-Interactive Arguments of\nKnowledge (zk-SNARK) and Strongly Unforgeable Signatures exist. The mitigation\nphase uses significantly fewer samples than the initial training algorithm.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u673a\u5668\u5b66\u4e60\u4e2d\u5bf9\u6297\u6027\u8f93\u5165\u7684\u68c0\u6d4b\u4e0e\u7f13\u89e3\uff0c\u5b9a\u4e49\u4e86\u9632\u5fa1\u68c0\u6d4b\uff08DbD\uff09\u548c\u9632\u5fa1\u7f13\u89e3\uff08DbM\uff09\uff0c\u5e76\u8bc1\u660e\u5728\u5206\u7c7b\u4efb\u52a1\u4e2d\u4e24\u8005\u7b49\u6548\uff0c\u4f46\u5728\u751f\u6210\u4efb\u52a1\u4e2d\u5206\u79bb\u3002", "motivation": "\u7814\u7a76\u5bf9\u6297\u6027\u8f93\u5165\u5728\u63a8\u7406\u9636\u6bb5\u7684\u9632\u5fa1\u7b56\u7565\uff0c\u4e3a\u673a\u5668\u5b66\u4e60\u7b97\u6cd5\u63d0\u4f9b\u7406\u8bba\u4fdd\u969c\u3002", "method": "\u901a\u8fc73\u8f6e\u534f\u8bae\u5f62\u5f0f\u5316\u5b9a\u4e49DbD\u548cDbM\uff0c\u5e76\u5206\u6790\u5176\u6b63\u786e\u6027\u3001\u5b8c\u5907\u6027\u548c\u53ef\u9760\u6027\u3002", "result": "\u5206\u7c7b\u4efb\u52a1\u4e2dDbD\u4e0eDbM\u7b49\u6548\uff0c\u751f\u6210\u4efb\u52a1\u4e2d\u5206\u79bb\uff0c\u4e14\u7f13\u89e3\u9636\u6bb5\u6837\u672c\u9700\u6c42\u663e\u8457\u51cf\u5c11\u3002", "conclusion": "\u751f\u6210\u4efb\u52a1\u4e2d\u7f13\u89e3\u4f18\u4e8e\u68c0\u6d4b\uff0c\u4e3a\u5b9e\u9645\u9632\u5fa1\u63d0\u4f9b\u7406\u8bba\u4f9d\u636e\u3002"}}
{"id": "2504.20736", "pdf": "https://arxiv.org/pdf/2504.20736", "abs": "https://arxiv.org/abs/2504.20736", "authors": ["Nafiseh Jabbari Tofighi", "Maxime Robic", "Fabio Morbidi", "Pascal Vasseur"], "title": "A Survey on Event-based Optical Marker Systems", "categories": ["cs.RO", "cs.CV"], "comment": "10 pages, 6 figures, 1 table", "summary": "The advent of event-based cameras, with their low latency, high dynamic\nrange, and reduced power consumption, marked a significant change in robotic\nvision and machine perception. In particular, the combination of these\nneuromorphic sensors with widely-available passive or active optical markers\n(e.g. AprilTags, arrays of blinking LEDs), has recently opened up a wide field\nof possibilities. This survey paper provides a comprehensive review on\nEvent-Based Optical Marker Systems (EBOMS). We analyze the basic principles and\ntechnologies on which these systems are based, with a special focus on their\nasynchronous operation and robustness against adverse lighting conditions. We\nalso describe the most relevant applications of EBOMS, including object\ndetection and tracking, pose estimation, and optical communication. The article\nconcludes with a discussion of possible future research directions in this\nrapidly-emerging and multidisciplinary field.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u57fa\u4e8e\u4e8b\u4ef6\u7684\u89c6\u89c9\u6807\u8bb0\u7cfb\u7edf\uff08EBOMS\uff09\uff0c\u5206\u6790\u4e86\u5176\u5f02\u6b65\u64cd\u4f5c\u548c\u5bf9\u6076\u52a3\u5149\u7167\u6761\u4ef6\u7684\u9c81\u68d2\u6027\uff0c\u5e76\u63a2\u8ba8\u4e86\u5176\u5e94\u7528\u548c\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "motivation": "\u4e8b\u4ef6\u76f8\u673a\u7684\u4f4e\u5ef6\u8fdf\u3001\u9ad8\u52a8\u6001\u8303\u56f4\u548c\u4f4e\u529f\u8017\u7279\u6027\u4e3a\u673a\u5668\u4eba\u89c6\u89c9\u548c\u673a\u5668\u611f\u77e5\u5e26\u6765\u4e86\u53d8\u9769\uff0c\u7ed3\u5408\u5149\u5b66\u6807\u8bb0\uff08\u5982AprilTags\u3001LED\u9635\u5217\uff09\u4e3aEBOMS\u63d0\u4f9b\u4e86\u5e7f\u9614\u7684\u5e94\u7528\u524d\u666f\u3002", "method": "\u7efc\u8ff0\u4e86EBOMS\u7684\u57fa\u672c\u539f\u7406\u548c\u6280\u672f\uff0c\u91cd\u70b9\u5206\u6790\u5176\u5f02\u6b65\u64cd\u4f5c\u548c\u5bf9\u6076\u52a3\u5149\u7167\u7684\u9002\u5e94\u6027\u3002", "result": "EBOMS\u5728\u76ee\u6807\u68c0\u6d4b\u4e0e\u8ddf\u8e2a\u3001\u59ff\u6001\u4f30\u8ba1\u548c\u5149\u5b66\u901a\u4fe1\u7b49\u9886\u57df\u6709\u91cd\u8981\u5e94\u7528\u3002", "conclusion": "EBOMS\u662f\u4e00\u4e2a\u5feb\u901f\u53d1\u5c55\u7684\u591a\u5b66\u79d1\u9886\u57df\uff0c\u672a\u6765\u7814\u7a76\u53ef\u8fdb\u4e00\u6b65\u63a2\u7d22\u5176\u6f5c\u529b\u3002"}}
{"id": "2504.20314", "pdf": "https://arxiv.org/pdf/2504.20314", "abs": "https://arxiv.org/abs/2504.20314", "authors": ["Qitao Tan", "Sung-En Chang", "Rui Xia", "Huidong Ji", "Chence Yang", "Ci Zhang", "Jun Liu", "Zheng Zhan", "Zhou Zou", "Yanzhi Wang", "Jin Lu", "Geng Yuan"], "title": "Perturbation-efficient Zeroth-order Optimization for Hardware-friendly On-device Training", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Zeroth-order (ZO) optimization is an emerging deep neural network (DNN)\ntraining paradigm that offers computational simplicity and memory savings.\nHowever, this seemingly promising approach faces a significant and long-ignored\nchallenge. ZO requires generating a substantial number of Gaussian random\nnumbers, which poses significant difficulties and even makes it infeasible for\nhardware platforms, such as FPGAs and ASICs. In this paper, we identify this\ncritical issue, which arises from the mismatch between algorithm and hardware\ndesigners. To address this issue, we proposed PeZO, a perturbation-efficient ZO\nframework. Specifically, we design random number reuse strategies to\nsignificantly reduce the demand for random number generation and introduce a\nhardware-friendly adaptive scaling method to replace the costly Gaussian\ndistribution with a uniform distribution. Our experiments show that PeZO\nreduces the required LUTs and FFs for random number generation by 48.6\\% and\n12.7\\%, and saves at maximum 86\\% power consumption, all without compromising\ntraining performance, making ZO optimization feasible for on-device training.\nTo the best of our knowledge, we are the first to explore the potential of\non-device ZO optimization, providing valuable insights for future research.", "AI": {"tldr": "PeZO\u662f\u4e00\u79cd\u9ad8\u6548\u7684\u96f6\u9636\u4f18\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u51cf\u5c11\u968f\u673a\u6570\u751f\u6210\u9700\u6c42\u548c\u786c\u4ef6\u53cb\u597d\u7684\u65b9\u6cd5\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u8d44\u6e90\u6d88\u8017\u548c\u529f\u8017\uff0c\u540c\u65f6\u4fdd\u6301\u8bad\u7ec3\u6027\u80fd\u3002", "motivation": "\u96f6\u9636\u4f18\u5316\uff08ZO\uff09\u5728DNN\u8bad\u7ec3\u4e2d\u5177\u6709\u8ba1\u7b97\u7b80\u5355\u548c\u5185\u5b58\u8282\u7701\u7684\u4f18\u52bf\uff0c\u4f46\u5176\u5927\u91cf\u751f\u6210\u9ad8\u65af\u968f\u673a\u6570\u7684\u9700\u6c42\u5728\u786c\u4ef6\u5e73\u53f0\u4e0a\u96be\u4ee5\u5b9e\u73b0\uff0c\u5bfc\u81f4\u7b97\u6cd5\u4e0e\u786c\u4ef6\u8bbe\u8ba1\u4e0d\u5339\u914d\u3002", "method": "\u63d0\u51faPeZO\u6846\u67b6\uff0c\u91c7\u7528\u968f\u673a\u6570\u91cd\u7528\u7b56\u7565\u51cf\u5c11\u968f\u673a\u6570\u751f\u6210\u9700\u6c42\uff0c\u5e76\u5f15\u5165\u786c\u4ef6\u53cb\u597d\u7684\u81ea\u9002\u5e94\u7f29\u653e\u65b9\u6cd5\uff0c\u7528\u5747\u5300\u5206\u5e03\u66ff\u4ee3\u9ad8\u65af\u5206\u5e03\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0cPeZO\u51cf\u5c11\u4e8648.6%\u7684LUTs\u548c12.7%\u7684FFs\u9700\u6c42\uff0c\u6700\u5927\u8282\u770186%\u529f\u8017\uff0c\u4e14\u4e0d\u5f71\u54cd\u8bad\u7ec3\u6027\u80fd\u3002", "conclusion": "PeZO\u9996\u6b21\u63a2\u7d22\u4e86\u8bbe\u5907\u4e0a\u96f6\u9636\u4f18\u5316\u7684\u6f5c\u529b\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u5b9d\u8d35\u89c1\u89e3\u3002"}}
{"id": "2504.20323", "pdf": "https://arxiv.org/pdf/2504.20323", "abs": "https://arxiv.org/abs/2504.20323", "authors": ["Chao-Lin Liu", "Po-Hsien Wu", "Yi-Ting Yu"], "title": "Labeling Case Similarity based on Co-Citation of Legal Articles in Judgment Documents with Empirical Dispute-Based Evaluation", "categories": ["cs.CL", "cs.AI", "cs.DL", "cs.IR", "cs.LG"], "comment": "16 pages, 9 figures, 2 tables, the Nineteenth International Workshop\n  on Juris-Informatics (JURISIN 2025), associated with the Seventeenth JSAI\n  International Symposium on AI (JSAI-isAI 2025)", "summary": "This report addresses the challenge of limited labeled datasets for\ndeveloping legal recommender systems, particularly in specialized domains like\nlabor disputes. We propose a new approach leveraging the co-citation of legal\narticles within cases to establish similarity and enable algorithmic\nannotation. This method draws a parallel to the concept of case co-citation,\nutilizing cited precedents as indicators of shared legal issues. To evaluate\nthe labeled results, we employ a system that recommends similar cases based on\nplaintiffs' accusations, defendants' rebuttals, and points of disputes. The\nevaluation demonstrates that the recommender, with finetuned text embedding\nmodels and a reasonable BiLSTM module can recommend labor cases whose\nsimilarity was measured by the co-citation of the legal articles. This research\ncontributes to the development of automated annotation techniques for legal\ndocuments, particularly in areas with limited access to comprehensive legal\ndatabases.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6cd5\u5f8b\u6761\u6b3e\u5171\u5f15\u7528\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u6cd5\u5f8b\u63a8\u8350\u7cfb\u7edf\u4e2d\u6807\u7b7e\u6570\u636e\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u52b3\u52a8\u7ea0\u7eb7\u9886\u57df\u3002", "motivation": "\u89e3\u51b3\u4e13\u4e1a\u6cd5\u5f8b\u9886\u57df\uff08\u5982\u52b3\u52a8\u7ea0\u7eb7\uff09\u4e2d\u6807\u7b7e\u6570\u636e\u6709\u9650\u7684\u95ee\u9898\uff0c\u5f00\u53d1\u81ea\u52a8\u5316\u6807\u6ce8\u6280\u672f\u3002", "method": "\u5229\u7528\u6cd5\u5f8b\u6761\u6b3e\u5728\u6848\u4ef6\u4e2d\u7684\u5171\u5f15\u7528\u5173\u7cfb\u5efa\u7acb\u76f8\u4f3c\u6027\uff0c\u7ed3\u5408\u6587\u672c\u5d4c\u5165\u6a21\u578b\u548cBiLSTM\u6a21\u5757\u8fdb\u884c\u6848\u4f8b\u63a8\u8350\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u63a8\u8350\u76f8\u4f3c\u52b3\u52a8\u7ea0\u7eb7\u6848\u4f8b\uff0c\u5e76\u901a\u8fc7\u5171\u5f15\u7528\u9a8c\u8bc1\u76f8\u4f3c\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u6cd5\u5f8b\u6587\u6863\u7684\u81ea\u52a8\u5316\u6807\u6ce8\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\uff0c\u5c24\u5176\u5728\u6570\u636e\u6709\u9650\u7684\u9886\u57df\u5177\u6709\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2504.20923", "pdf": "https://arxiv.org/pdf/2504.20923", "abs": "https://arxiv.org/abs/2504.20923", "authors": ["Andrea Di Pierno", "Luca Guarnera", "Dario Allegra", "Sebastiano Battiato"], "title": "End-to-end Audio Deepfake Detection from RAW Waveforms: a RawNet-Based Approach with Cross-Dataset Evaluation", "categories": ["cs.SD", "cs.CV", "eess.AS"], "comment": null, "summary": "Audio deepfakes represent a growing threat to digital security and trust,\nleveraging advanced generative models to produce synthetic speech that closely\nmimics real human voices. Detecting such manipulations is especially\nchallenging under open-world conditions, where spoofing methods encountered\nduring testing may differ from those seen during training. In this work, we\npropose an end-to-end deep learning framework for audio deepfake detection that\noperates directly on raw waveforms. Our model, RawNetLite, is a lightweight\nconvolutional-recurrent architecture designed to capture both spectral and\ntemporal features without handcrafted preprocessing. To enhance robustness, we\nintroduce a training strategy that combines data from multiple domains and\nadopts Focal Loss to emphasize difficult or ambiguous samples. We further\ndemonstrate that incorporating codec-based manipulations and applying\nwaveform-level audio augmentations (e.g., pitch shifting, noise, and time\nstretching) leads to significant generalization improvements under realistic\nacoustic conditions. The proposed model achieves over 99.7% F1 and 0.25% EER on\nin-domain data (FakeOrReal), and up to 83.4% F1 with 16.4% EER on a challenging\nout-of-distribution test set (AVSpoof2021 + CodecFake). These findings\nhighlight the importance of diverse training data, tailored objective functions\nand audio augmentations in building resilient and generalizable audio forgery\ndetectors. Code and pretrained models are available at\nhttps://iplab.dmi.unict.it/mfs/Deepfakes/PaperRawNet2025/.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u7684\u7aef\u5230\u7aef\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6RawNetLite\uff0c\u7528\u4e8e\u76f4\u63a5\u5904\u7406\u539f\u59cb\u6ce2\u5f62\u4ee5\u68c0\u6d4b\u97f3\u9891\u6df1\u5ea6\u4f2a\u9020\uff0c\u901a\u8fc7\u591a\u9886\u57df\u6570\u636e\u548cFocal Loss\u63d0\u5347\u9c81\u68d2\u6027\uff0c\u5e76\u5728\u591a\u79cd\u6d4b\u8bd5\u96c6\u4e0a\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u97f3\u9891\u6df1\u5ea6\u4f2a\u9020\u5bf9\u6570\u5b57\u5b89\u5168\u548c\u4fe1\u4efb\u6784\u6210\u5a01\u80c1\uff0c\u73b0\u6709\u68c0\u6d4b\u65b9\u6cd5\u5728\u5f00\u653e\u4e16\u754c\u6761\u4ef6\u4e0b\u8868\u73b0\u4e0d\u4f73\uff0c\u9700\u5e94\u5bf9\u8bad\u7ec3\u4e2d\u672a\u89c1\u8fc7\u7684\u4f2a\u9020\u65b9\u6cd5\u3002", "method": "\u63d0\u51faRawNetLite\u6a21\u578b\uff0c\u7ed3\u5408\u5377\u79ef-\u5faa\u73af\u67b6\u6784\u6355\u83b7\u9891\u8c31\u548c\u65f6\u5e8f\u7279\u5f81\uff0c\u65e0\u9700\u624b\u5de5\u9884\u5904\u7406\uff1b\u91c7\u7528\u591a\u9886\u57df\u6570\u636e\u8bad\u7ec3\u548cFocal Loss\uff0c\u5e76\u5f15\u5165\u97f3\u9891\u589e\u5f3a\u6280\u672f\u3002", "result": "\u5728FakeOrReal\u6570\u636e\u96c6\u4e0aF1\u8fbe99.7%\uff0cEER\u4e3a0.25%\uff1b\u5728AVSpoof2021 + CodecFake\u6570\u636e\u96c6\u4e0aF1\u8fbe83.4%\uff0cEER\u4e3a16.4%\u3002", "conclusion": "\u591a\u6837\u5316\u8bad\u7ec3\u6570\u636e\u3001\u5b9a\u5236\u76ee\u6807\u51fd\u6570\u548c\u97f3\u9891\u589e\u5f3a\u5bf9\u6784\u5efa\u9c81\u68d2\u4e14\u901a\u7528\u7684\u97f3\u9891\u4f2a\u9020\u68c0\u6d4b\u5668\u81f3\u5173\u91cd\u8981\u3002"}}
{"id": "2504.20342", "pdf": "https://arxiv.org/pdf/2504.20342", "abs": "https://arxiv.org/abs/2504.20342", "authors": ["Shou-Tzu Han"], "title": "Narrative-Centered Emotional Reflection: Scaffolding Autonomous Emotional Literacy with AI", "categories": ["cs.HC", "cs.AI", "cs.CY", "H.5.2; H.1.2"], "comment": "10 pages, 5 figures, preliminary results, early-stage work intended\n  for future conference submission", "summary": "Reflexion is an AI-powered platform designed to enable structured emotional\nself-reflection at scale. By integrating real-time emotion detection, layered\nreflective prompting, and metaphorical storytelling generation, Reflexion\nempowers users to engage in autonomous emotional exploration beyond basic\nsentiment categorization. Grounded in theories of expressive writing, cognitive\nrestructuring, self-determination, and critical consciousness development, the\nsystem scaffolds a progressive journey from surface-level emotional recognition\ntoward value-aligned action planning. Initial pilot studies with diverse\nparticipants demonstrate positive outcomes in emotional articulation, cognitive\nreframing, and perceived psychological resilience. Reflexion represents a\npromising direction for scalable, theory-informed affective computing\ninterventions aimed at fostering emotional literacy and psychological growth\nacross educational, therapeutic, and public health contexts.", "AI": {"tldr": "Reflexion\u662f\u4e00\u4e2aAI\u9a71\u52a8\u7684\u5e73\u53f0\uff0c\u901a\u8fc7\u5b9e\u65f6\u60c5\u7eea\u68c0\u6d4b\u3001\u5206\u5c42\u53cd\u601d\u63d0\u793a\u548c\u9690\u55bb\u6545\u4e8b\u751f\u6210\uff0c\u5e2e\u52a9\u7528\u6237\u8fdb\u884c\u81ea\u4e3b\u60c5\u7eea\u63a2\u7d22\uff0c\u5e76\u4fc3\u8fdb\u4ece\u60c5\u7eea\u8bc6\u522b\u5230\u884c\u52a8\u89c4\u5212\u7684\u8f6c\u53d8\u3002", "motivation": "\u65e8\u5728\u901a\u8fc7\u7406\u8bba\u652f\u6301\u7684\u6280\u672f\u624b\u6bb5\uff0c\u63d0\u5347\u7528\u6237\u7684\u60c5\u7eea\u8868\u8fbe\u3001\u8ba4\u77e5\u91cd\u6784\u548c\u5fc3\u7406\u97e7\u6027\uff0c\u9002\u7528\u4e8e\u6559\u80b2\u3001\u6cbb\u7597\u548c\u516c\u5171\u536b\u751f\u9886\u57df\u3002", "method": "\u7ed3\u5408\u5b9e\u65f6\u60c5\u7eea\u68c0\u6d4b\u3001\u5206\u5c42\u53cd\u601d\u63d0\u793a\u548c\u9690\u55bb\u6545\u4e8b\u751f\u6210\uff0c\u57fa\u4e8e\u8868\u8fbe\u6027\u5199\u4f5c\u3001\u8ba4\u77e5\u91cd\u6784\u3001\u81ea\u6211\u51b3\u5b9a\u548c\u6279\u5224\u610f\u8bc6\u53d1\u5c55\u7406\u8bba\u3002", "result": "\u521d\u6b65\u8bd5\u70b9\u7814\u7a76\u8868\u660e\uff0c\u7528\u6237\u5728\u60c5\u7eea\u8868\u8fbe\u3001\u8ba4\u77e5\u91cd\u6784\u548c\u5fc3\u7406\u97e7\u6027\u65b9\u9762\u6709\u79ef\u6781\u6548\u679c\u3002", "conclusion": "Reflexion\u662f\u4e00\u79cd\u6709\u524d\u666f\u7684\u3001\u53ef\u6269\u5c55\u7684\u60c5\u611f\u8ba1\u7b97\u5e72\u9884\u5de5\u5177\uff0c\u6709\u52a9\u4e8e\u63d0\u5347\u60c5\u7eea\u7d20\u517b\u548c\u5fc3\u7406\u6210\u957f\u3002"}}
{"id": "2504.20348", "pdf": "https://arxiv.org/pdf/2504.20348", "abs": "https://arxiv.org/abs/2504.20348", "authors": ["Varatheepan Paramanayakam", "Andreas Karatzas", "Iraklis Anagnostopoulos", "Dimitrios Stamoulis"], "title": "CarbonCall: Sustainability-Aware Function Calling for Large Language Models on Edge Devices", "categories": ["cs.PF", "cs.AI", "cs.SY", "eess.SY"], "comment": null, "summary": "Large Language Models (LLMs) enable real-time function calling in edge AI\nsystems but introduce significant computational overhead, leading to high power\nconsumption and carbon emissions. Existing methods optimize for performance\nwhile neglecting sustainability, making them inefficient for energy-constrained\nenvironments. We introduce CarbonCall, a sustainability-aware function-calling\nframework that integrates dynamic tool selection, carbon-aware execution, and\nquantized LLM adaptation. CarbonCall adjusts power thresholds based on\nreal-time carbon intensity forecasts and switches between model variants to\nsustain high tokens-per-second throughput under power constraints. Experiments\non an NVIDIA Jetson AGX Orin show that CarbonCall reduces carbon emissions by\nup to 52%, power consumption by 30%, and execution time by 30%, while\nmaintaining high efficiency.", "AI": {"tldr": "CarbonCall\u662f\u4e00\u4e2a\u53ef\u6301\u7eed\u6027\u611f\u77e5\u7684\u51fd\u6570\u8c03\u7528\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u5de5\u5177\u9009\u62e9\u3001\u78b3\u611f\u77e5\u6267\u884c\u548c\u91cf\u5316LLM\u9002\u5e94\uff0c\u663e\u8457\u964d\u4f4e\u78b3\u6392\u653e\u548c\u529f\u8017\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u4f18\u5316\u6027\u80fd\u65f6\u5ffd\u89c6\u4e86\u53ef\u6301\u7eed\u6027\uff0c\u5bfc\u81f4\u9ad8\u80fd\u8017\u548c\u78b3\u6392\u653e\uff0c\u4e0d\u9002\u5408\u80fd\u6e90\u53d7\u9650\u73af\u5883\u3002", "method": "CarbonCall\u6574\u5408\u52a8\u6001\u5de5\u5177\u9009\u62e9\u3001\u78b3\u611f\u77e5\u6267\u884c\u548c\u91cf\u5316LLM\u9002\u5e94\uff0c\u6839\u636e\u5b9e\u65f6\u78b3\u5f3a\u5ea6\u9884\u6d4b\u8c03\u6574\u529f\u7387\u9608\u503c\u3002", "result": "\u5728NVIDIA Jetson AGX Orin\u4e0a\u5b9e\u9a8c\u663e\u793a\uff0cCarbonCall\u51cf\u5c11\u78b3\u6392\u653e52%\u3001\u529f\u801730%\u3001\u6267\u884c\u65f6\u95f430%\u3002", "conclusion": "CarbonCall\u5728\u4fdd\u6301\u9ad8\u6548\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u4e86\u53ef\u6301\u7eed\u6027\uff0c\u9002\u7528\u4e8e\u8fb9\u7f18AI\u7cfb\u7edf\u3002"}}
{"id": "2504.20355", "pdf": "https://arxiv.org/pdf/2504.20355", "abs": "https://arxiv.org/abs/2504.20355", "authors": ["Yash Jain", "Vishal Chowdhary"], "title": "Local Prompt Optimization", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Accepted as Oral at NAACL 2025 (Main Conference)", "summary": "In recent years, the use of prompts to guide the output of Large Language\nModels have increased dramatically. However, even the best of experts struggle\nto choose the correct words to stitch up a prompt for the desired task. To\nsolve this, LLM driven prompt optimization emerged as an important problem.\nExisting prompt optimization methods optimize a prompt globally, where in all\nthe prompt tokens have to be optimized over a large vocabulary while solving a\ncomplex task. The large optimization space (tokens) leads to insufficient\nguidance for a better prompt. In this work, we introduce Local Prompt\nOptimization (LPO) that integrates with any general automatic prompt\nengineering method. We identify the optimization tokens in a prompt and nudge\nthe LLM to focus only on those tokens in its optimization step. We observe\nremarkable performance improvements on Math Reasoning (GSM8k and MultiArith)\nand BIG-bench Hard benchmarks across various automatic prompt engineering\nmethods. Further, we show that LPO converges to the optimal prompt faster than\nglobal methods.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5c40\u90e8\u63d0\u793a\u4f18\u5316\u65b9\u6cd5\uff08LPO\uff09\uff0c\u901a\u8fc7\u8bc6\u522b\u63d0\u793a\u4e2d\u7684\u5173\u952e\u4f18\u5316\u4ee4\u724c\uff0c\u663e\u8457\u63d0\u5347\u4e86\u81ea\u52a8\u63d0\u793a\u5de5\u7a0b\u7684\u6027\u80fd\uff0c\u5e76\u5728\u6570\u5b66\u63a8\u7406\u548cBIG-bench Hard\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u4f18\u5f02\u8868\u73b0\u3002", "motivation": "\u73b0\u6709\u63d0\u793a\u4f18\u5316\u65b9\u6cd5\u5168\u5c40\u4f18\u5316\u6240\u6709\u4ee4\u724c\uff0c\u5bfc\u81f4\u4f18\u5316\u7a7a\u95f4\u8fc7\u5927\u4e14\u7f3a\u4e4f\u9488\u5bf9\u6027\u6307\u5bfc\u3002LPO\u65e8\u5728\u901a\u8fc7\u5c40\u90e8\u4f18\u5316\u5173\u952e\u4ee4\u724c\u63d0\u5347\u6548\u7387\u548c\u6548\u679c\u3002", "method": "LPO\u8bc6\u522b\u63d0\u793a\u4e2d\u7684\u4f18\u5316\u4ee4\u724c\uff0c\u5e76\u5f15\u5bfcLLM\u4ec5\u5173\u6ce8\u8fd9\u4e9b\u4ee4\u724c\u8fdb\u884c\u4f18\u5316\uff0c\u4e0e\u73b0\u6709\u81ea\u52a8\u63d0\u793a\u5de5\u7a0b\u65b9\u6cd5\u517c\u5bb9\u3002", "result": "\u5728GSM8k\u3001MultiArith\u548cBIG-bench Hard\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cLPO\u8868\u73b0\u663e\u8457\u4f18\u4e8e\u5168\u5c40\u4f18\u5316\u65b9\u6cd5\uff0c\u4e14\u6536\u655b\u901f\u5ea6\u66f4\u5feb\u3002", "conclusion": "LPO\u901a\u8fc7\u5c40\u90e8\u4f18\u5316\u5173\u952e\u4ee4\u724c\uff0c\u6709\u6548\u63d0\u5347\u4e86\u63d0\u793a\u4f18\u5316\u7684\u6027\u80fd\u548c\u6548\u7387\uff0c\u4e3a\u81ea\u52a8\u63d0\u793a\u5de5\u7a0b\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2504.20357", "pdf": "https://arxiv.org/pdf/2504.20357", "abs": "https://arxiv.org/abs/2504.20357", "authors": ["Jason Wang", "Basem Suleiman", "Muhammad Johan Alibasa"], "title": "Automated Unit Test Case Generation: A Systematic Literature Review", "categories": ["cs.SE", "cs.AI"], "comment": null, "summary": "Software is omnipresent within all factors of society. It is thus important\nto ensure that software are well tested to mitigate bad user experiences as\nwell as the potential for severe financial and human losses. Software testing\nis however expensive and absorbs valuable time and resources. As a result, the\nfield of automated software testing has grown of interest to researchers in\npast decades. In our review of present and past research papers, we have\nidentified an information gap in the areas of improvement for the Genetic\nAlgorithm and Particle Swarm Optimisation. A gap in knowledge in the current\nchallenges that face automated testing has also been identified. We therefore\npresent this systematic literature review in an effort to consolidate existing\nknowledge in regards to the evolutionary approaches as well as their\nimprovements and resulting limitations. These improvements include hybrid\nalgorithm combinations as well as interoperability with mutation testing and\nneural networks. We will also explore the main test criterion that are used in\nthese algorithms alongside the challenges currently faced in the field related\nto readability, mocking and more.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u81ea\u52a8\u5316\u8f6f\u4ef6\u6d4b\u8bd5\u9886\u57df\u7684\u9057\u4f20\u7b97\u6cd5\u548c\u7c92\u5b50\u7fa4\u4f18\u5316\u7b97\u6cd5\u7684\u6539\u8fdb\u53ca\u6311\u6218\uff0c\u65e8\u5728\u586b\u8865\u73b0\u6709\u77e5\u8bc6\u7a7a\u767d\u3002", "motivation": "\u8f6f\u4ef6\u6d4b\u8bd5\u5bf9\u907f\u514d\u4e0d\u826f\u7528\u6237\u4f53\u9a8c\u548c\u6f5c\u5728\u635f\u5931\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u6210\u672c\u9ad8\u6602\u3002\u81ea\u52a8\u5316\u6d4b\u8bd5\u56e0\u6b64\u6210\u4e3a\u7814\u7a76\u70ed\u70b9\uff0c\u4f46\u76ee\u524d\u5bf9\u9057\u4f20\u7b97\u6cd5\u548c\u7c92\u5b50\u7fa4\u4f18\u5316\u7684\u6539\u8fdb\u53ca\u6311\u6218\u7f3a\u4e4f\u7cfb\u7edf\u603b\u7ed3\u3002", "method": "\u901a\u8fc7\u7cfb\u7edf\u6587\u732e\u7efc\u8ff0\uff0c\u6574\u5408\u8fdb\u5316\u65b9\u6cd5\u7684\u6539\u8fdb\uff08\u5982\u6df7\u5408\u7b97\u6cd5\u3001\u7a81\u53d8\u6d4b\u8bd5\u4e0e\u795e\u7ecf\u7f51\u7edc\u7684\u7ed3\u5408\uff09\u53ca\u5176\u5c40\u9650\u6027\uff0c\u5e76\u63a2\u8ba8\u4e3b\u8981\u6d4b\u8bd5\u6807\u51c6\u548c\u5f53\u524d\u6311\u6218\u3002", "result": "\u603b\u7ed3\u4e86\u9057\u4f20\u7b97\u6cd5\u548c\u7c92\u5b50\u7fa4\u4f18\u5316\u7684\u6539\u8fdb\u65b9\u6cd5\uff08\u5982\u6df7\u5408\u7b97\u6cd5\uff09\u53ca\u5176\u5c40\u9650\u6027\uff0c\u5e76\u8bc6\u522b\u4e86\u81ea\u52a8\u5316\u6d4b\u8bd5\u9886\u57df\u7684\u4e3b\u8981\u6311\u6218\uff08\u5982\u53ef\u8bfb\u6027\u3001\u6a21\u62df\u7b49\uff09\u3002", "conclusion": "\u672c\u6587\u586b\u8865\u4e86\u9057\u4f20\u7b97\u6cd5\u548c\u7c92\u5b50\u7fa4\u4f18\u5316\u5728\u81ea\u52a8\u5316\u6d4b\u8bd5\u4e2d\u7684\u77e5\u8bc6\u7a7a\u767d\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u65b9\u5411\u3002"}}
{"id": "2504.20368", "pdf": "https://arxiv.org/pdf/2504.20368", "abs": "https://arxiv.org/abs/2504.20368", "authors": ["David Gordon", "Panayiotis Petousis", "Susanne B. Nicholas", "Alex A. T. Bui"], "title": "AKIBoards: A Structure-Following Multiagent System for Predicting Acute Kidney Injury", "categories": ["cs.MA", "cs.AI", "cs.LG"], "comment": "Accepted at International Conference on Autonomous Agents and\n  Multiagent Systems (AAMAS) Workshop, 2025", "summary": "Diagnostic reasoning entails a physician's local (mental) model based on an\nassumed or known shared perspective (global model) to explain patient\nobservations with evidence assigned towards a clinical assessment. But in\nseveral (complex) medical situations, multiple experts work together as a team\nto optimize health evaluation and decision-making by leveraging different\nperspectives. Such consensus-driven reasoning reflects individual knowledge\ncontributing toward a broader perspective on the patient. In this light, we\nintroduce STRUCture-following for Multiagent Systems (STRUC-MAS), a framework\nautomating the learning of these global models and their incorporation as prior\nbeliefs for agents in multiagent systems (MAS) to follow. We demonstrate proof\nof concept with a prosocial MAS application for predicting acute kidney\ninjuries (AKIs). In this case, we found that incorporating a global structure\nenabled multiple agents to achieve better performance (average precision, AP)\nin predicting AKI 48 hours before onset (structure-following-fine-tuned, SF-FT,\nAP=0.195; SF-FT-retrieval-augmented generation, SF-FT-RAG, AP=0.194) vs.\nbaseline (non-structure-following-FT, NSF-FT, AP=0.141; NSF-FT-RAG, AP=0.180)\nfor balanced precision-weighted-recall-weighted voting. Markedly, SF-FT agents\nwith higher recall scores reported lower confidence levels in the initial round\non true positive and false negative cases. But after explicit interactions,\ntheir confidence in their decisions increased (suggesting reinforced belief).\nIn contrast, the SF-FT agent with the lowest recall decreased its confidence in\ntrue positive and false negative cases (suggesting a new belief). This approach\nsuggests that learning and leveraging global structures in MAS is necessary\nprior to achieving competitive classification and diagnostic reasoning\nperformance.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faSTRUC-MAS\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff08MAS\uff09\u5b66\u4e60\u5168\u5c40\u6a21\u578b\uff0c\u63d0\u5347\u590d\u6742\u533b\u7597\u8bca\u65ad\u4e2d\u7684\u534f\u4f5c\u63a8\u7406\u6027\u80fd\uff0c\u5e76\u4ee5\u6025\u6027\u80be\u635f\u4f24\uff08AKI\uff09\u9884\u6d4b\u4e3a\u4f8b\u9a8c\u8bc1\u5176\u6548\u679c\u3002", "motivation": "\u5728\u590d\u6742\u533b\u7597\u573a\u666f\u4e2d\uff0c\u591a\u4e13\u5bb6\u534f\u4f5c\u9700\u6574\u5408\u4e0d\u540c\u89c6\u89d2\uff0c\u4f46\u7f3a\u4e4f\u81ea\u52a8\u5316\u5b66\u4e60\u5168\u5c40\u6a21\u578b\u7684\u65b9\u6cd5\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7MAS\u6846\u67b6\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u63d0\u51faSTRUC-MAS\u6846\u67b6\uff0c\u81ea\u52a8\u5316\u5b66\u4e60\u5168\u5c40\u6a21\u578b\u5e76\u5c06\u5176\u4f5c\u4e3a\u667a\u80fd\u4f53\u7684\u5148\u9a8c\u4fe1\u5ff5\uff0c\u5e94\u7528\u4e8eAKI\u9884\u6d4b\u4efb\u52a1\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u91c7\u7528\u5168\u5c40\u7ed3\u6784\u7684\u667a\u80fd\u4f53\uff08SF-FT\u548cSF-FT-RAG\uff09\u5728AKI\u9884\u6d4b\u4e2d\u8868\u73b0\u4f18\u4e8e\u57fa\u7ebf\uff08NSF-FT\u548cNSF-FT-RAG\uff09\uff0c\u4e14\u4ea4\u4e92\u540e\u667a\u80fd\u4f53\u51b3\u7b56\u4fe1\u5fc3\u63d0\u5347\u3002", "conclusion": "\u5b66\u4e60\u5e76\u5229\u7528\u5168\u5c40\u7ed3\u6784\u5bf9MAS\u7684\u5206\u7c7b\u548c\u8bca\u65ad\u63a8\u7406\u6027\u80fd\u81f3\u5173\u91cd\u8981\uff0c\u672a\u6765\u53ef\u6269\u5c55\u81f3\u5176\u4ed6\u533b\u7597\u534f\u4f5c\u573a\u666f\u3002"}}
{"id": "2504.20408", "pdf": "https://arxiv.org/pdf/2504.20408", "abs": "https://arxiv.org/abs/2504.20408", "authors": ["Jae Yong Lee", "Gwang Jae Jung", "Byung Chan Lim", "Hyung Ju Hwang"], "title": "FourierSpecNet: Neural Collision Operator Approximation Inspired by the Fourier Spectral Method for Solving the Boltzmann Equation", "categories": ["cs.LG", "cs.AI", "cs.NA", "math.NA", "physics.comp-ph", "68T20, 35Q20, 35B40, 82C40"], "comment": "27 pages, 11 figures", "summary": "The Boltzmann equation, a fundamental model in kinetic theory, describes the\nevolution of particle distribution functions through a nonlinear,\nhigh-dimensional collision operator. However, its numerical solution remains\ncomputationally demanding, particularly for inelastic collisions and\nhigh-dimensional velocity domains. In this work, we propose the Fourier Neural\nSpectral Network (FourierSpecNet), a hybrid framework that integrates the\nFourier spectral method with deep learning to approximate the collision\noperator in Fourier space efficiently. FourierSpecNet achieves\nresolution-invariant learning and supports zero-shot super-resolution, enabling\naccurate predictions at unseen resolutions without retraining. Beyond empirical\nvalidation, we establish a consistency result showing that the trained operator\nconverges to the spectral solution as the discretization is refined. We\nevaluate our method on several benchmark cases, including Maxwellian and\nhard-sphere molecular models, as well as inelastic collision scenarios. The\nresults demonstrate that FourierSpecNet offers competitive accuracy while\nsignificantly reducing computational cost compared to traditional spectral\nsolvers. Our approach provides a robust and scalable alternative for solving\nthe Boltzmann equation across both elastic and inelastic regimes.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u5085\u91cc\u53f6\u8c31\u65b9\u6cd5\u548c\u6df1\u5ea6\u5b66\u4e60\u7684\u6df7\u5408\u6846\u67b6FourierSpecNet\uff0c\u7528\u4e8e\u9ad8\u6548\u8fd1\u4f3c\u73bb\u5c14\u5179\u66fc\u65b9\u7a0b\u4e2d\u7684\u78b0\u649e\u7b97\u5b50\uff0c\u652f\u6301\u96f6\u6837\u672c\u8d85\u5206\u8fa8\u7387\uff0c\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u3002", "motivation": "\u73bb\u5c14\u5179\u66fc\u65b9\u7a0b\u7684\u9ad8\u7ef4\u975e\u7ebf\u6027\u78b0\u649e\u7b97\u5b50\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u5c24\u5176\u5728\u975e\u5f39\u6027\u78b0\u649e\u548c\u9ad8\u7ef4\u901f\u5ea6\u57df\u4e2d\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u6570\u503c\u89e3\u6cd5\u3002", "method": "\u7ed3\u5408\u5085\u91cc\u53f6\u8c31\u65b9\u6cd5\u548c\u6df1\u5ea6\u5b66\u4e60\uff0c\u6784\u5efaFourierSpecNet\u6846\u67b6\uff0c\u5b9e\u73b0\u5206\u8fa8\u7387\u65e0\u5173\u5b66\u4e60\u548c\u96f6\u6837\u672c\u8d85\u5206\u8fa8\u7387\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cFourierSpecNet\u8868\u73b0\u51fa\u4e0e\u4f20\u7edf\u8c31\u65b9\u6cd5\u76f8\u5f53\u7684\u7cbe\u5ea6\uff0c\u540c\u65f6\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u3002", "conclusion": "FourierSpecNet\u4e3a\u73bb\u5c14\u5179\u66fc\u65b9\u7a0b\u7684\u6c42\u89e3\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u5f39\u6027\u548c\u975e\u5f39\u6027\u78b0\u649e\u573a\u666f\u3002"}}
{"id": "2504.20412", "pdf": "https://arxiv.org/pdf/2504.20412", "abs": "https://arxiv.org/abs/2504.20412", "authors": ["Alex Mathai", "Chenxi Huang", "Suwei Ma", "Jihwan Kim", "Hailie Mitchell", "Aleksandr Nogikh", "Petros Maniatis", "Franjo Ivan\u010di\u0107", "Junfeng Yang", "Baishakhi Ray"], "title": "CrashFixer: A crash resolution agent for the Linux kernel", "categories": ["cs.SE", "cs.AI", "cs.OS"], "comment": null, "summary": "Code large language models (LLMs) have shown impressive capabilities on a\nmultitude of software engineering tasks. In particular, they have demonstrated\nremarkable utility in the task of code repair. However, common benchmarks used\nto evaluate the performance of code LLMs are often limited to small-scale\nsettings. In this work, we build upon kGym, which shares a benchmark for\nsystem-level Linux kernel bugs and a platform to run experiments on the Linux\nkernel.\n  This paper introduces CrashFixer, the first LLM-based software repair agent\nthat is applicable to Linux kernel bugs. Inspired by the typical workflow of a\nkernel developer, we identify the key capabilities an expert developer\nleverages to resolve a kernel crash. Using this as our guide, we revisit the\nkGym platform and identify key system improvements needed to practically run\nLLM-based agents at the scale of the Linux kernel (50K files and 20M lines of\ncode). We implement these changes by extending kGym to create an improved\nplatform - called kGymSuite, which will be open-sourced. Finally, the paper\npresents an evaluation of various repair strategies for such complex kernel\nbugs and showcases the value of explicitly generating a hypothesis before\nattempting to fix bugs in complex systems such as the Linux kernel. We also\nevaluated CrashFixer's capabilities on still open bugs, and found at least two\npatch suggestions considered plausible to resolve the reported bug.", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86CrashFixer\uff0c\u9996\u4e2a\u9002\u7528\u4e8eLinux\u5185\u6838\u6f0f\u6d1e\u7684\u57fa\u4e8eLLM\u7684\u4fee\u590d\u5de5\u5177\uff0c\u901a\u8fc7\u6539\u8fdbkGym\u5e73\u53f0\uff08kGymSuite\uff09\u5e76\u8bc4\u4f30\u4fee\u590d\u7b56\u7565\uff0c\u5c55\u793a\u4e86\u5728\u590d\u6742\u7cfb\u7edf\u4e2d\u751f\u6210\u5047\u8bbe\u7684\u4ef7\u503c\u3002", "motivation": "\u73b0\u6709\u4ee3\u7801LLM\u8bc4\u4f30\u57fa\u51c6\u5c40\u9650\u4e8e\u5c0f\u89c4\u6a21\u573a\u666f\uff0c\u800cLinux\u5185\u6838\u6f0f\u6d1e\u4fee\u590d\u9700\u8981\u66f4\u5f3a\u5927\u7684\u5de5\u5177\u548c\u5e73\u53f0\u652f\u6301\u3002", "method": "\u57fa\u4e8ekGym\u5e73\u53f0\u6539\u8fdb\u4e3akGymSuite\uff0c\u8bbe\u8ba1CrashFixer\u5de5\u5177\uff0c\u6a21\u62df\u5f00\u53d1\u8005\u5de5\u4f5c\u6d41\u7a0b\uff0c\u751f\u6210\u4fee\u590d\u5047\u8bbe\u5e76\u9a8c\u8bc1\u3002", "result": "CrashFixer\u5728\u672a\u4fee\u590d\u6f0f\u6d1e\u4e2d\u63d0\u51fa\u81f3\u5c11\u4e24\u4e2a\u53ef\u884c\u7684\u8865\u4e01\u5efa\u8bae\u3002", "conclusion": "CrashFixer\u5c55\u793a\u4e86LLM\u5728\u590d\u6742\u7cfb\u7edf\u4fee\u590d\u4e2d\u7684\u6f5c\u529b\uff0c\u6539\u8fdb\u5e73\u53f0\u548c\u7b56\u7565\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u57fa\u7840\u3002"}}
{"id": "2504.20434", "pdf": "https://arxiv.org/pdf/2504.20434", "abs": "https://arxiv.org/abs/2504.20434", "authors": ["Manish Bhattarai", "Miguel Cordova", "Javier Santos", "Dan O'Malley"], "title": "ARCS: Agentic Retrieval-Augmented Code Synthesis with Iterative Refinement", "categories": ["cs.SE", "cs.AI"], "comment": null, "summary": "In supercomputing, efficient and optimized code generation is essential to\nleverage high-performance systems effectively. We propose Agentic\nRetrieval-Augmented Code Synthesis (ARCS), an advanced framework for accurate,\nrobust, and efficient code generation, completion, and translation. ARCS\nintegrates Retrieval-Augmented Generation (RAG) with Chain-of-Thought (CoT)\nreasoning to systematically break down and iteratively refine complex\nprogramming tasks. An agent-based RAG mechanism retrieves relevant code\nsnippets, while real-time execution feedback drives the synthesis of candidate\nsolutions. This process is formalized as a state-action search tree\noptimization, balancing code correctness with editing efficiency. Evaluations\non the Geeks4Geeks and HumanEval benchmarks demonstrate that ARCS significantly\noutperforms traditional prompting methods in translation and generation\nquality. By enabling scalable and precise code synthesis, ARCS offers\ntransformative potential for automating and optimizing code development in\nsupercomputing applications, enhancing computational resource utilization.", "AI": {"tldr": "ARCS\u6846\u67b6\u7ed3\u5408\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u4e0e\u601d\u7ef4\u94fe\u63a8\u7406\uff0c\u901a\u8fc7\u4ee3\u7406\u673a\u5236\u548c\u5b9e\u65f6\u53cd\u9988\u4f18\u5316\u4ee3\u7801\u751f\u6210\uff0c\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "motivation": "\u5728\u8d85\u7ea7\u8ba1\u7b97\u4e2d\uff0c\u9ad8\u6548\u4e14\u4f18\u5316\u7684\u4ee3\u7801\u751f\u6210\u5bf9\u5145\u5206\u5229\u7528\u9ad8\u6027\u80fd\u7cfb\u7edf\u81f3\u5173\u91cd\u8981\u3002", "method": "ARCS\u6574\u5408\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u4e0e\u601d\u7ef4\u94fe\u63a8\u7406\uff0c\u901a\u8fc7\u4ee3\u7406\u673a\u5236\u68c0\u7d22\u4ee3\u7801\u7247\u6bb5\uff0c\u5e76\u5229\u7528\u5b9e\u65f6\u6267\u884c\u53cd\u9988\u4f18\u5316\u5019\u9009\u89e3\u51b3\u65b9\u6848\u3002", "result": "\u5728Geeks4Geeks\u548cHumanEval\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cARCS\u5728\u4ee3\u7801\u7ffb\u8bd1\u548c\u751f\u6210\u8d28\u91cf\u4e0a\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "conclusion": "ARCS\u4e3a\u8d85\u7ea7\u8ba1\u7b97\u5e94\u7528\u4e2d\u7684\u4ee3\u7801\u5f00\u53d1\u81ea\u52a8\u5316\u548c\u4f18\u5316\u63d0\u4f9b\u4e86\u53d8\u9769\u6027\u6f5c\u529b\uff0c\u63d0\u5347\u4e86\u8ba1\u7b97\u8d44\u6e90\u5229\u7528\u7387\u3002"}}
{"id": "2504.20437", "pdf": "https://arxiv.org/pdf/2504.20437", "abs": "https://arxiv.org/abs/2504.20437", "authors": ["DiJia Su", "Andrew Gu", "Jane Xu", "Yuandong Tian", "Jiawei Zhao"], "title": "GaLore 2: Large-Scale LLM Pre-Training by Gradient Low-Rank Projection", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) have revolutionized natural language\nunderstanding and generation but face significant memory bottlenecks during\ntraining. GaLore, Gradient Low-Rank Projection, addresses this issue by\nleveraging the inherent low-rank structure of weight gradients, enabling\nsubstantial memory savings without sacrificing performance. Recent works\nfurther extend GaLore from various aspects, including low-bit quantization and\nhigher-order tensor structures. However, there are several remaining challenges\nfor GaLore, such as the computational overhead of SVD for subspace updates and\nthe integration with state-of-the-art training parallelization strategies\n(e.g., FSDP). In this paper, we present GaLore 2, an efficient and scalable\nGaLore framework that addresses these challenges and incorporates recent\nadvancements. In addition, we demonstrate the scalability of GaLore 2 by\npre-training Llama 7B from scratch using up to 500 billion training tokens,\nhighlighting its potential impact on real LLM pre-training scenarios.", "AI": {"tldr": "GaLore 2\u662f\u4e00\u79cd\u9ad8\u6548\u4e14\u53ef\u6269\u5c55\u7684\u6846\u67b6\uff0c\u89e3\u51b3\u4e86GaLore\u5728\u8ba1\u7b97\u5f00\u9500\u548c\u5e76\u884c\u5316\u7b56\u7565\u96c6\u6210\u65b9\u9762\u7684\u6311\u6218\uff0c\u5e76\u5728\u5927\u89c4\u6a21\u9884\u8bad\u7ec3\u4e2d\u5c55\u793a\u4e86\u5176\u6f5c\u529b\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u8bad\u7ec3\u4e2d\u9762\u4e34\u663e\u8457\u7684\u5185\u5b58\u74f6\u9888\uff0cGaLore\u901a\u8fc7\u5229\u7528\u68af\u5ea6\u7684\u4f4e\u79e9\u7ed3\u6784\u89e3\u51b3\u4e86\u8fd9\u4e00\u95ee\u9898\uff0c\u4f46\u4ecd\u5b58\u5728\u8ba1\u7b97\u5f00\u9500\u548c\u5e76\u884c\u5316\u96c6\u6210\u7b49\u6311\u6218\u3002", "method": "GaLore 2\u6539\u8fdb\u4e86GaLore\u6846\u67b6\uff0c\u89e3\u51b3\u4e86SVD\u8ba1\u7b97\u5f00\u9500\u548c\u4e0e\u5148\u8fdb\u5e76\u884c\u5316\u7b56\u7565\uff08\u5982FSDP\uff09\u7684\u96c6\u6210\u95ee\u9898\u3002", "result": "GaLore 2\u5728\u5927\u89c4\u6a21\u9884\u8bad\u7ec3\uff08\u5982Llama 7B\u6a21\u578b\uff0c5000\u4ebf\u8bad\u7ec3\u6807\u8bb0\uff09\u4e2d\u5c55\u793a\u4e86\u9ad8\u6548\u6027\u548c\u53ef\u6269\u5c55\u6027\u3002", "conclusion": "GaLore 2\u4e3aLLM\u9884\u8bad\u7ec3\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2504.20444", "pdf": "https://arxiv.org/pdf/2504.20444", "abs": "https://arxiv.org/abs/2504.20444", "authors": ["Mika H\u00e4m\u00e4l\u00e4inen"], "title": "On Psychology of AI -- Does Primacy Effect Affect ChatGPT and Other LLMs?", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "We study the primacy effect in three commercial LLMs: ChatGPT, Gemini and\nClaude. We do this by repurposing the famous experiment Asch (1946) conducted\nusing human subjects. The experiment is simple, given two candidates with equal\ndescriptions which one is preferred if one description has positive adjectives\nfirst before negative ones and another description has negative adjectives\nfollowed by positive ones. We test this in two experiments. In one experiment,\nLLMs are given both candidates simultaneously in the same prompt, and in\nanother experiment, LLMs are given both candidates separately. We test all the\nmodels with 200 candidate pairs. We found that, in the first experiment,\nChatGPT preferred the candidate with positive adjectives listed first, while\nGemini preferred both equally often. Claude refused to make a choice. In the\nsecond experiment, ChatGPT and Claude were most likely to rank both candidates\nequally. In the case where they did not give an equal rating, both showed a\nclear preference to a candidate that had negative adjectives listed first.\nGemini was most likely to prefer a candidate with negative adjectives listed\nfirst.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u4e09\u79cd\u5546\u4e1aLLM\uff08ChatGPT\u3001Gemini\u548cClaude\uff09\u4e2d\u7684\u9996\u56e0\u6548\u5e94\uff0c\u901a\u8fc7\u91cd\u505aAsch\uff081946\uff09\u5b9e\u9a8c\u53d1\u73b0\uff0c\u4e0d\u540c\u6a21\u578b\u5728\u4e0d\u540c\u5b9e\u9a8c\u6761\u4ef6\u4e0b\u5bf9\u5019\u9009\u8005\u7684\u504f\u597d\u4e0d\u540c\u3002", "motivation": "\u63a2\u7a76LLM\u662f\u5426\u50cf\u4eba\u7c7b\u4e00\u6837\u53d7\u5230\u9996\u56e0\u6548\u5e94\u7684\u5f71\u54cd\uff0c\u5373\u5728\u63cf\u8ff0\u987a\u5e8f\u4e0d\u540c\u65f6\u662f\u5426\u8868\u73b0\u51fa\u504f\u597d\u5dee\u5f02\u3002", "method": "\u901a\u8fc7\u4e24\u79cd\u5b9e\u9a8c\u8bbe\u8ba1\uff1a1\uff09\u540c\u65f6\u5448\u73b0\u4e24\u4e2a\u5019\u9009\u8005\uff1b2\uff09\u5206\u522b\u5448\u73b0\u5019\u9009\u8005\uff0c\u6d4b\u8bd5\u6a21\u578b\u5bf9\u63cf\u8ff0\u987a\u5e8f\u7684\u504f\u597d\u3002", "result": "ChatGPT\u5728\u540c\u65f6\u5448\u73b0\u65f6\u504f\u597d\u6b63\u9762\u5f62\u5bb9\u8bcd\u5148\u51fa\u73b0\u7684\u5019\u9009\u8005\uff0c\u800cGemini\u65e0\u504f\u597d\uff0cClaude\u62d2\u7edd\u9009\u62e9\uff1b\u5728\u5206\u522b\u5448\u73b0\u65f6\uff0cChatGPT\u548cClaude\u591a\u5e73\u7b49\u8bc4\u4ef7\uff0c\u5426\u5219\u504f\u597d\u8d1f\u9762\u5f62\u5bb9\u8bcd\u5148\u51fa\u73b0\u7684\u5019\u9009\u8005\uff0cGemini\u5219\u66f4\u504f\u597d\u8d1f\u9762\u5f62\u5bb9\u8bcd\u5148\u51fa\u73b0\u3002", "conclusion": "LLM\u7684\u8868\u73b0\u56e0\u6a21\u578b\u548c\u5b9e\u9a8c\u8bbe\u8ba1\u800c\u5f02\uff0c\u90e8\u5206\u6a21\u578b\u8868\u73b0\u51fa\u7c7b\u4f3c\u4eba\u7c7b\u7684\u504f\u597d\u6a21\u5f0f\uff0c\u4f46\u5e76\u975e\u6240\u6709\u6a21\u578b\u90fd\u53d7\u9996\u56e0\u6548\u5e94\u5f71\u54cd\u3002"}}
{"id": "2504.20447", "pdf": "https://arxiv.org/pdf/2504.20447", "abs": "https://arxiv.org/abs/2504.20447", "authors": ["Zhicheng Lian", "Lizhi Wang", "Hua Huang"], "title": "APG-MOS: Auditory Perception Guided-MOS Predictor for Synthetic Speech", "categories": ["cs.SD", "cs.AI", "eess.AS"], "comment": null, "summary": "Automatic speech quality assessment aims to quantify subjective human\nperception of speech through computational models to reduce the need for\nlabor-consuming manual evaluations. While models based on deep learning have\nachieved progress in predicting mean opinion scores (MOS) to assess synthetic\nspeech, the neglect of fundamental auditory perception mechanisms limits\nconsistency with human judgments. To address this issue, we propose an auditory\nperception guided-MOS prediction model (APG-MOS) that synergistically\nintegrates auditory modeling with semantic analysis to enhance consistency with\nhuman judgments. Specifically, we first design a perceptual module, grounded in\nbiological auditory mechanisms, to simulate cochlear functions, which encodes\nacoustic signals into biologically aligned electrochemical representations.\nSecondly, we propose a residual vector quantization (RVQ)-based semantic\ndistortion modeling method to quantify the degradation of speech quality at the\nsemantic level. Finally, we design a residual cross-attention architecture,\ncoupled with a progressive learning strategy, to enable multimodal fusion of\nencoded electrochemical signals and semantic representations. Experiments\ndemonstrate that APG-MOS achieves superior performance on two primary\nbenchmarks. Our code and checkpoint will be available on a public repository\nupon publication.", "AI": {"tldr": "APG-MOS\u6a21\u578b\u901a\u8fc7\u7ed3\u5408\u542c\u89c9\u611f\u77e5\u673a\u5236\u548c\u8bed\u4e49\u5206\u6790\uff0c\u63d0\u5347\u8bed\u97f3\u8d28\u91cf\u8bc4\u4f30\u4e0e\u4eba\u7c7b\u5224\u65ad\u7684\u4e00\u81f4\u6027\u3002", "motivation": "\u51cf\u5c11\u4eba\u5de5\u8bc4\u4f30\u7684\u8017\u65f6\uff0c\u540c\u65f6\u89e3\u51b3\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5ffd\u89c6\u542c\u89c9\u611f\u77e5\u673a\u5236\u7684\u95ee\u9898\u3002", "method": "\u8bbe\u8ba1\u57fa\u4e8e\u751f\u7269\u542c\u89c9\u673a\u5236\u7684\u611f\u77e5\u6a21\u5757\u3001RVQ\u8bed\u4e49\u5931\u771f\u5efa\u6a21\u65b9\u6cd5\u53ca\u6b8b\u5dee\u4ea4\u53c9\u6ce8\u610f\u529b\u67b6\u6784\u3002", "result": "\u5728\u4e24\u5927\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "APG-MOS\u4e3a\u8bed\u97f3\u8d28\u91cf\u8bc4\u4f30\u63d0\u4f9b\u4e86\u66f4\u63a5\u8fd1\u4eba\u7c7b\u611f\u77e5\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2504.20452", "pdf": "https://arxiv.org/pdf/2504.20452", "abs": "https://arxiv.org/abs/2504.20452", "authors": ["Hai-Dang Kieu", "Delvin Ce Zhang", "Minh Duc Nguyen", "Min Xu", "Qiang Wu", "Dung D. Le"], "title": "Enhancing News Recommendation with Hierarchical LLM Prompting", "categories": ["cs.IR", "cs.AI"], "comment": null, "summary": "Personalized news recommendation systems often struggle to effectively\ncapture the complexity of user preferences, as they rely heavily on shallow\nrepresentations, such as article titles and abstracts. To address this problem,\nwe introduce a novel method, namely PNR-LLM, for Large Language Models for\nPersonalized News Recommendation. Specifically, PNR-LLM harnesses the\ngeneration capabilities of LLMs to enrich news titles and abstracts, and\nconsequently improves recommendation quality. PNR-LLM contains a novel module,\nNews Enrichment via LLMs, which generates deeper semantic information and\nrelevant entities from articles, transforming shallow contents into richer\nrepresentations. We further propose an attention mechanism to aggregate\nenriched semantic- and entity-level data, forming unified user and news\nembeddings that reveal a more accurate user-news match. Extensive experiments\non MIND datasets show that PNR-LLM outperforms state-of-the-art baselines.\nMoreover, the proposed data enrichment module is model-agnostic, and we\nempirically show that applying our proposed module to multiple existing models\ncan further improve their performance, verifying the advantage of our design.", "AI": {"tldr": "PNR-LLM\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u751f\u6210\u65b0\u95fb\u6807\u9898\u548c\u6458\u8981\u7684\u6df1\u5c42\u8bed\u4e49\u4fe1\u606f\uff0c\u63d0\u5347\u4e2a\u6027\u5316\u65b0\u95fb\u63a8\u8350\u8d28\u91cf\u3002", "motivation": "\u73b0\u6709\u63a8\u8350\u7cfb\u7edf\u4f9d\u8d56\u6d45\u5c42\u5185\u5bb9\uff08\u5982\u6807\u9898\u548c\u6458\u8981\uff09\uff0c\u96be\u4ee5\u6355\u6349\u7528\u6237\u504f\u597d\u7684\u590d\u6742\u6027\u3002", "method": "\u63d0\u51faPNR-LLM\u65b9\u6cd5\uff0c\u5305\u542b\u65b0\u95fb\u589e\u5f3a\u6a21\u5757\uff08\u5229\u7528LLM\u751f\u6210\u8bed\u4e49\u548c\u5b9e\u4f53\u4fe1\u606f\uff09\u548c\u6ce8\u610f\u529b\u673a\u5236\u805a\u5408\u6570\u636e\u3002", "result": "\u5728MIND\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u4e14\u589e\u5f3a\u6a21\u5757\u53ef\u63d0\u5347\u5176\u4ed6\u6a21\u578b\u7684\u6027\u80fd\u3002", "conclusion": "PNR-LLM\u901a\u8fc7LLM\u589e\u5f3a\u5185\u5bb9\u8868\u793a\uff0c\u663e\u8457\u63d0\u5347\u63a8\u8350\u6548\u679c\uff0c\u5177\u6709\u901a\u7528\u6027\u3002"}}
{"id": "2504.20471", "pdf": "https://arxiv.org/pdf/2504.20471", "abs": "https://arxiv.org/abs/2504.20471", "authors": ["Baining Chen", "Yiming Zhang", "Yuqiao Han", "Ruyue Zhang", "Ruihuan Du", "Zhishuo Zhou", "Zhengdan Zhu", "Xun Liu", "Jiecheng Guo"], "title": "The Estimation of Continual Causal Effect for Dataset Shifting Streams", "categories": ["cs.LG", "cs.AI", "stat.ME"], "comment": null, "summary": "Causal effect estimation has been widely used in marketing optimization. The\nframework of an uplift model followed by a constrained optimization algorithm\nis popular in practice. To enhance performance in the online environment, the\nframework needs to be improved to address the complexities caused by temporal\ndataset shift. This paper focuses on capturing the dataset shift from user\nbehavior and domain distribution changing over time. We propose an Incremental\nCausal Effect with Proxy Knowledge Distillation (ICE-PKD) framework to tackle\nthis challenge. The ICE-PKD framework includes two components: (i) a\nmulti-treatment uplift network that eliminates confounding bias using\ncounterfactual regression; (ii) an incremental training strategy that adapts to\nthe temporal dataset shift by updating with the latest data and protects\ngeneralization via replay-based knowledge distillation. We also revisit the\nuplift modeling metrics and introduce a novel metric for more precise online\nevaluation in multiple treatment scenarios. Extensive experiments on both\nsimulated and online datasets show that the proposed framework achieves better\nperformance. The ICE-PKD framework has been deployed in the marketing system of\nHuaxiaozhu, a ride-hailing platform in China.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faICE-PKD\u6846\u67b6\uff0c\u901a\u8fc7\u589e\u91cf\u5b66\u4e60\u548c\u77e5\u8bc6\u84b8\u998f\u89e3\u51b3\u8425\u9500\u4f18\u5316\u4e2d\u7684\u56e0\u679c\u6548\u5e94\u4f30\u8ba1\u95ee\u9898\uff0c\u9002\u5e94\u65f6\u95f4\u6570\u636e\u504f\u79fb\u3002", "motivation": "\u89e3\u51b3\u5728\u7ebf\u73af\u5883\u4e2d\u56e0\u7528\u6237\u884c\u4e3a\u548c\u9886\u57df\u5206\u5e03\u968f\u65f6\u95f4\u53d8\u5316\u5bfc\u81f4\u7684\u6570\u636e\u504f\u79fb\u95ee\u9898\uff0c\u63d0\u5347\u56e0\u679c\u6548\u5e94\u4f30\u8ba1\u7684\u6027\u80fd\u3002", "method": "\u63d0\u51faICE-PKD\u6846\u67b6\uff0c\u5305\u62ec\u591a\u5904\u7406\u63d0\u5347\u7f51\u7edc\uff08\u6d88\u9664\u6df7\u6742\u504f\u5dee\uff09\u548c\u589e\u91cf\u8bad\u7ec3\u7b56\u7565\uff08\u9002\u5e94\u6570\u636e\u504f\u79fb\uff09\u3002", "result": "\u5728\u6a21\u62df\u548c\u5728\u7ebf\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u5df2\u90e8\u7f72\u4e8e\u534e\u590f\u51fa\u884c\u5e73\u53f0\u3002", "conclusion": "ICE-PKD\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u65f6\u95f4\u6570\u636e\u504f\u79fb\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u5728\u7ebf\u8425\u9500\u4f18\u5316\u7684\u6027\u80fd\u3002"}}
{"id": "2504.20482", "pdf": "https://arxiv.org/pdf/2504.20482", "abs": "https://arxiv.org/abs/2504.20482", "authors": ["Chao Li", "Changhua Zhou", "Jia Chen"], "title": "Group Relative Knowledge Distillation: Learning from Teacher's Relational Inductive Bias", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Knowledge distillation typically transfers knowledge from a teacher model to\na student model by minimizing differences between their output distributions.\nHowever, existing distillation approaches largely focus on mimicking absolute\nprobabilities and neglect the valuable relational inductive biases embedded in\nthe teacher's relative predictions, leading to exposure bias. In this paper, we\npropose Group Relative Knowledge Distillation (GRKD), a novel framework that\ndistills teacher knowledge by learning the relative ranking among classes,\nrather than directly fitting the absolute distribution. Specifically, we\nintroduce a group relative loss that encourages the student model to preserve\nthe pairwise preference orderings provided by the teacher's outputs. Extensive\nexperiments on classification benchmarks demonstrate that GRKD achieves\nsuperior generalization compared to existing methods, especially in tasks\nrequiring fine-grained class differentiation. Our method provides a new\nperspective on exploiting teacher knowledge, focusing on relational structure\nrather than absolute likelihood.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aGRKD\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u5173\u6ce8\u6559\u5e08\u6a21\u578b\u8f93\u51fa\u7684\u76f8\u5bf9\u6392\u540d\u800c\u975e\u7edd\u5bf9\u6982\u7387\uff0c\u6539\u8fdb\u4e86\u77e5\u8bc6\u84b8\u998f\u7684\u6548\u679c\u3002", "motivation": "\u73b0\u6709\u77e5\u8bc6\u84b8\u998f\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u7edd\u5bf9\u6982\u7387\u7684\u6a21\u4eff\uff0c\u5ffd\u7565\u4e86\u6559\u5e08\u6a21\u578b\u4e2d\u76f8\u5bf9\u9884\u6d4b\u7684\u5b9d\u8d35\u5173\u7cfb\u5f52\u7eb3\u504f\u5dee\uff0c\u5bfc\u81f4\u66b4\u9732\u504f\u5dee\u3002", "method": "\u63d0\u51fa\u4e86Group Relative Knowledge Distillation (GRKD)\u6846\u67b6\uff0c\u901a\u8fc7\u5f15\u5165\u7ec4\u76f8\u5bf9\u635f\u5931\u51fd\u6570\uff0c\u4f7f\u5b66\u751f\u6a21\u578b\u5b66\u4e60\u6559\u5e08\u8f93\u51fa\u7684\u7c7b\u522b\u76f8\u5bf9\u6392\u540d\u3002", "result": "\u5728\u5206\u7c7b\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cGRKD\u8868\u73b0\u51fa\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u5c24\u5176\u5728\u7ec6\u7c92\u5ea6\u5206\u7c7b\u4efb\u52a1\u4e2d\u6548\u679c\u663e\u8457\u3002", "conclusion": "GRKD\u4e3a\u5229\u7528\u6559\u5e08\u77e5\u8bc6\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\uff0c\u5f3a\u8c03\u5173\u7cfb\u7ed3\u6784\u800c\u975e\u7edd\u5bf9\u6982\u7387\uff0c\u63d0\u5347\u4e86\u77e5\u8bc6\u84b8\u998f\u7684\u6548\u679c\u3002"}}
{"id": "2504.20493", "pdf": "https://arxiv.org/pdf/2504.20493", "abs": "https://arxiv.org/abs/2504.20493", "authors": ["Yu Cui", "Yujun Cai", "Yiwei Wang"], "title": "Token-Efficient Prompt Injection Attack: Provoking Cessation in LLM Reasoning via Adaptive Token Compression", "categories": ["cs.CR", "cs.AI"], "comment": null, "summary": "While reasoning large language models (LLMs) demonstrate remarkable\nperformance across various tasks, they also contain notable security\nvulnerabilities. Recent research has uncovered a \"thinking-stopped\"\nvulnerability in DeepSeek-R1, where model-generated reasoning tokens can\nforcibly interrupt the inference process, resulting in empty responses that\ncompromise LLM-integrated applications. However, existing methods triggering\nthis vulnerability require complex mathematical word problems with long\nprompts--even exceeding 5,000 tokens. To reduce the token cost and formally\ndefine this vulnerability, we propose a novel prompt injection attack named\n\"Reasoning Interruption Attack\", based on adaptive token compression. We\ndemonstrate that simple standalone arithmetic tasks can effectively trigger\nthis vulnerability, and the prompts based on such tasks exhibit simpler logical\nstructures than mathematical word problems. We develop a systematic approach to\nefficiently collect attack prompts and an adaptive token compression framework\nthat utilizes LLMs to automatically compress these prompts. Experiments show\nour compression framework significantly reduces prompt length while maintaining\neffective attack capabilities. We further investigate the attack's performance\nvia output prefix and analyze the underlying causes of the vulnerability,\nproviding valuable insights for improving security in reasoning LLMs.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u201c\u63a8\u7406\u4e2d\u65ad\u653b\u51fb\u201d\u7684\u65b0\u578b\u63d0\u793a\u6ce8\u5165\u653b\u51fb\u65b9\u6cd5\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u4ee4\u724c\u538b\u7f29\u964d\u4f4e\u89e6\u53d1\u6f0f\u6d1e\u7684\u63d0\u793a\u957f\u5ea6\uff0c\u5e76\u5206\u6790\u4e86\u6f0f\u6d1e\u7684\u6839\u6e90\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u63a8\u7406\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5b58\u5728\u5b89\u5168\u6f0f\u6d1e\uff0c\u5982\u201c\u601d\u7ef4\u505c\u6b62\u201d\u6f0f\u6d1e\u3002\u73b0\u6709\u89e6\u53d1\u65b9\u6cd5\u9700\u8981\u590d\u6742\u4e14\u5197\u957f\u7684\u63d0\u793a\uff0c\u4e9f\u9700\u66f4\u9ad8\u6548\u7684\u653b\u51fb\u65b9\u5f0f\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u81ea\u9002\u5e94\u4ee4\u724c\u538b\u7f29\u7684\u201c\u63a8\u7406\u4e2d\u65ad\u653b\u51fb\u201d\uff0c\u5229\u7528\u7b80\u5355\u7b97\u672f\u4efb\u52a1\u89e6\u53d1\u6f0f\u6d1e\uff0c\u5e76\u5f00\u53d1\u7cfb\u7edf\u5316\u65b9\u6cd5\u6536\u96c6\u653b\u51fb\u63d0\u793a\u53ca\u538b\u7f29\u6846\u67b6\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u538b\u7f29\u6846\u67b6\u663e\u8457\u7f29\u77ed\u63d0\u793a\u957f\u5ea6\u4e14\u4fdd\u6301\u653b\u51fb\u6548\u679c\uff0c\u540c\u65f6\u901a\u8fc7\u8f93\u51fa\u524d\u7f00\u5206\u6790\u6f0f\u6d1e\u6027\u80fd\u53ca\u6210\u56e0\u3002", "conclusion": "\u7814\u7a76\u4e3a\u63d0\u5347\u63a8\u7406LLMs\u7684\u5b89\u5168\u6027\u63d0\u4f9b\u4e86\u91cd\u8981\u89c1\u89e3\uff0c\u5e76\u5c55\u793a\u4e86\u9ad8\u6548\u653b\u51fb\u65b9\u6cd5\u7684\u53ef\u884c\u6027\u3002"}}
{"id": "2504.20520", "pdf": "https://arxiv.org/pdf/2504.20520", "abs": "https://arxiv.org/abs/2504.20520", "authors": ["Haowen Sun", "Han Wang", "Chengzhong Ma", "Shaolong Zhang", "Jiawei Ye", "Xingyu Chen", "Xuguang Lan"], "title": "PRISM: Projection-based Reward Integration for Scene-Aware Real-to-Sim-to-Real Transfer with Few Demonstrations", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "Learning from few demonstrations to develop policies robust to variations in\nrobot initial positions and object poses is a problem of significant practical\ninterest in robotics. Compared to imitation learning, which often struggles to\ngeneralize from limited samples, reinforcement learning (RL) can autonomously\nexplore to obtain robust behaviors. Training RL agents through direct\ninteraction with the real world is often impractical and unsafe, while building\nsimulation environments requires extensive manual effort, such as designing\nscenes and crafting task-specific reward functions. To address these\nchallenges, we propose an integrated real-to-sim-to-real pipeline that\nconstructs simulation environments based on expert demonstrations by\nidentifying scene objects from images and retrieving their corresponding 3D\nmodels from existing libraries. We introduce a projection-based reward model\nfor RL policy training that is supervised by a vision-language model (VLM)\nusing human-guided object projection relationships as prompts, with the policy\nfurther fine-tuned using expert demonstrations. In general, our work focuses on\nthe construction of simulation environments and RL-based policy training,\nultimately enabling the deployment of reliable robotic control policies in\nreal-world scenarios.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u771f\u5b9e\u5230\u4eff\u771f\u518d\u5230\u771f\u5b9e\u7684\u6d41\u7a0b\uff0c\u901a\u8fc7\u4e13\u5bb6\u6f14\u793a\u6784\u5efa\u4eff\u771f\u73af\u5883\uff0c\u5e76\u5229\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u76d1\u7763\u7684\u5956\u52b1\u6a21\u578b\u8bad\u7ec3RL\u7b56\u7565\uff0c\u6700\u7ec8\u5b9e\u73b0\u53ef\u9760\u7684\u673a\u5668\u4eba\u63a7\u5236\u7b56\u7565\u3002", "motivation": "\u89e3\u51b3\u673a\u5668\u4eba\u521d\u59cb\u4f4d\u7f6e\u548c\u7269\u4f53\u59ff\u6001\u53d8\u5316\u4e0b\u7684\u7b56\u7565\u9c81\u68d2\u6027\u95ee\u9898\uff0c\u540c\u65f6\u907f\u514d\u76f4\u63a5\u4ea4\u4e92\u7684\u4e0d\u5b89\u5168\u6027\u548c\u4eff\u771f\u73af\u5883\u6784\u5efa\u7684\u9ad8\u6210\u672c\u3002", "method": "\u901a\u8fc7\u4e13\u5bb6\u6f14\u793a\u6784\u5efa\u4eff\u771f\u73af\u5883\uff0c\u5229\u7528VLM\u76d1\u7763\u7684\u6295\u5f71\u5956\u52b1\u6a21\u578b\u8bad\u7ec3RL\u7b56\u7565\uff0c\u5e76\u7ed3\u5408\u4e13\u5bb6\u6f14\u793a\u8fdb\u884c\u5fae\u8c03\u3002", "result": "\u5b9e\u73b0\u4e86\u5728\u771f\u5b9e\u573a\u666f\u4e2d\u90e8\u7f72\u53ef\u9760\u7684\u673a\u5668\u4eba\u63a7\u5236\u7b56\u7565\u3002", "conclusion": "\u63d0\u51fa\u7684\u6d41\u7a0b\u6709\u6548\u89e3\u51b3\u4e86\u4eff\u771f\u73af\u5883\u6784\u5efa\u548c\u7b56\u7565\u8bad\u7ec3\u7684\u95ee\u9898\uff0c\u4e3a\u673a\u5668\u4eba\u63a7\u5236\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2504.20560", "pdf": "https://arxiv.org/pdf/2504.20560", "abs": "https://arxiv.org/abs/2504.20560", "authors": ["Francisco Sede\u00f1o", "Jamal Toutouh", "Francisco Chicano"], "title": "Generate more than one child in your co-evolutionary semi-supervised learning GAN", "categories": ["cs.NE", "cs.AI", "cs.LG"], "comment": "Submitted to The Leading European Event on Bio-Inspired AI (EvoStar\n  2025)", "summary": "Generative Adversarial Networks (GANs) are very useful methods to address\nsemi-supervised learning (SSL) datasets, thanks to their ability to generate\nsamples similar to real data. This approach, called SSL-GAN has attracted many\nresearchers in the last decade. Evolutionary algorithms have been used to guide\nthe evolution and training of SSL-GANs with great success. In particular,\nseveral co-evolutionary approaches have been applied where the two networks of\na GAN (the generator and the discriminator) are evolved in separate\npopulations. The co-evolutionary approaches published to date assume some\nspatial structure of the populations, based on the ideas of cellular\nevolutionary algorithms. They also create one single individual per generation\nand follow a generational replacement strategy in the evolution. In this paper,\nwe re-consider those algorithmic design decisions and propose a new\nco-evolutionary approach, called Co-evolutionary Elitist SSL-GAN (CE-SSLGAN),\nwith panmictic population, elitist replacement, and more than one individual in\nthe offspring. We evaluate the performance of our proposed method using three\nstandard benchmark datasets. The results show that creating more than one\noffspring per population and using elitism improves the results in comparison\nwith a classical SSL-GAN.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u534f\u540c\u8fdb\u5316\u65b9\u6cd5CE-SSLGAN\uff0c\u6539\u8fdb\u4e86\u4f20\u7edfSSL-GAN\u7684\u8bbe\u8ba1\uff0c\u901a\u8fc7\u5f15\u5165\u6cdb\u79cd\u7fa4\u3001\u7cbe\u82f1\u66ff\u6362\u548c\u591a\u5b50\u4ee3\u7b56\u7565\uff0c\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "\u4f20\u7edfSSL-GAN\u7684\u534f\u540c\u8fdb\u5316\u65b9\u6cd5\u57fa\u4e8e\u7a7a\u95f4\u7ed3\u6784\u548c\u5355\u5b50\u4ee3\u7b56\u7565\uff0c\u9650\u5236\u4e86\u6027\u80fd\u63d0\u5347\u3002\u672c\u6587\u65e8\u5728\u4f18\u5316\u8fd9\u4e9b\u8bbe\u8ba1\u51b3\u7b56\u3002", "method": "\u63d0\u51faCE-SSLGAN\u65b9\u6cd5\uff0c\u91c7\u7528\u6cdb\u79cd\u7fa4\u3001\u7cbe\u82f1\u66ff\u6362\u548c\u591a\u5b50\u4ee3\u7b56\u7565\uff0c\u5e76\u5728\u4e09\u4e2a\u6807\u51c6\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\u6027\u80fd\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u591a\u5b50\u4ee3\u548c\u7cbe\u82f1\u66ff\u6362\u7b56\u7565\u663e\u8457\u4f18\u4e8e\u4f20\u7edfSSL-GAN\u3002", "conclusion": "CE-SSLGAN\u901a\u8fc7\u6539\u8fdb\u534f\u540c\u8fdb\u5316\u7b56\u7565\uff0c\u6709\u6548\u63d0\u5347\u4e86SSL-GAN\u7684\u6027\u80fd\u3002"}}
{"id": "2504.20566", "pdf": "https://arxiv.org/pdf/2504.20566", "abs": "https://arxiv.org/abs/2504.20566", "authors": ["Shunjie Wen", "Thomas Heinis", "Dong-Wan Choi"], "title": "Inclusive Training Separation and Implicit Knowledge Interaction for Balanced Online Class-Incremental Learning", "categories": ["cs.LG", "cs.AI"], "comment": "Under review", "summary": "Online class-incremental learning (OCIL) focuses on gradually learning new\nclasses (called plasticity) from a stream of data in a single-pass, while\nconcurrently preserving knowledge of previously learned classes (called\nstability). The primary challenge in OCIL lies in maintaining a good balance\nbetween the knowledge of old and new classes within the continually updated\nmodel. Most existing methods rely on explicit knowledge interaction through\nexperience replay, and often employ exclusive training separation to address\nbias problems. Nevertheless, it still remains a big challenge to achieve a\nwell-balanced learner, as these methods often exhibit either reduced plasticity\nor limited stability due to difficulties in continually integrating knowledge\nin the OCIL setting. In this paper, we propose a novel replay-based method,\ncalled Balanced Online Incremental Learning (BOIL), which can achieve both high\nplasticity and stability, thus ensuring more balanced performance in OCIL. Our\nBOIL method proposes an inclusive training separation strategy using dual\nclassifiers so that knowledge from both old and new classes can effectively be\nintegrated into the model, while introducing implicit approaches for\ntransferring knowledge across the two classifiers. Extensive experimental\nevaluations over three widely-used OCIL benchmark datasets demonstrate the\nsuperiority of BOIL, showing more balanced yet better performance compared to\nstate-of-the-art replay-based OCIL methods.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aBOIL\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u53cc\u5206\u7c7b\u5668\u548c\u5305\u5bb9\u6027\u8bad\u7ec3\u5206\u79bb\u7b56\u7565\uff0c\u5728\u5728\u7ebf\u7c7b\u589e\u91cf\u5b66\u4e60\u4e2d\u5b9e\u73b0\u4e86\u9ad8\u53ef\u5851\u6027\u548c\u7a33\u5b9a\u6027\u7684\u5e73\u8861\u3002", "motivation": "\u5728\u7ebf\u7c7b\u589e\u91cf\u5b66\u4e60\uff08OCIL\uff09\u4e2d\uff0c\u5e73\u8861\u65b0\u65e7\u7c7b\u77e5\u8bc6\u662f\u4e00\u4e2a\u4e3b\u8981\u6311\u6218\uff0c\u73b0\u6709\u65b9\u6cd5\u5f80\u5f80\u96be\u4ee5\u517c\u987e\u53ef\u5851\u6027\u548c\u7a33\u5b9a\u6027\u3002", "method": "\u63d0\u51faBOIL\u65b9\u6cd5\uff0c\u91c7\u7528\u53cc\u5206\u7c7b\u5668\u548c\u5305\u5bb9\u6027\u8bad\u7ec3\u5206\u79bb\u7b56\u7565\uff0c\u901a\u8fc7\u9690\u5f0f\u77e5\u8bc6\u8f6c\u79fb\u5b9e\u73b0\u77e5\u8bc6\u6574\u5408\u3002", "result": "\u5728\u4e09\u4e2a\u5e7f\u6cdb\u4f7f\u7528\u7684OCIL\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\uff0cBOIL\u8868\u73b0\u51fa\u66f4\u5e73\u8861\u4e14\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u7684\u6027\u80fd\u3002", "conclusion": "BOIL\u65b9\u6cd5\u5728OCIL\u4e2d\u5b9e\u73b0\u4e86\u9ad8\u53ef\u5851\u6027\u548c\u7a33\u5b9a\u6027\u7684\u5e73\u8861\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2504.20571", "pdf": "https://arxiv.org/pdf/2504.20571", "abs": "https://arxiv.org/abs/2504.20571", "authors": ["Yiping Wang", "Qing Yang", "Zhiyuan Zeng", "Liliang Ren", "Lucas Liu", "Baolin Peng", "Hao Cheng", "Xuehai He", "Kuan Wang", "Jianfeng Gao", "Weizhu Chen", "Shuohang Wang", "Simon Shaolei Du", "Yelong Shen"], "title": "Reinforcement Learning for Reasoning in Large Language Models with One Training Example", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "28 pages, 12 figures, link: https://github.com/ypwang61/One-Shot-RLVR", "summary": "We show that reinforcement learning with verifiable reward using one training\nexample (1-shot RLVR) is effective in incentivizing the math reasoning\ncapabilities of large language models (LLMs). Applying RLVR to the base model\nQwen2.5-Math-1.5B, we identify a single example that elevates model performance\non MATH500 from 36.0% to 73.6%, and improves the average performance across six\ncommon mathematical reasoning benchmarks from 17.6% to 35.7%. This result\nmatches the performance obtained using the 1.2k DeepScaleR subset (MATH500:\n73.6%, average: 35.9%), which includes the aforementioned example. Similar\nsubstantial improvements are observed across various models (Qwen2.5-Math-7B,\nLlama3.2-3B-Instruct, DeepSeek-R1-Distill-Qwen-1.5B), RL algorithms (GRPO and\nPPO), and different math examples (many of which yield approximately 30% or\ngreater improvement on MATH500 when employed as a single training example). In\naddition, we identify some interesting phenomena during 1-shot RLVR, including\ncross-domain generalization, increased frequency of self-reflection, and\nsustained test performance improvement even after the training accuracy has\nsaturated, a phenomenon we term post-saturation generalization. Moreover, we\nverify that the effectiveness of 1-shot RLVR primarily arises from the policy\ngradient loss, distinguishing it from the \"grokking\" phenomenon. We also show\nthe critical role of promoting exploration (e.g., by adding entropy loss with\nan appropriate coefficient) in 1-shot RLVR training. As a bonus, we observe\nthat applying entropy loss alone, without any outcome reward, significantly\nenhances Qwen2.5-Math-1.5B's performance on MATH500 by 27.4%. These findings\ncan inspire future work on RLVR data efficiency and encourage a re-examination\nof both recent progress and the underlying mechanisms in RLVR. Our code, model,\nand data are open source at https://github.com/ypwang61/One-Shot-RLVR", "AI": {"tldr": "1-shot RLVR\u663e\u8457\u63d0\u5347LLMs\u7684\u6570\u5b66\u63a8\u7406\u80fd\u529b\uff0c\u5355\u4e2a\u8bad\u7ec3\u793a\u4f8b\u5373\u53ef\u5927\u5e45\u63d0\u9ad8\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u63a2\u7d22\u5982\u4f55\u901a\u8fc7\u5c11\u91cf\u793a\u4f8b\uff081-shot\uff09\u5f3a\u5316\u5b66\u4e60\u9a8c\u8bc1\u5956\u52b1\uff08RLVR\uff09\u63d0\u5347\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u6570\u5b66\u63a8\u7406\u80fd\u529b\u3002", "method": "\u5e94\u7528RLVR\u4e8e\u4e0d\u540c\u6a21\u578b\uff08\u5982Qwen2.5-Math-1.5B\uff09\uff0c\u4f7f\u7528\u5355\u4e2a\u8bad\u7ec3\u793a\u4f8b\uff0c\u7ed3\u5408GRPO\u548cPPO\u7b97\u6cd5\uff0c\u5e76\u5206\u6790\u63a2\u7d22\u7b56\u7565\uff08\u5982\u71b5\u635f\u5931\uff09\u7684\u4f5c\u7528\u3002", "result": "\u5355\u4e2a\u793a\u4f8b\u5c06MATH500\u6027\u80fd\u4ece36.0%\u63d0\u5347\u81f373.6%\uff0c\u5e73\u5747\u6027\u80fd\u4ece17.6%\u63d0\u5347\u81f335.7%\uff0c\u5e76\u89c2\u5bdf\u5230\u8de8\u57df\u6cdb\u5316\u548c\u540e\u9971\u548c\u6cdb\u5316\u73b0\u8c61\u3002", "conclusion": "1-shot RLVR\u9ad8\u6548\u4e14\u6570\u636e\u5229\u7528\u7387\u9ad8\uff0c\u5176\u6548\u679c\u4e3b\u8981\u6e90\u4e8e\u7b56\u7565\u68af\u5ea6\u635f\u5931\uff0c\u672a\u6765\u53ef\u8fdb\u4e00\u6b65\u63a2\u7d22RLVR\u673a\u5236\u548c\u6570\u636e\u6548\u7387\u3002"}}
{"id": "2504.20610", "pdf": "https://arxiv.org/pdf/2504.20610", "abs": "https://arxiv.org/abs/2504.20610", "authors": ["Michele Garetto", "Alessandro Cornacchia", "Franco Galante", "Emilio Leonardi", "Alessandro Nordio", "Alberto Tarable"], "title": "Information Retrieval in the Age of Generative AI: The RGB Model", "categories": ["cs.IR", "cs.AI", "cs.PF"], "comment": "To be presented at ACM SIGIR 25", "summary": "The advent of Large Language Models (LLMs) and generative AI is fundamentally\ntransforming information retrieval and processing on the Internet, bringing\nboth great potential and significant concerns regarding content authenticity\nand reliability. This paper presents a novel quantitative approach to shed\nlight on the complex information dynamics arising from the growing use of\ngenerative AI tools. Despite their significant impact on the digital ecosystem,\nthese dynamics remain largely uncharted and poorly understood. We propose a\nstochastic model to characterize the generation, indexing, and dissemination of\ninformation in response to new topics. This scenario particularly challenges\ncurrent LLMs, which often rely on real-time Retrieval-Augmented Generation\n(RAG) techniques to overcome their static knowledge limitations. Our findings\nsuggest that the rapid pace of generative AI adoption, combined with increasing\nuser reliance, can outpace human verification, escalating the risk of\ninaccurate information proliferation across digital resources. An in-depth\nanalysis of Stack Exchange data confirms that high-quality answers inevitably\nrequire substantial time and human effort to emerge. This underscores the\nconsiderable risks associated with generating persuasive text in response to\nnew questions and highlights the critical need for responsible development and\ndeployment of future generative AI tools.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5b9a\u91cf\u65b9\u6cd5\uff0c\u7814\u7a76\u751f\u6210\u5f0fAI\u5de5\u5177\u4f7f\u7528\u589e\u52a0\u5e26\u6765\u7684\u4fe1\u606f\u52a8\u6001\u53d8\u5316\uff0c\u53d1\u73b0\u5176\u5feb\u901f\u666e\u53ca\u53ef\u80fd\u52a0\u5267\u4e0d\u51c6\u786e\u4fe1\u606f\u7684\u4f20\u64ad\u98ce\u9669\u3002", "motivation": "\u7814\u7a76\u751f\u6210\u5f0fAI\u5de5\u5177\u5bf9\u4fe1\u606f\u68c0\u7d22\u548c\u5904\u7406\u7684\u6df1\u8fdc\u5f71\u54cd\uff0c\u5c24\u5176\u662f\u5185\u5bb9\u771f\u5b9e\u6027\u548c\u53ef\u9760\u6027\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u968f\u673a\u6a21\u578b\uff0c\u63cf\u8ff0\u4fe1\u606f\u751f\u6210\u3001\u7d22\u5f15\u548c\u4f20\u64ad\u7684\u52a8\u6001\u8fc7\u7a0b\uff0c\u5e76\u7ed3\u5408Stack Exchange\u6570\u636e\u8fdb\u884c\u9a8c\u8bc1\u3002", "result": "\u751f\u6210\u5f0fAI\u7684\u5feb\u901f\u666e\u53ca\u53ef\u80fd\u8d85\u8d8a\u4eba\u5de5\u9a8c\u8bc1\u901f\u5ea6\uff0c\u589e\u52a0\u4e0d\u51c6\u786e\u4fe1\u606f\u4f20\u64ad\u7684\u98ce\u9669\u3002", "conclusion": "\u5f3a\u8c03\u672a\u6765\u751f\u6210\u5f0fAI\u5de5\u5177\u9700\u8d1f\u8d23\u4efb\u5730\u5f00\u53d1\u548c\u90e8\u7f72\uff0c\u4ee5\u51cf\u5c11\u6f5c\u5728\u98ce\u9669\u3002"}}
{"id": "2504.20612", "pdf": "https://arxiv.org/pdf/2504.20612", "abs": "https://arxiv.org/abs/2504.20612", "authors": ["Swaroop Dora", "Deven Lunkad", "Naziya Aslam", "S. Venkatesan", "Sandeep Kumar Shukla"], "title": "The Hidden Risks of LLM-Generated Web Application Code: A Security-Centric Evaluation of Code Generation Capabilities in Large Language Models", "categories": ["cs.CR", "cs.AI", "cs.ET"], "comment": "9 pages", "summary": "The rapid advancement of Large Language Models (LLMs) has enhanced software\ndevelopment processes, minimizing the time and effort required for coding and\nenhancing developer productivity. However, despite their potential benefits,\ncode generated by LLMs has been shown to generate insecure code in controlled\nenvironments, raising critical concerns about their reliability and security in\nreal-world applications. This paper uses predefined security parameters to\nevaluate the security compliance of LLM-generated code across multiple models,\nsuch as ChatGPT, DeepSeek, Claude, Gemini and Grok. The analysis reveals\ncritical vulnerabilities in authentication mechanisms, session management,\ninput validation and HTTP security headers. Although some models implement\nsecurity measures to a limited extent, none fully align with industry best\npractices, highlighting the associated risks in automated software development.\nOur findings underscore that human expertise is crucial to ensure secure\nsoftware deployment or review of LLM-generated code. Also, there is a need for\nrobust security assessment frameworks to enhance the reliability of\nLLM-generated code in real-world applications.", "AI": {"tldr": "\u8bba\u6587\u8bc4\u4f30\u4e86\u591a\u79cd\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08\u5982ChatGPT\u3001DeepSeek\u7b49\uff09\u751f\u6210\u7684\u4ee3\u7801\u7684\u5b89\u5168\u6027\uff0c\u53d1\u73b0\u666e\u904d\u5b58\u5728\u6f0f\u6d1e\uff0c\u5f3a\u8c03\u4eba\u7c7b\u4e13\u5bb6\u5ba1\u67e5\u7684\u5fc5\u8981\u6027\u3002", "motivation": "\u5c3d\u7ba1LLM\u63d0\u5347\u4e86\u5f00\u53d1\u6548\u7387\uff0c\u4f46\u5176\u751f\u6210\u7684\u4ee3\u7801\u5728\u5b89\u5168\u6027\u4e0a\u5b58\u5728\u9690\u60a3\uff0c\u9700\u8bc4\u4f30\u5176\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u53ef\u9760\u6027\u3002", "method": "\u4f7f\u7528\u9884\u5b9a\u4e49\u7684\u5b89\u5168\u53c2\u6570\u5bf9\u591a\u79cdLLM\u751f\u6210\u7684\u4ee3\u7801\u8fdb\u884c\u5b89\u5168\u6027\u8bc4\u4f30\u3002", "result": "\u53d1\u73b0\u8ba4\u8bc1\u673a\u5236\u3001\u4f1a\u8bdd\u7ba1\u7406\u7b49\u5173\u952e\u6f0f\u6d1e\uff0c\u65e0\u6a21\u578b\u5b8c\u5168\u7b26\u5408\u884c\u4e1a\u6700\u4f73\u5b9e\u8df5\u3002", "conclusion": "\u4eba\u7c7b\u4e13\u5bb6\u5ba1\u67e5\u548c\u66f4\u5f3a\u5927\u7684\u5b89\u5168\u8bc4\u4f30\u6846\u67b6\u5bf9\u786e\u4fddLLM\u751f\u6210\u4ee3\u7801\u7684\u5b89\u5168\u6027\u81f3\u5173\u91cd\u8981\u3002"}}
{"id": "2504.20625", "pdf": "https://arxiv.org/pdf/2504.20625", "abs": "https://arxiv.org/abs/2504.20625", "authors": ["Sagi Della Torre", "Mirco Pezzoli", "Fabio Antonacci", "Sharon Gannot"], "title": "DiffusionRIR: Room Impulse Response Interpolation using Diffusion Models", "categories": ["cs.SD", "cs.AI", "eess.AS"], "comment": null, "summary": "Room Impulse Responses (RIRs) characterize acoustic environments and are\ncrucial in multiple audio signal processing tasks. High-quality RIR estimates\ndrive applications such as virtual microphones, sound source localization,\naugmented reality, and data augmentation. However, obtaining RIR measurements\nwith high spatial resolution is resource-intensive, making it impractical for\nlarge spaces or when dense sampling is required. This research addresses the\nchallenge of estimating RIRs at unmeasured locations within a room using\nDenoising Diffusion Probabilistic Models (DDPM). Our method leverages the\nanalogy between RIR matrices and image inpainting, transforming RIR data into a\nformat suitable for diffusion-based reconstruction.\n  Using simulated RIR data based on the image method, we demonstrate our\napproach's effectiveness on microphone arrays of different curvatures, from\nlinear to semi-circular. Our method successfully reconstructs missing RIRs,\neven in large gaps between microphones. Under these conditions, it achieves\naccurate reconstruction, significantly outperforming baseline Spline Cubic\nInterpolation in terms of Normalized Mean Square Error and Cosine Distance\nbetween actual and interpolated RIRs.\n  This research highlights the potential of using generative models for\neffective RIR interpolation, paving the way for generating additional data from\nlimited real-world measurements.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u53bb\u566a\u6269\u6563\u6982\u7387\u6a21\u578b\uff08DDPM\uff09\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u4f30\u8ba1\u623f\u95f4\u5185\u672a\u6d4b\u91cf\u4f4d\u7f6e\u7684\u623f\u95f4\u8109\u51b2\u54cd\u5e94\uff08RIR\uff09\uff0c\u89e3\u51b3\u4e86\u9ad8\u7a7a\u95f4\u5206\u8fa8\u7387RIR\u6d4b\u91cf\u8d44\u6e90\u5bc6\u96c6\u7684\u95ee\u9898\u3002", "motivation": "\u9ad8\u7a7a\u95f4\u5206\u8fa8\u7387\u7684RIR\u6d4b\u91cf\u8d44\u6e90\u5bc6\u96c6\u4e14\u4e0d\u5207\u5b9e\u9645\uff0c\u5c24\u5176\u662f\u5728\u5927\u7a7a\u95f4\u6216\u9700\u8981\u5bc6\u96c6\u91c7\u6837\u65f6\u3002\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u751f\u6210\u6a21\u578b\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u5c06RIR\u6570\u636e\u8f6c\u5316\u4e3a\u9002\u5408\u6269\u6563\u6a21\u578b\u91cd\u5efa\u7684\u683c\u5f0f\uff0c\u5229\u7528DDPM\u8fdb\u884c\u7f3a\u5931RIR\u7684\u4f30\u8ba1\uff0c\u5e76\u5728\u4e0d\u540c\u66f2\u7387\u7684\u9ea6\u514b\u98ce\u9635\u5217\u4e0a\u9a8c\u8bc1\u3002", "result": "\u65b9\u6cd5\u5728\u7f3a\u5931RIR\u7684\u91cd\u5efa\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u6837\u6761\u4e09\u6b21\u63d2\u503c\u6cd5\uff0c\u5f52\u4e00\u5316\u5747\u65b9\u8bef\u5dee\u548c\u4f59\u5f26\u8ddd\u79bb\u6307\u6807\u5747\u66f4\u4f18\u3002", "conclusion": "\u7814\u7a76\u5c55\u793a\u4e86\u751f\u6210\u6a21\u578b\u5728RIR\u63d2\u503c\u4e2d\u7684\u6f5c\u529b\uff0c\u4e3a\u4ece\u6709\u9650\u5b9e\u6d4b\u6570\u636e\u751f\u6210\u66f4\u591a\u6570\u636e\u63d0\u4f9b\u4e86\u53ef\u80fd\u3002"}}
{"id": "2504.20634", "pdf": "https://arxiv.org/pdf/2504.20634", "abs": "https://arxiv.org/abs/2504.20634", "authors": ["Andrew Fitzgibbon", "Stephen Felix"], "title": "On Stochastic Rounding with Few Random Bits", "categories": ["math.NA", "cs.AI", "cs.LG", "cs.MS", "cs.NA"], "comment": "Published at ARITH 2025", "summary": "Large-scale numerical computations make increasing use of low-precision (LP)\nfloating point formats and mixed precision arithmetic, which can be enhanced by\nthe technique of stochastic rounding (SR), that is, rounding an intermediate\nhigh-precision value up or down randomly as a function of the value's distance\nto the two rounding candidates. Stochastic rounding requires, in addition to\nthe high-precision input value, a source of random bits. As the provision of\nhigh-quality random bits is an additional computational cost, it is of interest\nto require as few bits as possible while maintaining the desirable properties\nof SR in a given computation, or computational domain. This paper examines a\nnumber of possible implementations of few-bit stochastic rounding (FBSR), and\nshows how several natural implementations can introduce sometimes significant\nbias into the rounding process, which are not present in the case of\ninfinite-bit, infinite-precision examinations of these implementations. The\npaper explores the impact of these biases in machine learning examples, and\nhence opens another class of configuration parameters of which practitioners\nshould be aware when developing or adopting low-precision floating point. Code\nis available at\nhttp://github.com/graphcore-research/arith25-stochastic-rounding.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u4f4e\u7cbe\u5ea6\u6d6e\u70b9\u8ba1\u7b97\u4e2d\u4f7f\u7528\u7684\u5c11\u4f4d\u968f\u673a\u820d\u5165\uff08FBSR\uff09\u65b9\u6cd5\uff0c\u5206\u6790\u4e86\u5176\u53ef\u80fd\u5f15\u5165\u7684\u504f\u5dee\u53ca\u5176\u5bf9\u673a\u5668\u5b66\u4e60\u7684\u5f71\u54cd\u3002", "motivation": "\u4f4e\u7cbe\u5ea6\u6d6e\u70b9\u683c\u5f0f\u548c\u6df7\u5408\u7cbe\u5ea6\u8ba1\u7b97\u5728\u5927\u89c4\u6a21\u6570\u503c\u8ba1\u7b97\u4e2d\u65e5\u76ca\u666e\u53ca\uff0c\u968f\u673a\u820d\u5165\uff08SR\uff09\u6280\u672f\u53ef\u589e\u5f3a\u5176\u6027\u80fd\u3002\u7136\u800c\uff0c\u9ad8\u8d28\u91cf\u7684\u968f\u673a\u4f4d\u751f\u6210\u6210\u672c\u9ad8\uff0c\u56e0\u6b64\u9700\u8981\u7814\u7a76\u5982\u4f55\u5728\u5c11\u4f4d\u60c5\u51b5\u4e0b\u4fdd\u6301SR\u7684\u4f18\u826f\u7279\u6027\u3002", "method": "\u7814\u7a76\u4e86\u51e0\u79cd\u5c11\u4f4d\u968f\u673a\u820d\u5165\uff08FBSR\uff09\u7684\u5b9e\u73b0\u65b9\u5f0f\uff0c\u5206\u6790\u4e86\u8fd9\u4e9b\u5b9e\u73b0\u53ef\u80fd\u5f15\u5165\u7684\u504f\u5dee\u3002", "result": "\u53d1\u73b0\u67d0\u4e9b\u81ea\u7136\u5b9e\u73b0\u4f1a\u5f15\u5165\u663e\u8457\u504f\u5dee\uff0c\u8fd9\u4e9b\u504f\u5dee\u5728\u65e0\u9650\u4f4d\u60c5\u51b5\u4e0b\u4e0d\u5b58\u5728\u3002\u901a\u8fc7\u673a\u5668\u5b66\u4e60\u793a\u4f8b\u5c55\u793a\u4e86\u8fd9\u4e9b\u504f\u5dee\u7684\u5f71\u54cd\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u4e3a\u5f00\u53d1\u6216\u91c7\u7528\u4f4e\u7cbe\u5ea6\u6d6e\u70b9\u8ba1\u7b97\u7684\u5b9e\u8df5\u8005\u63d0\u4f9b\u4e86\u65b0\u7684\u914d\u7f6e\u53c2\u6570\u53c2\u8003\u3002\u4ee3\u7801\u5df2\u5f00\u6e90\u3002"}}
{"id": "2504.20643", "pdf": "https://arxiv.org/pdf/2504.20643", "abs": "https://arxiv.org/abs/2504.20643", "authors": ["Moran Mizrahi", "Chen Shani", "Gabriel Stanovsky", "Dan Jurafsky", "Dafna Shahaf"], "title": "Cooking Up Creativity: A Cognitively-Inspired Approach for Enhancing LLM Creativity through Structured Representations", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "10 pages, 8 figures", "summary": "Large Language Models (LLMs) excel at countless tasks, yet struggle with\ncreativity. In this paper, we introduce a novel approach that couples LLMs with\nstructured representations and cognitively inspired manipulations to generate\nmore creative and diverse ideas. Our notion of creativity goes beyond\nsuperficial token-level variations; rather, we explicitly recombine structured\nrepresentations of existing ideas, allowing our algorithm to effectively\nexplore the more abstract landscape of ideas. We demonstrate our approach in\nthe culinary domain with DishCOVER, a model that generates creative recipes.\nExperiments comparing our model's results to those of GPT-4o show greater\ndiversity. Domain expert evaluations reveal that our outputs, which are mostly\ncoherent and feasible culinary creations, significantly surpass GPT-4o in terms\nof novelty, thus outperforming it in creative generation. We hope our work\ninspires further research into structured creativity in AI.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408LLMs\u4e0e\u7ed3\u6784\u5316\u8868\u793a\u548c\u8ba4\u77e5\u542f\u53d1\u64cd\u4f5c\u7684\u65b9\u6cd5\uff0c\u4ee5\u751f\u6210\u66f4\u5177\u521b\u9020\u6027\u548c\u591a\u6837\u6027\u7684\u60f3\u6cd5\uff0c\u5e76\u5728\u70f9\u996a\u9886\u57df\u9a8c\u8bc1\u4e86\u5176\u6548\u679c\u3002", "motivation": "\u5c3d\u7ba1LLMs\u5728\u4f17\u591a\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u521b\u9020\u529b\u65b9\u9762\u4ecd\u6709\u4e0d\u8db3\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u7ed3\u6784\u5316\u8868\u793a\u548c\u8ba4\u77e5\u542f\u53d1\u64cd\u4f5c\u63d0\u5347LLMs\u7684\u521b\u9020\u529b\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u5408LLMs\u4e0e\u7ed3\u6784\u5316\u8868\u793a\u548c\u8ba4\u77e5\u542f\u53d1\u64cd\u4f5c\uff0c\u751f\u6210\u66f4\u62bd\u8c61\u548c\u591a\u6837\u5316\u7684\u60f3\u6cd5\u3002\u5728\u70f9\u996a\u9886\u57df\u5f00\u53d1\u4e86DishCOVER\u6a21\u578b\u8fdb\u884c\u9a8c\u8bc1\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cDishCOVER\u751f\u6210\u7684\u98df\u8c31\u6bd4GPT-4o\u66f4\u5177\u591a\u6837\u6027\uff0c\u4e13\u5bb6\u8bc4\u4f30\u663e\u793a\u5176\u65b0\u9896\u6027\u663e\u8457\u4f18\u4e8eGPT-4o\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u63d0\u5347LLMs\u521b\u9020\u529b\u65b9\u9762\u5177\u6709\u6f5c\u529b\uff0c\u5e76\u5e0c\u671b\u6fc0\u53d1\u66f4\u591a\u5173\u4e8eAI\u7ed3\u6784\u5316\u521b\u9020\u529b\u7684\u7814\u7a76\u3002"}}
{"id": "2504.20656", "pdf": "https://arxiv.org/pdf/2504.20656", "abs": "https://arxiv.org/abs/2504.20656", "authors": ["Joshua Hatherley", "Anders S\u00f8gaard", "Angela Ballantyne", "Ruben Pauwels"], "title": "Federated learning, ethics, and the double black box problem in medical AI", "categories": ["cs.LG", "cs.AI", "cs.CY", "cs.HC"], "comment": null, "summary": "Federated learning (FL) is a machine learning approach that allows multiple\ndevices or institutions to collaboratively train a model without sharing their\nlocal data with a third-party. FL is considered a promising way to address\npatient privacy concerns in medical artificial intelligence. The ethical risks\nof medical FL systems themselves, however, have thus far been underexamined.\nThis paper aims to address this gap. We argue that medical FL presents a new\nvariety of opacity -- federation opacity -- that, in turn, generates a\ndistinctive double black box problem in healthcare AI. We highlight several\ninstances in which the anticipated benefits of medical FL may be exaggerated,\nand conclude by highlighting key challenges that must be overcome to make FL\nethically feasible in medicine.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u533b\u7597\u8054\u90a6\u5b66\u4e60\uff08FL\uff09\u7684\u4f26\u7406\u98ce\u9669\uff0c\u63d0\u51fa\u4e86\u201c\u8054\u90a6\u4e0d\u900f\u660e\u6027\u201d\u6982\u5ff5\uff0c\u5e76\u6307\u51fa\u5176\u5728\u533b\u7597AI\u4e2d\u53ef\u80fd\u5bfc\u81f4\u53cc\u91cd\u9ed1\u7bb1\u95ee\u9898\u3002", "motivation": "\u533b\u7597FL\u867d\u80fd\u4fdd\u62a4\u60a3\u8005\u9690\u79c1\uff0c\u4f46\u5176\u4f26\u7406\u98ce\u9669\u5c1a\u672a\u5145\u5206\u7814\u7a76\uff0c\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u901a\u8fc7\u5206\u6790\u533b\u7597FL\u7684\u7279\u70b9\uff0c\u63d0\u51fa\u201c\u8054\u90a6\u4e0d\u900f\u660e\u6027\u201d\u6982\u5ff5\uff0c\u5e76\u63a2\u8ba8\u5176\u5f15\u53d1\u7684\u53cc\u91cd\u9ed1\u7bb1\u95ee\u9898\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u533b\u7597FL\u7684\u9884\u671f\u6548\u76ca\u53ef\u80fd\u88ab\u5938\u5927\uff0c\u5e76\u5b58\u5728\u4f26\u7406\u6311\u6218\u3002", "conclusion": "\u9700\u514b\u670d\u5173\u952e\u6311\u6218\uff0c\u624d\u80fd\u4f7f\u533b\u7597FL\u5728\u4f26\u7406\u4e0a\u53ef\u884c\u3002"}}
{"id": "2504.20673", "pdf": "https://arxiv.org/pdf/2504.20673", "abs": "https://arxiv.org/abs/2504.20673", "authors": ["Wenjing Yin", "Tianze Sun", "Yijiong Yu", "Jiawei Fang", "Guangyao Su", "Jiancheng Wang", "Zekun Wang", "Wei Wang", "Ran Chen", "Ziyun Dai", "Shuai Yuan", "Menghang Dong", "Peng Luo", "Dong Cao", "Da Lei", "Yajun Zhang", "Hao Chen", "Xiang Ma", "Yong Liu", "Weifeng Liu", "Yuanjian Xu", "Ji Pei"], "title": "CoCo-Bench: A Comprehensive Code Benchmark For Multi-task Large Language Model Evaluation", "categories": ["cs.SE", "cs.AI"], "comment": "Submitted to ACL 2025. Under review", "summary": "Large language models (LLMs) play a crucial role in software engineering,\nexcelling in tasks like code generation and maintenance. However, existing\nbenchmarks are often narrow in scope, focusing on a specific task and lack a\ncomprehensive evaluation framework that reflects real-world applications. To\naddress these gaps, we introduce CoCo-Bench (Comprehensive Code Benchmark),\ndesigned to evaluate LLMs across four critical dimensions: code understanding,\ncode generation, code modification, and code review. These dimensions capture\nessential developer needs, ensuring a more systematic and representative\nevaluation. CoCo-Bench includes multiple programming languages and varying task\ndifficulties, with rigorous manual review to ensure data quality and accuracy.\nEmpirical results show that CoCo-Bench aligns with existing benchmarks while\nuncovering significant variations in model performance, effectively\nhighlighting strengths and weaknesses. By offering a holistic and objective\nevaluation, CoCo-Bench provides valuable insights to guide future research and\ntechnological advancements in code-oriented LLMs, establishing a reliable\nbenchmark for the field.", "AI": {"tldr": "CoCo-Bench\u662f\u4e00\u4e2a\u5168\u9762\u7684\u4ee3\u7801\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u4ee3\u7801\u7406\u89e3\u3001\u751f\u6210\u3001\u4fee\u6539\u548c\u5ba1\u67e5\u56db\u4e2a\u5173\u952e\u7ef4\u5ea6\u7684\u8868\u73b0\uff0c\u586b\u8865\u4e86\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u7684\u4e0d\u8db3\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u8303\u56f4\u72ed\u7a84\uff0c\u65e0\u6cd5\u5168\u9762\u53cd\u6620\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u8f6f\u4ef6\u5de5\u7a0b\u4e2d\u7684\u5b9e\u9645\u5e94\u7528\u9700\u6c42\u3002", "method": "\u8bbe\u8ba1CoCo-Bench\uff0c\u6db5\u76d6\u591a\u79cd\u7f16\u7a0b\u8bed\u8a00\u548c\u4efb\u52a1\u96be\u5ea6\uff0c\u5e76\u8fdb\u884c\u4e25\u683c\u7684\u4eba\u5de5\u5ba1\u6838\u4ee5\u786e\u4fdd\u6570\u636e\u8d28\u91cf\u3002", "result": "\u5b9e\u8bc1\u7ed3\u679c\u663e\u793aCoCo-Bench\u80fd\u6709\u6548\u63ed\u793a\u6a21\u578b\u6027\u80fd\u7684\u5dee\u5f02\uff0c\u7a81\u51fa\u5176\u4f18\u7f3a\u70b9\u3002", "conclusion": "CoCo-Bench\u4e3a\u4ee3\u7801\u5bfc\u5411\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u4e86\u5168\u9762\u5ba2\u89c2\u7684\u8bc4\u4f30\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u548c\u6280\u672f\u53d1\u5c55\u63d0\u4f9b\u4e86\u53ef\u9760\u57fa\u51c6\u3002"}}
{"id": "2504.20699", "pdf": "https://arxiv.org/pdf/2504.20699", "abs": "https://arxiv.org/abs/2504.20699", "authors": ["Evangelia Gogoulou", "Shorouq Zahra", "Liane Guillou", "Luise D\u00fcrlich", "Joakim Nivre"], "title": "Can LLMs Detect Intrinsic Hallucinations in Paraphrasing and Machine Translation?", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "A frequently observed problem with LLMs is their tendency to generate output\nthat is nonsensical, illogical, or factually incorrect, often referred to\nbroadly as hallucination. Building on the recently proposed HalluciGen task for\nhallucination detection and generation, we evaluate a suite of open-access LLMs\non their ability to detect intrinsic hallucinations in two conditional\ngeneration tasks: translation and paraphrasing. We study how model performance\nvaries across tasks and language and we investigate the impact of model size,\ninstruction tuning, and prompt choice. We find that performance varies across\nmodels but is consistent across prompts. Finally, we find that NLI models\nperform comparably well, suggesting that LLM-based detectors are not the only\nviable option for this specific task.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86LLMs\u5728\u7ffb\u8bd1\u548c\u91ca\u4e49\u4efb\u52a1\u4e2d\u68c0\u6d4b\u5185\u5728\u5e7b\u89c9\u7684\u80fd\u529b\uff0c\u53d1\u73b0\u6a21\u578b\u6027\u80fd\u56e0\u4efb\u52a1\u548c\u8bed\u8a00\u800c\u5f02\uff0c\u4f46\u63d0\u793a\u9009\u62e9\u5f71\u54cd\u4e0d\u5927\u3002NLI\u6a21\u578b\u8868\u73b0\u76f8\u5f53\uff0c\u8868\u660eLLM\u68c0\u6d4b\u5668\u5e76\u975e\u552f\u4e00\u53ef\u884c\u65b9\u6848\u3002", "motivation": "LLMs\u5e38\u751f\u6210\u65e0\u610f\u4e49\u6216\u9519\u8bef\u7684\u8f93\u51fa\uff08\u5e7b\u89c9\uff09\uff0c\u7814\u7a76\u65e8\u5728\u8bc4\u4f30\u5176\u5728\u68c0\u6d4b\u5185\u5728\u5e7b\u89c9\u65b9\u9762\u7684\u80fd\u529b\u3002", "method": "\u57fa\u4e8eHalluciGen\u4efb\u52a1\uff0c\u8bc4\u4f30\u5f00\u6e90LLMs\u5728\u7ffb\u8bd1\u548c\u91ca\u4e49\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u8003\u5bdf\u6a21\u578b\u5927\u5c0f\u3001\u6307\u4ee4\u8c03\u6574\u548c\u63d0\u793a\u9009\u62e9\u7684\u5f71\u54cd\u3002", "result": "\u6a21\u578b\u6027\u80fd\u56e0\u4efb\u52a1\u548c\u8bed\u8a00\u800c\u5f02\uff0c\u4f46\u63d0\u793a\u9009\u62e9\u5f71\u54cd\u4e0d\u5927\uff1bNLI\u6a21\u578b\u8868\u73b0\u4e0eLLMs\u76f8\u5f53\u3002", "conclusion": "LLM\u68c0\u6d4b\u5668\u5e76\u975e\u552f\u4e00\u53ef\u884c\u65b9\u6848\uff0cNLI\u6a21\u578b\u5728\u68c0\u6d4b\u5e7b\u89c9\u4efb\u52a1\u4e2d\u8868\u73b0\u826f\u597d\u3002"}}
{"id": "2504.20708", "pdf": "https://arxiv.org/pdf/2504.20708", "abs": "https://arxiv.org/abs/2504.20708", "authors": ["Hasan Abed Al Kader Hammoud", "Hani Itani", "Bernard Ghanem"], "title": "Beyond the Last Answer: Your Reasoning Trace Uncovers More than You Think", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Preprint", "summary": "Large Language Models (LLMs) leverage step-by-step reasoning to solve complex\nproblems. Standard evaluation practice involves generating a complete reasoning\ntrace and assessing the correctness of the final answer presented at its\nconclusion. In this paper, we challenge the reliance on the final answer by\nposing the following two questions: Does the final answer reliably represent\nthe model's optimal conclusion? Can alternative reasoning paths yield different\nresults? To answer these questions, we analyze intermediate reasoning steps,\ntermed subthoughts, and propose a method based on our findings. Our approach\ninvolves segmenting a reasoning trace into sequential subthoughts based on\nlinguistic cues. We start by prompting the model to generate continuations from\nthe end-point of each intermediate subthought. We extract a potential answer\nfrom every completed continuation originating from different subthoughts. We\nfind that aggregating these answers by selecting the most frequent one (the\nmode) often yields significantly higher accuracy compared to relying solely on\nthe answer derived from the original complete trace. Analyzing the consistency\namong the answers derived from different subthoughts reveals characteristics\nthat correlate with the model's confidence and correctness, suggesting\npotential for identifying less reliable answers. Our experiments across various\nLLMs and challenging mathematical reasoning datasets (AIME2024 and AIME2025)\nshow consistent accuracy improvements, with gains reaching up to 13\\% and 10\\%\nrespectively. Implementation is available at:\nhttps://github.com/hammoudhasan/SubthoughtReasoner.", "AI": {"tldr": "\u8bba\u6587\u8d28\u7591\u4ec5\u4f9d\u8d56\u6700\u7ec8\u7b54\u6848\u8bc4\u4f30LLM\u63a8\u7406\u7684\u53ef\u9760\u6027\uff0c\u63d0\u51fa\u901a\u8fc7\u5206\u6790\u4e2d\u95f4\u63a8\u7406\u6b65\u9aa4\uff08\u5b50\u601d\u60f3\uff09\u5e76\u805a\u5408\u591a\u8def\u5f84\u7b54\u6848\u7684\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u51c6\u786e\u6027\u3002", "motivation": "\u6311\u6218\u4f20\u7edf\u8bc4\u4f30\u65b9\u6cd5\uff0c\u63a2\u7a76\u6700\u7ec8\u7b54\u6848\u662f\u5426\u80fd\u4ee3\u8868\u6a21\u578b\u6700\u4f18\u7ed3\u8bba\uff0c\u4ee5\u53ca\u4e0d\u540c\u63a8\u7406\u8def\u5f84\u662f\u5426\u4f1a\u5bfc\u81f4\u4e0d\u540c\u7ed3\u679c\u3002", "method": "\u5c06\u63a8\u7406\u8f68\u8ff9\u5206\u6bb5\u4e3a\u5b50\u601d\u60f3\uff0c\u751f\u6210\u591a\u8def\u5f84\u7b54\u6848\u5e76\u9009\u62e9\u51fa\u73b0\u9891\u7387\u6700\u9ad8\u7684\u7b54\u6848\uff08\u4f17\u6570\uff09\u3002", "result": "\u5728\u591a\u4e2aLLM\u548c\u6570\u5b66\u63a8\u7406\u6570\u636e\u96c6\u4e0a\uff0c\u51c6\u786e\u6027\u63d0\u5347\u9ad8\u8fbe13%\u548c10%\u3002", "conclusion": "\u5206\u6790\u5b50\u601d\u60f3\u4e00\u81f4\u6027\u53ef\u8bc6\u522b\u4e0d\u53ef\u9760\u7b54\u6848\uff0c\u591a\u8def\u5f84\u7b54\u6848\u805a\u5408\u65b9\u6cd5\u663e\u8457\u63d0\u5347LLM\u63a8\u7406\u51c6\u786e\u6027\u3002"}}
{"id": "2504.20726", "pdf": "https://arxiv.org/pdf/2504.20726", "abs": "https://arxiv.org/abs/2504.20726", "authors": ["Hattan Althebeiti", "Mohammed Alkinoon", "Manar Mohaisen", "Saeed Salem", "DaeHun Nyang", "David Mohaisen"], "title": "Enhancing Vulnerability Reports with Automated and Augmented Description Summarization", "categories": ["cs.CR", "cs.AI", "cs.LG"], "comment": "12 pages, 3 tables, 12 figures. Accepted for publication in IEEE\n  Transactions on Big Data. Extended version of arXiv:2210.01260", "summary": "Public vulnerability databases, such as the National Vulnerability Database\n(NVD), document vulnerabilities and facilitate threat information sharing.\nHowever, they often suffer from short descriptions and outdated or insufficient\ninformation. In this paper, we introduce Zad, a system designed to enrich NVD\nvulnerability descriptions by leveraging external resources. Zad consists of\ntwo pipelines: one collects and filters supplementary data using two encoders\nto build a detailed dataset, while the other fine-tunes a pre-trained model on\nthis dataset to generate enriched descriptions. By addressing brevity and\nimproving content quality, Zad produces more comprehensive and cohesive\nvulnerability descriptions. We evaluate Zad using standard summarization\nmetrics and human assessments, demonstrating its effectiveness in enhancing\nvulnerability information.", "AI": {"tldr": "Zad\u7cfb\u7edf\u901a\u8fc7\u5916\u90e8\u8d44\u6e90\u4e30\u5bccNVD\u6f0f\u6d1e\u63cf\u8ff0\uff0c\u89e3\u51b3\u4fe1\u606f\u4e0d\u8db3\u95ee\u9898\uff0c\u63d0\u5347\u5185\u5bb9\u8d28\u91cf\u3002", "motivation": "\u516c\u5171\u6f0f\u6d1e\u6570\u636e\u5e93\uff08\u5982NVD\uff09\u63cf\u8ff0\u7b80\u77ed\u4e14\u4fe1\u606f\u4e0d\u8db3\uff0c\u9700\u6539\u8fdb\u3002", "method": "Zad\u7cfb\u7edf\u5305\u542b\u4e24\u4e2a\u6d41\u7a0b\uff1a\u6570\u636e\u6536\u96c6\u4e0e\u8fc7\u6ee4\u3001\u9884\u8bad\u7ec3\u6a21\u578b\u5fae\u8c03\u751f\u6210\u63cf\u8ff0\u3002", "result": "\u8bc4\u4f30\u663e\u793aZad\u80fd\u6709\u6548\u63d0\u5347\u6f0f\u6d1e\u63cf\u8ff0\u7684\u5168\u9762\u6027\u548c\u4e00\u81f4\u6027\u3002", "conclusion": "Zad\u6210\u529f\u89e3\u51b3\u4e86NVD\u63cf\u8ff0\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u4fe1\u606f\u8d28\u91cf\u3002"}}
{"id": "2504.20733", "pdf": "https://arxiv.org/pdf/2504.20733", "abs": "https://arxiv.org/abs/2504.20733", "authors": ["Simon Kl\u00fcttermann", "Tim Katzke", "Emmanuel M\u00fcller"], "title": "Unsupervised Surrogate Anomaly Detection", "categories": ["cs.LG", "cs.AI"], "comment": "13 pages + references and appendix = 35 pages", "summary": "In this paper, we study unsupervised anomaly detection algorithms that learn\na neural network representation, i.e. regular patterns of normal data, which\nanomalies are deviating from. Inspired by a similar concept in engineering, we\nrefer to our methodology as surrogate anomaly detection. We formalize the\nconcept of surrogate anomaly detection into a set of axioms required for\noptimal surrogate models and propose a new algorithm, named DEAN (Deep Ensemble\nANomaly detection), designed to fulfill these criteria. We evaluate DEAN on 121\nbenchmark datasets, demonstrating its competitive performance against 19\nexisting methods, as well as the scalability and reliability of our method.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aDEAN\u7684\u65e0\u76d1\u7763\u5f02\u5e38\u68c0\u6d4b\u7b97\u6cd5\uff0c\u901a\u8fc7\u5b66\u4e60\u6b63\u5e38\u6570\u636e\u7684\u6a21\u5f0f\u6765\u68c0\u6d4b\u5f02\u5e38\uff0c\u5e76\u5728121\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u5176\u6027\u80fd\u3002", "motivation": "\u53d7\u5de5\u7a0b\u4e2d\u7c7b\u4f3c\u6982\u5ff5\u7684\u542f\u53d1\uff0c\u7814\u7a76\u5982\u4f55\u901a\u8fc7\u5b66\u4e60\u6b63\u5e38\u6570\u636e\u7684\u6a21\u5f0f\u6765\u68c0\u6d4b\u5f02\u5e38\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aDEAN\u7684\u7b97\u6cd5\uff0c\u57fa\u4e8e\u4e00\u7ec4\u6700\u4f18\u4ee3\u7406\u6a21\u578b\u7684\u516c\u7406\u8bbe\u8ba1\u3002", "result": "\u5728121\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86DEAN\u7684\u6027\u80fd\uff0c\u4f18\u4e8e19\u79cd\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u5c55\u793a\u4e86\u5176\u53ef\u6269\u5c55\u6027\u548c\u53ef\u9760\u6027\u3002", "conclusion": "DEAN\u662f\u4e00\u79cd\u9ad8\u6548\u4e14\u53ef\u9760\u7684\u65e0\u76d1\u7763\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u5927\u89c4\u6a21\u6570\u636e\u3002"}}
{"id": "2504.20741", "pdf": "https://arxiv.org/pdf/2504.20741", "abs": "https://arxiv.org/abs/2504.20741", "authors": ["Joshua Hatherley", "Lauritz Munch", "Jens Christian Bjerring"], "title": "In defence of post-hoc explanations in medical AI", "categories": ["cs.HC", "cs.AI", "cs.CY", "cs.LG"], "comment": null, "summary": "Since the early days of the Explainable AI movement, post-hoc explanations\nhave been praised for their potential to improve user understanding, promote\ntrust, and reduce patient safety risks in black box medical AI systems.\nRecently, however, critics have argued that the benefits of post-hoc\nexplanations are greatly exaggerated since they merely approximate, rather than\nreplicate, the actual reasoning processes that black box systems take to arrive\nat their outputs. In this article, we aim to defend the value of post-hoc\nexplanations against this recent critique. We argue that even if post-hoc\nexplanations do not replicate the exact reasoning processes of black box\nsystems, they can still improve users' functional understanding of black box\nsystems, increase the accuracy of clinician-AI teams, and assist clinicians in\njustifying their AI-informed decisions. While post-hoc explanations are not a\n\"silver bullet\" solution to the black box problem in medical AI, we conclude\nthat they remain a useful strategy for addressing the black box problem in\nmedical AI.", "AI": {"tldr": "\u6587\u7ae0\u4e3a\u540e\u9a8c\u89e3\u91ca\u5728\u533b\u7597AI\u4e2d\u7684\u4ef7\u503c\u8fa9\u62a4\uff0c\u8ba4\u4e3a\u5176\u867d\u4e0d\u5b8c\u7f8e\uff0c\u4f46\u4ecd\u80fd\u63d0\u5347\u7528\u6237\u7406\u89e3\u548c\u4e34\u5e8a\u51b3\u7b56\u3002", "motivation": "\u56de\u5e94\u6279\u8bc4\uff0c\u8bc1\u660e\u540e\u9a8c\u89e3\u91ca\u5728\u533b\u7597AI\u4e2d\u7684\u5b9e\u9645\u4ef7\u503c\u3002", "method": "\u901a\u8fc7\u8bba\u8bc1\u540e\u9a8c\u89e3\u91ca\u7684\u529f\u80fd\u6027\u4f5c\u7528\uff0c\u53cd\u9a73\u5176\u4ec5\u662f\u8fd1\u4f3c\u800c\u975e\u590d\u5236\u7684\u89c2\u70b9\u3002", "result": "\u540e\u9a8c\u89e3\u91ca\u80fd\u63d0\u5347\u7528\u6237\u7406\u89e3\u3001\u56e2\u961f\u51c6\u786e\u6027\u548c\u51b3\u7b56\u5408\u7406\u6027\u3002", "conclusion": "\u540e\u9a8c\u89e3\u91ca\u662f\u89e3\u51b3\u533b\u7597AI\u9ed1\u76d2\u95ee\u9898\u7684\u6709\u6548\u7b56\u7565\uff0c\u867d\u975e\u4e07\u80fd\u3002"}}
{"id": "2504.20752", "pdf": "https://arxiv.org/pdf/2504.20752", "abs": "https://arxiv.org/abs/2504.20752", "authors": ["Roman Abramov", "Felix Steinbauer", "Gjergji Kasneci"], "title": "Grokking in the Wild: Data Augmentation for Real-World Multi-Hop Reasoning with Transformers", "categories": ["cs.CL", "cs.AI", "cs.LG", "I.2.7; I.2.6; I.2.3; I.7"], "comment": null, "summary": "Transformers have achieved great success in numerous NLP tasks but continue\nto exhibit notable gaps in multi-step factual reasoning, especially when\nreal-world knowledge is sparse. Recent advances in grokking have demonstrated\nthat neural networks can transition from memorizing to perfectly generalizing\nonce they detect underlying logical patterns - yet these studies have primarily\nused small, synthetic tasks. In this paper, for the first time, we extend\ngrokking to real-world factual data and address the challenge of dataset\nsparsity by augmenting existing knowledge graphs with carefully designed\nsynthetic data to raise the ratio $\\phi_r$ of inferred facts to atomic facts\nabove the threshold required for grokking. Surprisingly, we find that even\nfactually incorrect synthetic data can strengthen emergent reasoning circuits\nrather than degrade accuracy, as it forces the model to rely on relational\nstructure rather than memorization. When evaluated on multi-hop reasoning\nbenchmarks, our approach achieves up to 95-100% accuracy on 2WikiMultiHopQA -\nsubstantially improving over strong baselines and matching or exceeding current\nstate-of-the-art results. We further provide an in-depth analysis of how\nincreasing $\\phi_r$ drives the formation of generalizing circuits inside\nTransformers. Our findings suggest that grokking-based data augmentation can\nunlock implicit multi-hop reasoning capabilities, opening the door to more\nrobust and interpretable factual reasoning in large-scale language models.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u5408\u6210\u6570\u636e\u589e\u5f3a\u77e5\u8bc6\u56fe\u8c31\u7684\u65b9\u6cd5\uff0c\u4ee5\u63d0\u5347Transformer\u6a21\u578b\u5728\u591a\u6b65\u4e8b\u5b9e\u63a8\u7406\u4e2d\u7684\u8868\u73b0\uff0c\u5c24\u5176\u662f\u5728\u7a00\u758f\u77e5\u8bc6\u573a\u666f\u4e0b\u3002\u5b9e\u9a8c\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u63a8\u7406\u51c6\u786e\u6027\u3002", "motivation": "\u89e3\u51b3Transformer\u5728\u7a00\u758f\u77e5\u8bc6\u573a\u666f\u4e0b\u591a\u6b65\u4e8b\u5b9e\u63a8\u7406\u80fd\u529b\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u5e76\u63a2\u7d22\u5982\u4f55\u901a\u8fc7\u6570\u636e\u589e\u5f3a\u89e6\u53d1\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u901a\u8fc7\u8bbe\u8ba1\u5408\u6210\u6570\u636e\u589e\u5f3a\u73b0\u6709\u77e5\u8bc6\u56fe\u8c31\uff0c\u63d0\u9ad8\u63a8\u65ad\u4e8b\u5b9e\u4e0e\u539f\u5b50\u4e8b\u5b9e\u7684\u6bd4\u4f8b\uff08\u03c6_r\uff09\uff0c\u4ece\u800c\u89e6\u53d1\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u3002", "result": "\u57282WikiMultiHopQA\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u523095-100%\u7684\u51c6\u786e\u7387\uff0c\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\uff0c\u5e76\u5339\u914d\u6216\u8d85\u8d8a\u5f53\u524d\u6700\u4f18\u7ed3\u679c\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u57fa\u4e8egrokking\u7684\u6570\u636e\u589e\u5f3a\u53ef\u4ee5\u91ca\u653eTransformer\u7684\u9690\u5f0f\u591a\u6b65\u63a8\u7406\u80fd\u529b\uff0c\u4e3a\u5927\u89c4\u6a21\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u66f4\u9c81\u68d2\u548c\u53ef\u89e3\u91ca\u7684\u4e8b\u5b9e\u63a8\u7406\u65b9\u6cd5\u3002"}}
{"id": "2504.20769", "pdf": "https://arxiv.org/pdf/2504.20769", "abs": "https://arxiv.org/abs/2504.20769", "authors": ["Wenxiao Wang", "Parsa Hosseini", "Soheil Feizi"], "title": "Chain-of-Defensive-Thought: Structured Reasoning Elicits Robustness in Large Language Models against Reference Corruption", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Chain-of-thought prompting has demonstrated great success in facilitating the\nreasoning abilities of large language models. In this work, we explore how\nthese enhanced reasoning abilities can be exploited to improve the robustness\nof large language models in tasks that are not necessarily reasoning-focused.\nIn particular, we show how a wide range of large language models exhibit\nsignificantly improved robustness against reference corruption using a simple\nmethod called chain-of-defensive-thought, where only a few exemplars with\nstructured and defensive reasoning are provided as demonstrations. Empirically,\nthe improvements can be astounding, especially given the simplicity and\napplicability of the method. For example, in the Natural Questions task, the\naccuracy of GPT-4o degrades from 60% to as low as 3% with standard prompting\nwhen 1 out of 10 references provided is corrupted with prompt injection\nattacks. In contrast, GPT-4o using chain-of-defensive-thought prompting\nmaintains an accuracy of 50%.", "AI": {"tldr": "\u94fe\u5f0f\u9632\u5fa1\u601d\u7ef4\u63d0\u793a\u663e\u8457\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u5728\u975e\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u9c81\u68d2\u6027\u3002", "motivation": "\u63a2\u7d22\u5982\u4f55\u5229\u7528\u94fe\u5f0f\u601d\u7ef4\u63d0\u793a\u589e\u5f3a\u7684\u63a8\u7406\u80fd\u529b\uff0c\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u5728\u975e\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u9c81\u68d2\u6027\u3002", "method": "\u63d0\u51fa\u94fe\u5f0f\u9632\u5fa1\u601d\u7ef4\u63d0\u793a\u65b9\u6cd5\uff0c\u4ec5\u9700\u5c11\u91cf\u7ed3\u6784\u5316\u9632\u5fa1\u6027\u63a8\u7406\u793a\u4f8b\u4f5c\u4e3a\u6f14\u793a\u3002", "result": "\u5728\u81ea\u7136\u95ee\u9898\u4efb\u52a1\u4e2d\uff0cGPT-4o\u4f7f\u7528\u8be5\u65b9\u6cd5\u540e\u51c6\u786e\u7387\u4ece3%\u63d0\u5347\u81f350%\uff0c\u663e\u8457\u4f18\u4e8e\u6807\u51c6\u63d0\u793a\u3002", "conclusion": "\u94fe\u5f0f\u9632\u5fa1\u601d\u7ef4\u63d0\u793a\u7b80\u5355\u6709\u6548\uff0c\u663e\u8457\u63d0\u5347\u6a21\u578b\u5bf9\u53c2\u8003\u6570\u636e\u6c61\u67d3\u7684\u9c81\u68d2\u6027\u3002"}}
{"id": "2504.20770", "pdf": "https://arxiv.org/pdf/2504.20770", "abs": "https://arxiv.org/abs/2504.20770", "authors": ["Ji Shi", "Chengxun Xie", "Zhonghao Li", "Xinming Zhang", "Miao Zhang"], "title": "JTreeformer: Graph-Transformer via Latent-Diffusion Model for Molecular Generation", "categories": ["cs.LG", "cs.AI"], "comment": "9 pages, 6figures", "summary": "The discovery of new molecules based on the original chemical molecule\ndistributions is of great importance in medicine. The graph transformer, with\nits advantages of high performance and scalability compared to traditional\ngraph networks, has been widely explored in recent research for applications of\ngraph structures. However, current transformer-based graph decoders struggle to\neffectively utilize graph information, which limits their capacity to leverage\nonly sequences of nodes rather than the complex topological structures of\nmolecule graphs. This paper focuses on building a graph transformer-based\nframework for molecular generation, which we call \\textbf{JTreeformer} as it\ntransforms graph generation into junction tree generation. It combines GCN\nparallel with multi-head attention as the encoder. It integrates a directed\nacyclic GCN into a graph-based Transformer to serve as a decoder, which can\niteratively synthesize the entire molecule by leveraging information from the\npartially constructed molecular structure at each step. In addition, a\ndiffusion model is inserted in the latent space generated by the encoder, to\nenhance the efficiency and effectiveness of sampling further. The empirical\nresults demonstrate that our novel framework outperforms existing molecule\ngeneration methods, thus offering a promising tool to advance drug discovery\n(https://anonymous.4open.science/r/JTreeformer-C74C).", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aJTreeformer\u7684\u56fe\u53d8\u6362\u5668\u6846\u67b6\uff0c\u7528\u4e8e\u5206\u5b50\u751f\u6210\uff0c\u901a\u8fc7\u7ed3\u5408GCN\u548c\u591a\u5934\u6ce8\u610f\u529b\u7f16\u7801\u5668\uff0c\u4ee5\u53ca\u6709\u5411\u65e0\u73afGCN\u89e3\u7801\u5668\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5206\u5b50\u751f\u6210\u7684\u6548\u7387\u548c\u6548\u679c\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u53d8\u6362\u5668\u7684\u56fe\u89e3\u7801\u5668\u96be\u4ee5\u6709\u6548\u5229\u7528\u56fe\u4fe1\u606f\uff0c\u9650\u5236\u4e86\u5206\u5b50\u751f\u6210\u7684\u80fd\u529b\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65b0\u65b9\u6cd5\u6765\u66f4\u597d\u5730\u5229\u7528\u5206\u5b50\u56fe\u7684\u590d\u6742\u62d3\u6251\u7ed3\u6784\u3002", "method": "JTreeformer\u5c06\u56fe\u751f\u6210\u8f6c\u5316\u4e3a\u8fde\u63a5\u6811\u751f\u6210\uff0c\u7ed3\u5408GCN\u548c\u591a\u5934\u6ce8\u610f\u529b\u7f16\u7801\u5668\uff0c\u4ee5\u53ca\u6709\u5411\u65e0\u73afGCN\u89e3\u7801\u5668\uff0c\u5e76\u5728\u6f5c\u5728\u7a7a\u95f4\u4e2d\u5f15\u5165\u6269\u6563\u6a21\u578b\u4ee5\u589e\u5f3a\u91c7\u6837\u6548\u7387\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cJTreeformer\u5728\u5206\u5b50\u751f\u6210\u4efb\u52a1\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u4e3a\u836f\u7269\u53d1\u73b0\u63d0\u4f9b\u4e86\u6709\u529b\u5de5\u5177\u3002", "conclusion": "JTreeformer\u6846\u67b6\u901a\u8fc7\u6539\u8fdb\u7f16\u7801\u5668\u548c\u89e3\u7801\u5668\u8bbe\u8ba1\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5206\u5b50\u751f\u6210\u7684\u6027\u80fd\uff0c\u5177\u6709\u5e7f\u6cdb\u7684\u5e94\u7528\u524d\u666f\u3002"}}
{"id": "2504.20776", "pdf": "https://arxiv.org/pdf/2504.20776", "abs": "https://arxiv.org/abs/2504.20776", "authors": ["David Funosas", "Elodie Massol", "Yves Bas", "Svenja Schmidt", "Dominik Arend", "Alexander Gebhard", "Luc Barbaro", "Sebastian K\u00f6nig", "Rafael Carbonell Font", "David Sannier", "Fernand Deroussen", "J\u00e9r\u00f4me Sueur", "Christian Roesti", "Tomi Trilar", "Wolfgang Forstmeier", "Lucas Roger", "Elo\u00efsa Matheu", "Piotr Guzik", "Julien Barataud", "Laurent Pelozuelo", "St\u00e9phane Puissant", "Sandra Mueller", "Bj\u00f6rn Schuller", "Jose M. Montoya", "Andreas Triantafyllopoulos", "Maxime Cauchoix"], "title": "ECOSoundSet: a finely annotated dataset for the automated acoustic identification of Orthoptera and Cicadidae in North, Central and temperate Western Europe", "categories": ["cs.SD", "cs.AI", "eess.AS"], "comment": "3 Figures + 2 Supplementary Figures, 2 Tables + 3 Supplementary\n  Tables", "summary": "Currently available tools for the automated acoustic recognition of European\ninsects in natural soundscapes are limited in scope. Large and ecologically\nheterogeneous acoustic datasets are currently needed for these algorithms to\ncross-contextually recognize the subtle and complex acoustic signatures\nproduced by each species, thus making the availability of such datasets a key\nrequisite for their development. Here we present ECOSoundSet (European\nCicadidae and Orthoptera Sound dataSet), a dataset containing 10,653 recordings\nof 200 orthopteran and 24 cicada species (217 and 26 respective taxa when\nincluding subspecies) present in North, Central, and temperate Western Europe\n(Andorra, Belgium, Denmark, mainland France and Corsica, Germany, Ireland,\nLuxembourg, Monaco, Netherlands, United Kingdom, Switzerland), collected partly\nthrough targeted fieldwork in South France and Catalonia and partly through\ncontributions from various European entomologists. The dataset is composed of a\ncombination of coarsely labeled recordings, for which we can only infer the\npresence, at some point, of their target species (weak labeling), and finely\nannotated recordings, for which we know the specific time and frequency range\nof each insect sound present in the recording (strong labeling). We also\nprovide a train/validation/test split of the strongly labeled recordings, with\nrespective approximate proportions of 0.8, 0.1 and 0.1, in order to facilitate\ntheir incorporation in the training and evaluation of deep learning algorithms.\nThis dataset could serve as a meaningful complement to recordings already\navailable online for the training of deep learning algorithms for the acoustic\nclassification of orthopterans and cicadas in North, Central, and temperate\nWestern Europe.", "AI": {"tldr": "ECOSoundSet\u662f\u4e00\u4e2a\u5305\u542b\u6b27\u6d32\u76f4\u7fc5\u76ee\u548c\u8749\u7c7b\u58f0\u97f3\u7684\u6570\u636e\u96c6\uff0c\u65e8\u5728\u652f\u6301\u6df1\u5ea6\u5b66\u4e60\u7b97\u6cd5\u5728\u81ea\u7136\u58f0\u666f\u4e2d\u7684\u6606\u866b\u58f0\u97f3\u8bc6\u522b\u3002", "motivation": "\u73b0\u6709\u5de5\u5177\u5728\u8de8\u4e0a\u4e0b\u6587\u8bc6\u522b\u6606\u866b\u58f0\u97f3\u65b9\u9762\u53d7\u9650\uff0c\u9700\u8981\u5927\u89c4\u6a21\u4e14\u751f\u6001\u591a\u6837\u7684\u6570\u636e\u96c6\u6765\u63d0\u5347\u7b97\u6cd5\u6027\u80fd\u3002", "method": "\u6570\u636e\u96c6\u5305\u542b10,653\u6761\u5f55\u97f3\uff0c\u6db5\u76d6200\u79cd\u76f4\u7fc5\u76ee\u548c24\u79cd\u8749\u7c7b\uff0c\u90e8\u5206\u4e3a\u5f31\u6807\u6ce8\uff08\u4ec5\u77e5\u7269\u79cd\u5b58\u5728\uff09\uff0c\u90e8\u5206\u4e3a\u5f3a\u6807\u6ce8\uff08\u7cbe\u786e\u65f6\u95f4\u9891\u7387\u4fe1\u606f\uff09\u3002", "result": "\u63d0\u4f9b\u4e86\u8bad\u7ec3/\u9a8c\u8bc1/\u6d4b\u8bd5\u96c6\u7684\u5212\u5206\uff08\u6bd4\u4f8b0.8/0.1/0.1\uff09\uff0c\u4fbf\u4e8e\u7b97\u6cd5\u8bad\u7ec3\u4e0e\u8bc4\u4f30\u3002", "conclusion": "ECOSoundSet\u53ef\u4f5c\u4e3a\u73b0\u6709\u5728\u7ebf\u8d44\u6e90\u7684\u8865\u5145\uff0c\u63d0\u5347\u6b27\u6d32\u76f4\u7fc5\u76ee\u548c\u8749\u7c7b\u7684\u58f0\u5b66\u5206\u7c7b\u6548\u679c\u3002"}}
{"id": "2504.20781", "pdf": "https://arxiv.org/pdf/2504.20781", "abs": "https://arxiv.org/abs/2504.20781", "authors": ["Xiyu Zhou", "Ruiyin Li", "Peng Liang", "Beiqi Zhang", "Mojtaba Shahin", "Zengyang Li", "Chen Yang"], "title": "Using LLMs in Generating Design Rationale for Software Architecture Decisions", "categories": ["cs.SE", "cs.AI"], "comment": "28 pages, 5 images, 7 tables, Manuscript submitted to a journal\n  (2025)", "summary": "Design Rationale (DR) for software architecture decisions refers to the\nreasoning underlying architectural choices, which provides valuable insights\ninto the different phases of the architecting process throughout software\ndevelopment. However, in practice, DR is often inadequately documented due to a\nlack of motivation and effort from developers. With the recent advancements in\nLarge Language Models (LLMs), their capabilities in text comprehension,\nreasoning, and generation may enable the generation and recovery of DR for\narchitecture decisions. In this study, we evaluated the performance of LLMs in\ngenerating DR for architecture decisions. First, we collected 50 Stack Overflow\n(SO) posts, 25 GitHub issues, and 25 GitHub discussions related to architecture\ndecisions to construct a dataset of 100 architecture-related problems. Then, we\nselected five LLMs to generate DR for the architecture decisions with three\nprompting strategies, including zero-shot, chain of thought (CoT), and\nLLM-based agents. With the DR provided by human experts as ground truth, the\nPrecision of LLM-generated DR with the three prompting strategies ranges from\n0.267 to 0.278, Recall from 0.627 to 0.715, and F1-score from 0.351 to 0.389.\nAdditionally, 64.45% to 69.42% of the arguments of DR not mentioned by human\nexperts are also helpful, 4.12% to 4.87% of the arguments have uncertain\ncorrectness, and 1.59% to 3.24% of the arguments are potentially misleading.\nBased on the results, we further discussed the pros and cons of the three\nprompting strategies and the strengths and limitations of the DR generated by\nLLMs.", "AI": {"tldr": "\u8be5\u7814\u7a76\u8bc4\u4f30\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u751f\u6210\u8f6f\u4ef6\u67b6\u6784\u51b3\u7b56\u8bbe\u8ba1\u7406\u7531\uff08DR\uff09\u4e2d\u7684\u8868\u73b0\uff0c\u4f7f\u7528\u4e86\u4e09\u79cd\u63d0\u793a\u7b56\u7565\uff0c\u5e76\u6bd4\u8f83\u4e86\u5176\u7cbe\u786e\u5ea6\u3001\u53ec\u56de\u7387\u548cF1\u5206\u6570\u3002", "motivation": "\u5b9e\u8df5\u4e2d\uff0cDR\u5e38\u56e0\u5f00\u53d1\u8005\u7f3a\u4e4f\u52a8\u529b\u548c\u52aa\u529b\u800c\u672a\u5145\u5206\u8bb0\u5f55\uff0cLLMs\u7684\u6587\u672c\u7406\u89e3\u548c\u751f\u6210\u80fd\u529b\u53ef\u80fd\u5e2e\u52a9\u751f\u6210\u548c\u6062\u590dDR\u3002", "method": "\u6536\u96c6\u4e86100\u4e2a\u67b6\u6784\u76f8\u5173\u95ee\u9898\uff0c\u4f7f\u7528\u4e94\u79cdLLMs\u548c\u4e09\u79cd\u63d0\u793a\u7b56\u7565\uff08\u96f6\u6837\u672c\u3001\u601d\u7ef4\u94fe\u3001LLM\u4ee3\u7406\uff09\u751f\u6210DR\uff0c\u5e76\u4e0e\u4e13\u5bb6\u63d0\u4f9b\u7684DR\u8fdb\u884c\u5bf9\u6bd4\u3002", "result": "LLM\u751f\u6210\u7684DR\u5728\u7cbe\u786e\u5ea6\uff080.267-0.278\uff09\u3001\u53ec\u56de\u7387\uff080.627-0.715\uff09\u548cF1\u5206\u6570\uff080.351-0.389\uff09\u4e0a\u8868\u73b0\u4e0d\u4e00\uff0c\u90e8\u5206\u672a\u63d0\u53ca\u7684\u8bba\u70b9\u4e5f\u6709\u5e2e\u52a9\u3002", "conclusion": "\u7814\u7a76\u8ba8\u8bba\u4e86\u4e09\u79cd\u63d0\u793a\u7b56\u7565\u7684\u4f18\u7f3a\u70b9\u53caLLM\u751f\u6210DR\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u672a\u6765\u6539\u8fdb\u63d0\u4f9b\u4e86\u65b9\u5411\u3002"}}
{"id": "2504.20799", "pdf": "https://arxiv.org/pdf/2504.20799", "abs": "https://arxiv.org/abs/2504.20799", "authors": ["Yunseo Lee", "John Youngeun Song", "Dongsun Kim", "Jindae Kim", "Mijung Kim", "Jaechang Nam"], "title": "Hallucination by Code Generation LLMs: Taxonomy, Benchmarks, Mitigation, and Challenges", "categories": ["cs.SE", "cs.AI"], "comment": "15 pages, 4 figures", "summary": "Recent technical breakthroughs in large language models (LLMs) have enabled\nthem to fluently generate source code. Software developers often leverage both\ngeneral-purpose and code-specialized LLMs to revise existing code or even\ngenerate a whole function from scratch. These capabilities are also beneficial\nin no-code or low-code contexts, in which one can write programs without a\ntechnical background. However, due to their internal design, LLMs are prone to\ngenerating hallucinations, which are incorrect, nonsensical, and not\njustifiable information but difficult to identify its presence. This problem\nalso occurs when generating source code. Once hallucinated code is produced, it\nis often challenging for users to identify and fix it, especially when such\nhallucinations can be identified under specific execution paths. As a result,\nthe hallucinated code may remain unnoticed within the codebase. This survey\ninvestigates recent studies and techniques relevant to hallucinations generated\nby CodeLLMs. We categorize the types of hallucinations in the code generated by\nCodeLLMs, review existing benchmarks and mitigation strategies, and identify\nopen challenges. Based on these findings, this survey outlines further research\ndirections in the detection and removal of hallucinations produced by CodeLLMs.", "AI": {"tldr": "\u672c\u6587\u8c03\u67e5\u4e86\u7531\u4ee3\u7801\u4e13\u7528\u5927\u8bed\u8a00\u6a21\u578b\uff08CodeLLMs\uff09\u751f\u6210\u7684\u4ee3\u7801\u4e2d\u7684\u5e7b\u89c9\u95ee\u9898\uff0c\u5206\u7c7b\u4e86\u5e7b\u89c9\u7c7b\u578b\uff0c\u56de\u987e\u4e86\u73b0\u6709\u57fa\u51c6\u548c\u7f13\u89e3\u7b56\u7565\uff0c\u5e76\u63d0\u51fa\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "motivation": "\u7531\u4e8eCodeLLMs\u5728\u751f\u6210\u4ee3\u7801\u65f6\u5bb9\u6613\u4ea7\u751f\u96be\u4ee5\u8bc6\u522b\u7684\u5e7b\u89c9\uff08\u9519\u8bef\u6216\u8352\u8c2c\u4fe1\u606f\uff09\uff0c\u8fd9\u4e00\u95ee\u9898\u53ef\u80fd\u5bfc\u81f4\u4ee3\u7801\u5e93\u4e2d\u5b58\u5728\u672a\u88ab\u53d1\u73b0\u7684\u7f3a\u9677\uff0c\u56e0\u6b64\u9700\u8981\u7cfb\u7edf\u7814\u7a76\u3002", "method": "\u901a\u8fc7\u5206\u7c7bCodeLLMs\u751f\u6210\u7684\u4ee3\u7801\u4e2d\u7684\u5e7b\u89c9\u7c7b\u578b\uff0c\u5e76\u7efc\u8ff0\u73b0\u6709\u57fa\u51c6\u548c\u7f13\u89e3\u7b56\u7565\u3002", "result": "\u603b\u7ed3\u4e86\u5f53\u524d\u7814\u7a76\u8fdb\u5c55\uff0c\u6307\u51fa\u4e86\u5e7b\u89c9\u68c0\u6d4b\u548c\u6d88\u9664\u7684\u6311\u6218\u3002", "conclusion": "\u63d0\u51fa\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\uff0c\u4ee5\u6539\u8fdbCodeLLMs\u751f\u6210\u7684\u4ee3\u7801\u7684\u53ef\u9760\u6027\u3002"}}
{"id": "2504.20808", "pdf": "https://arxiv.org/pdf/2504.20808", "abs": "https://arxiv.org/abs/2504.20808", "authors": ["Florian Vahl", "J\u00f6rn Griepenburg", "Jan Gutsche", "Jasper G\u00fcldenstein", "Jianwei Zhang"], "title": "SoccerDiffusion: Toward Learning End-to-End Humanoid Robot Soccer from Gameplay Recordings", "categories": ["cs.RO", "cs.AI", "cs.LG"], "comment": null, "summary": "This paper introduces SoccerDiffusion, a transformer-based diffusion model\ndesigned to learn end-to-end control policies for humanoid robot soccer\ndirectly from real-world gameplay recordings. Using data collected from RoboCup\ncompetitions, the model predicts joint command trajectories from multi-modal\nsensor inputs, including vision, proprioception, and game state. We employ a\ndistillation technique to enable real-time inference on embedded platforms that\nreduces the multi-step diffusion process to a single step. Our results\ndemonstrate the model's ability to replicate complex motion behaviors such as\nwalking, kicking, and fall recovery both in simulation and on physical robots.\nAlthough high-level tactical behavior remains limited, this work provides a\nrobust foundation for subsequent reinforcement learning or preference\noptimization methods. We release the dataset, pretrained models, and code\nunder: https://bit-bots.github.io/SoccerDiffusion", "AI": {"tldr": "SoccerDiffusion\u662f\u4e00\u79cd\u57fa\u4e8eTransformer\u7684\u6269\u6563\u6a21\u578b\uff0c\u7528\u4e8e\u4ece\u771f\u5b9e\u8db3\u7403\u6bd4\u8d5b\u5f55\u50cf\u4e2d\u5b66\u4e60\u4eba\u5f62\u673a\u5668\u4eba\u8db3\u7403\u7684\u7aef\u5230\u7aef\u63a7\u5236\u7b56\u7565\u3002", "motivation": "\u901a\u8fc7\u4eceRoboCup\u6bd4\u8d5b\u4e2d\u6536\u96c6\u7684\u6570\u636e\uff0c\u76f4\u63a5\u5b66\u4e60\u590d\u6742\u7684\u8fd0\u52a8\u884c\u4e3a\uff08\u5982\u884c\u8d70\u3001\u8e22\u7403\u548c\u8dcc\u5012\u6062\u590d\uff09\uff0c\u4e3a\u4eba\u5f62\u673a\u5668\u4eba\u8db3\u7403\u63d0\u4f9b\u9ad8\u6548\u7684\u63a7\u5236\u7b56\u7565\u3002", "method": "\u4f7f\u7528\u591a\u6a21\u6001\u4f20\u611f\u5668\u8f93\u5165\uff08\u89c6\u89c9\u3001\u672c\u4f53\u611f\u89c9\u548c\u6e38\u620f\u72b6\u6001\uff09\u9884\u6d4b\u5173\u8282\u6307\u4ee4\u8f68\u8ff9\uff0c\u5e76\u901a\u8fc7\u84b8\u998f\u6280\u672f\u5c06\u591a\u6b65\u6269\u6563\u8fc7\u7a0b\u7b80\u5316\u4e3a\u5355\u6b65\u63a8\u7406\uff0c\u5b9e\u73b0\u5d4c\u5165\u5f0f\u5e73\u53f0\u7684\u5b9e\u65f6\u6027\u80fd\u3002", "result": "\u6a21\u578b\u5728\u4eff\u771f\u548c\u7269\u7406\u673a\u5668\u4eba\u4e0a\u6210\u529f\u590d\u73b0\u4e86\u590d\u6742\u8fd0\u52a8\u884c\u4e3a\uff0c\u4f46\u9ad8\u7ea7\u6218\u672f\u884c\u4e3a\u4ecd\u6709\u5c40\u9650\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u4e3a\u540e\u7eed\u5f3a\u5316\u5b66\u4e60\u6216\u504f\u597d\u4f18\u5316\u65b9\u6cd5\u63d0\u4f9b\u4e86\u575a\u5b9e\u57fa\u7840\uff0c\u5e76\u516c\u5f00\u4e86\u6570\u636e\u96c6\u3001\u9884\u8bad\u7ec3\u6a21\u578b\u548c\u4ee3\u7801\u3002"}}
{"id": "2504.20834", "pdf": "https://arxiv.org/pdf/2504.20834", "abs": "https://arxiv.org/abs/2504.20834", "authors": ["Alan Lee", "Harry Tong"], "title": "Reinforcement Learning for LLM Reasoning Under Memory Constraints", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "We explore reinforcement learning (RL) techniques to enhance reasoning within\ntargeted problem spaces in large language models (LLMs) under memory and\ncompute constraints. Our focus is on critic-free methods compatible with LoRA\nfine-tuning on a single 40GB GPU, a common limitation in academic settings. We\nintroduce S-GRPO, a memory-efficient variant of Group Relative Policy\nOptimization, and T-SPMO, a token-level prefix matching strategy for\nfine-grained credit assignment. Despite limited resources, when used to\nfine-tune Qwen2-1.5B both methods significantly improve SVAMP benchmark\naccuracy from 46% to above 70% using LoRA training. T-SPMO also excels in\nmulti-digit multiplication tasks, underscoring the potential of RL fine-tuning\nunder hardware constraints. Additionally, we find that our full-token GRPO\nbaseline under LoRA fine-tuning did not improve model performance (compared to\nbase model) on either task, suggesting that our memory-efficient methods may\nact as a form of regularization that stabilizes training when only a small\nsubset of parameters are updated.", "AI": {"tldr": "\u8bba\u6587\u63a2\u7d22\u4e86\u5728\u5185\u5b58\u548c\u8ba1\u7b97\u53d7\u9650\u6761\u4ef6\u4e0b\uff0c\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u6280\u672f\u63d0\u5347\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u63a8\u7406\u80fd\u529b\uff0c\u63d0\u51fa\u4e86\u4e24\u79cd\u9ad8\u6548\u65b9\u6cd5S-GRPO\u548cT-SPMO\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u5728\u5b66\u672f\u73af\u5883\u4e2d\uff0c\u8d44\u6e90\u6709\u9650\uff08\u5982\u5355\u575740GB GPU\uff09\uff0c\u9700\u8981\u5f00\u53d1\u9ad8\u6548\u7684RL\u65b9\u6cd5\u4ee5\u63d0\u5347LLMs\u7684\u63a8\u7406\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u4e86S-GRPO\uff08\u5185\u5b58\u9ad8\u6548\u7684GRPO\u53d8\u4f53\uff09\u548cT-SPMO\uff08\u57fa\u4e8e\u4ee4\u724c\u7ea7\u524d\u7f00\u5339\u914d\u7684\u4fe1\u7528\u5206\u914d\u7b56\u7565\uff09\uff0c\u5e76\u4e0eLoRA\u5fae\u8c03\u7ed3\u5408\u4f7f\u7528\u3002", "result": "\u5728SVAMP\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u6a21\u578b\u51c6\u786e\u7387\u4ece46%\u63d0\u5347\u81f370%\u4ee5\u4e0a\uff1bT-SPMO\u5728\u591a\u4f4d\u6570\u4e58\u6cd5\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "\u5185\u5b58\u9ad8\u6548\u65b9\u6cd5\u53ef\u80fd\u901a\u8fc7\u6b63\u5219\u5316\u4f5c\u7528\u7a33\u5b9a\u8bad\u7ec3\uff0c\u5c24\u5176\u662f\u5728\u4ec5\u66f4\u65b0\u5c11\u91cf\u53c2\u6570\u65f6\u3002"}}
{"id": "2504.20848", "pdf": "https://arxiv.org/pdf/2504.20848", "abs": "https://arxiv.org/abs/2504.20848", "authors": ["Junyuan Fang", "Huimin Liu", "Han Yang", "Jiajing Wu", "Zibin Zheng", "Chi K. Tse"], "title": "Mitigating the Structural Bias in Graph Adversarial Defenses", "categories": ["cs.LG", "cs.AI", "cs.CR"], "comment": "Under Review", "summary": "In recent years, graph neural networks (GNNs) have shown great potential in\naddressing various graph structure-related downstream tasks. However, recent\nstudies have found that current GNNs are susceptible to malicious adversarial\nattacks. Given the inevitable presence of adversarial attacks in the real\nworld, a variety of defense methods have been proposed to counter these attacks\nand enhance the robustness of GNNs. Despite the commendable performance of\nthese defense methods, we have observed that they tend to exhibit a structural\nbias in terms of their defense capability on nodes with low degree (i.e., tail\nnodes), which is similar to the structural bias of traditional GNNs on nodes\nwith low degree in the clean graph. Therefore, in this work, we propose a\ndefense strategy by including hetero-homo augmented graph construction, $k$NN\naugmented graph construction, and multi-view node-wise attention modules to\nmitigate the structural bias of GNNs against adversarial attacks. Notably, the\nhetero-homo augmented graph consists of removing heterophilic links (i.e.,\nlinks connecting nodes with dissimilar features) globally and adding homophilic\nlinks (i.e., links connecting nodes with similar features) for nodes with low\ndegree. To further enhance the defense capability, an attention mechanism is\nadopted to adaptively combine the representations from the above two kinds of\ngraph views. We conduct extensive experiments to demonstrate the defense and\ndebiasing effect of the proposed strategy on benchmark datasets.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u56fe\u795e\u7ecf\u7f51\u7edc\uff08GNNs\uff09\u5bf9\u6297\u653b\u51fb\u7684\u9632\u5fa1\u7b56\u7565\uff0c\u901a\u8fc7\u5f02\u8d28-\u540c\u8d28\u589e\u5f3a\u56fe\u6784\u5efa\u3001kNN\u589e\u5f3a\u56fe\u6784\u5efa\u548c\u591a\u89c6\u89d2\u8282\u70b9\u6ce8\u610f\u529b\u6a21\u5757\uff0c\u51cf\u5c11GNNs\u5728\u4f4e\u5ea6\u8282\u70b9\u4e0a\u7684\u7ed3\u6784\u504f\u5dee\u3002", "motivation": "\u73b0\u6709GNNs\u9632\u5fa1\u65b9\u6cd5\u5728\u4f4e\u5ea6\u8282\u70b9\uff08\u5c3e\u90e8\u8282\u70b9\uff09\u4e0a\u5b58\u5728\u7ed3\u6784\u504f\u5dee\uff0c\u7c7b\u4f3c\u4e8e\u4f20\u7edfGNNs\u5728\u5e72\u51c0\u56fe\u4e2d\u7684\u95ee\u9898\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u5747\u8861\u7684\u9632\u5fa1\u7b56\u7565\u3002", "method": "\u91c7\u7528\u5f02\u8d28-\u540c\u8d28\u589e\u5f3a\u56fe\u6784\u5efa\uff08\u79fb\u9664\u5f02\u8d28\u94fe\u63a5\u5e76\u6dfb\u52a0\u540c\u8d28\u94fe\u63a5\uff09\u3001kNN\u589e\u5f3a\u56fe\u6784\u5efa\u548c\u591a\u89c6\u89d2\u8282\u70b9\u6ce8\u610f\u529b\u6a21\u5757\uff0c\u81ea\u9002\u5e94\u7ed3\u5408\u4e0d\u540c\u56fe\u89c6\u56fe\u7684\u8868\u5f81\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\u8be5\u7b56\u7565\u5728\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u6709\u6548\u63d0\u5347\u4e86\u9632\u5fa1\u80fd\u529b\u5e76\u51cf\u5c11\u4e86\u7ed3\u6784\u504f\u5dee\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u4e0d\u4ec5\u589e\u5f3a\u4e86GNNs\u5bf9\u6297\u653b\u51fb\u7684\u9c81\u68d2\u6027\uff0c\u8fd8\u663e\u8457\u6539\u5584\u4e86\u4f4e\u5ea6\u8282\u70b9\u7684\u9632\u5fa1\u6027\u80fd\u3002"}}
{"id": "2504.20851", "pdf": "https://arxiv.org/pdf/2504.20851", "abs": "https://arxiv.org/abs/2504.20851", "authors": ["Qianrun Mao"], "title": "Fostering Self-Directed Growth with Generative AI: Toward a New Learning Analytics Framework", "categories": ["cs.CY", "cs.AI"], "comment": null, "summary": "In an era increasingly shaped by decentralized knowledge ecosystems and\npervasive AI technologies, fostering sustainable learner agency has become a\ncritical educational imperative. This study introduces a novel conceptual\nframework integrating Generative Artificial Intelligence and Learning Analytics\nto cultivate Self-Directed Growth, a dynamic competency that enables learners\nto iteratively drive their own developmental pathways across diverse\ncontexts.Building upon critical gaps in current research on Self Directed\nLearning and AI-mediated education, the proposed Aspire to Potentials for\nLearners (A2PL) model reconceptualizes the interplay of learner aspirations,\ncomplex thinking, and summative self-assessment within GAI supported\nenvironments.Methodological implications for future intervention design and\nlearning analytics applications are discussed, positioning Self-Directed Growth\nas a pivotal axis for developing equitable, adaptive, and sustainable learning\nsystems in the digital era.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faA2PL\u6a21\u578b\uff0c\u7ed3\u5408\u751f\u6210\u5f0fAI\u4e0e\u5b66\u4e60\u5206\u6790\uff0c\u652f\u6301\u5b66\u4e60\u8005\u5728\u591a\u6837\u5316\u73af\u5883\u4e2d\u5b9e\u73b0\u81ea\u6211\u5bfc\u5411\u6210\u957f\u3002", "motivation": "\u5f53\u524d\u5173\u4e8e\u81ea\u6211\u5bfc\u5411\u5b66\u4e60\u548cAI\u6559\u80b2\u7684\u7814\u7a76\u5b58\u5728\u5173\u952e\u7a7a\u767d\uff0c\u9700\u8981\u65b0\u7684\u6846\u67b6\u6765\u6574\u5408\u5b66\u4e60\u8005\u62b1\u8d1f\u3001\u590d\u6742\u601d\u7ef4\u548c\u603b\u7ed3\u6027\u81ea\u6211\u8bc4\u4f30\u3002", "method": "\u63d0\u51faA2PL\u6a21\u578b\uff0c\u6574\u5408\u751f\u6210\u5f0fAI\u4e0e\u5b66\u4e60\u5206\u6790\uff0c\u8bbe\u8ba1\u672a\u6765\u5e72\u9884\u63aa\u65bd\u548c\u5b66\u4e60\u5206\u6790\u5e94\u7528\u3002", "result": "A2PL\u6a21\u578b\u4e3a\u6570\u5b57\u65f6\u4ee3\u6784\u5efa\u516c\u5e73\u3001\u9002\u5e94\u6027\u5f3a\u4e14\u53ef\u6301\u7eed\u7684\u5b66\u4e60\u7cfb\u7edf\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002", "conclusion": "\u81ea\u6211\u5bfc\u5411\u6210\u957f\u662f\u6570\u5b57\u65f6\u4ee3\u5b66\u4e60\u7cfb\u7edf\u7684\u5173\u952e\uff0cA2PL\u6a21\u578b\u4e3a\u6b64\u63d0\u4f9b\u4e86\u7406\u8bba\u548c\u65b9\u6cd5\u652f\u6301\u3002"}}
{"id": "2504.20854", "pdf": "https://arxiv.org/pdf/2504.20854", "abs": "https://arxiv.org/abs/2504.20854", "authors": ["Jinsun Yoo", "ChonLam Lao", "Lianjie Cao", "Bob Lantz", "Minlan Yu", "Tushar Krishna", "Puneet Sharma"], "title": "Towards Easy and Realistic Network Infrastructure Testing for Large-scale Machine Learning", "categories": ["cs.NI", "cs.AI", "cs.DC", "cs.SY", "eess.SY"], "comment": "Presented as a poster in NSDI 25", "summary": "This paper lays the foundation for Genie, a testing framework that captures\nthe impact of real hardware network behavior on ML workload performance,\nwithout requiring expensive GPUs. Genie uses CPU-initiated traffic over a\nhardware testbed to emulate GPU to GPU communication, and adapts the ASTRA-sim\nsimulator to model interaction between the network and the ML workload.", "AI": {"tldr": "Genie\u662f\u4e00\u4e2a\u6d4b\u8bd5\u6846\u67b6\uff0c\u901a\u8fc7\u6a21\u62dfGPU\u901a\u4fe1\u548c\u7f51\u7edc\u4e0eML\u5de5\u4f5c\u8d1f\u8f7d\u7684\u4ea4\u4e92\uff0c\u8bc4\u4f30\u771f\u5b9e\u786c\u4ef6\u7f51\u7edc\u884c\u4e3a\u5bf9ML\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u65e0\u9700\u6602\u8d35GPU\u3002", "motivation": "\u7814\u7a76\u771f\u5b9e\u786c\u4ef6\u7f51\u7edc\u884c\u4e3a\u5bf9ML\u5de5\u4f5c\u8d1f\u8f7d\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u4f46\u907f\u514d\u4f7f\u7528\u6602\u8d35\u7684GPU\u8d44\u6e90\u3002", "method": "\u5229\u7528CPU\u53d1\u8d77\u7684\u6d41\u91cf\u6a21\u62dfGPU\u95f4\u901a\u4fe1\uff0c\u5e76\u6539\u9020ASTRA-sim\u6a21\u62df\u5668\u4ee5\u5efa\u6a21\u7f51\u7edc\u4e0eML\u5de5\u4f5c\u8d1f\u8f7d\u7684\u4ea4\u4e92\u3002", "result": "Genie\u80fd\u591f\u6709\u6548\u8bc4\u4f30\u7f51\u7edc\u884c\u4e3a\u5bf9ML\u6027\u80fd\u7684\u5f71\u54cd\u3002", "conclusion": "Genie\u4e3a\u4f4e\u6210\u672c\u7814\u7a76\u7f51\u7edc\u5bf9ML\u6027\u80fd\u7684\u5f71\u54cd\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2504.20859", "pdf": "https://arxiv.org/pdf/2504.20859", "abs": "https://arxiv.org/abs/2504.20859", "authors": ["Guy Hadad", "Haggai Roitman", "Yotam Eshel", "Bracha Shapira", "Lior Rokach"], "title": "X-Cross: Dynamic Integration of Language Models for Cross-Domain Sequential Recommendation", "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.LG"], "comment": "Accepted for publication in SIGIR '25", "summary": "As new products are emerging daily, recommendation systems are required to\nquickly adapt to possible new domains without needing extensive retraining.\nThis work presents ``X-Cross'' -- a novel cross-domain\nsequential-recommendation model that recommends products in new domains by\nintegrating several domain-specific language models; each model is fine-tuned\nwith low-rank adapters (LoRA). Given a recommendation prompt, operating layer\nby layer, X-Cross dynamically refines the representation of each source\nlanguage model by integrating knowledge from all other models. These refined\nrepresentations are propagated from one layer to the next, leveraging the\nactivations from each domain adapter to ensure domain-specific nuances are\npreserved while enabling adaptability across domains. Using Amazon datasets for\nsequential recommendation, X-Cross achieves performance comparable to a model\nthat is fine-tuned with LoRA, while using only 25% of the additional\nparameters. In cross-domain tasks, such as adapting from Toys domain to Tools,\nElectronics or Sports, X-Cross demonstrates robust performance, while requiring\nabout 50%-75% less fine-tuning data than LoRA to make fine-tuning effective.\nFurthermore, X-Cross achieves significant improvement in accuracy over\nalternative cross-domain baselines. Overall, X-Cross enables scalable and\nadaptive cross-domain recommendations, reducing computational overhead and\nproviding an efficient solution for data-constrained environments.", "AI": {"tldr": "X-Cross\u662f\u4e00\u79cd\u65b0\u578b\u8de8\u57df\u5e8f\u5217\u63a8\u8350\u6a21\u578b\uff0c\u901a\u8fc7\u96c6\u6210\u591a\u4e2a\u57df\u7279\u5b9a\u8bed\u8a00\u6a21\u578b\uff0c\u52a8\u6001\u4f18\u5316\u8868\u793a\uff0c\u51cf\u5c11\u53c2\u6570\u548c\u8bad\u7ec3\u6570\u636e\u9700\u6c42\uff0c\u63d0\u5347\u8de8\u57df\u63a8\u8350\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u63a8\u8350\u7cfb\u7edf\u5728\u65b0\u9886\u57df\u4e2d\u5feb\u901f\u9002\u5e94\u4e14\u65e0\u9700\u5927\u91cf\u91cd\u65b0\u8bad\u7ec3\u7684\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u4f4e\u79e9\u9002\u914d\u5668\uff08LoRA\uff09\u5fae\u8c03\u57df\u7279\u5b9a\u8bed\u8a00\u6a21\u578b\uff0c\u52a8\u6001\u6574\u5408\u5404\u6a21\u578b\u77e5\u8bc6\uff0c\u9010\u5c42\u4f18\u5316\u8868\u793a\u3002", "result": "\u5728\u4e9a\u9a6c\u900a\u6570\u636e\u96c6\u4e0a\uff0cX-Cross\u6027\u80fd\u63a5\u8fd1LoRA\u5fae\u8c03\u6a21\u578b\uff0c\u53c2\u6570\u51cf\u5c1175%\uff1b\u8de8\u57df\u4efb\u52a1\u4e2d\uff0c\u5fae\u8c03\u6570\u636e\u9700\u6c42\u51cf\u5c1150%-75%\uff0c\u51c6\u786e\u6027\u663e\u8457\u63d0\u5347\u3002", "conclusion": "X-Cross\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u7684\u8de8\u57df\u63a8\u8350\u89e3\u51b3\u65b9\u6848\uff0c\u964d\u4f4e\u8ba1\u7b97\u5f00\u9500\uff0c\u9002\u7528\u4e8e\u6570\u636e\u53d7\u9650\u73af\u5883\u3002"}}
{"id": "2504.20862", "pdf": "https://arxiv.org/pdf/2504.20862", "abs": "https://arxiv.org/abs/2504.20862", "authors": ["Dayananda Herurkar", "J\u00f6rn Hees", "Vesselin Tzvetkov", "Andreas Dengel"], "title": "Tabular Data Adapters: Improving Outlier Detection for Unlabeled Private Data", "categories": ["cs.LG", "cs.AI"], "comment": "outlier detection, tabular data, neural networks, weak annotations,\n  soft labeling, unsupervised approach", "summary": "The remarkable success of Deep Learning approaches is often based and\ndemonstrated on large public datasets. However, when applying such approaches\nto internal, private datasets, one frequently faces challenges arising from\nstructural differences in the datasets, domain shift, and the lack of labels.\nIn this work, we introduce Tabular Data Adapters (TDA), a novel method for\ngenerating soft labels for unlabeled tabular data in outlier detection tasks.\nBy identifying statistically similar public datasets and transforming private\ndata (based on a shared autoencoder) into a format compatible with\nstate-of-the-art public models, our approach enables the generation of weak\nlabels. It thereby can help to mitigate the cold start problem of labeling by\nbasing on existing outlier detection models for public datasets. In experiments\non 50 tabular datasets across different domains, we demonstrate that our method\nis able to provide more accurate annotations than baseline approaches while\nreducing computational time. Our approach offers a scalable, efficient, and\ncost-effective solution, to bridge the gap between public research models and\nreal-world industrial applications.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aTabular Data Adapters (TDA)\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u4e3a\u65e0\u6807\u7b7e\u7684\u8868\u683c\u6570\u636e\u751f\u6210\u8f6f\u6807\u7b7e\uff0c\u4ee5\u89e3\u51b3\u79c1\u6709\u6570\u636e\u96c6\u5728\u5f02\u5e38\u68c0\u6d4b\u4efb\u52a1\u4e2d\u7684\u51b7\u542f\u52a8\u95ee\u9898\u3002", "motivation": "\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u5728\u5927\u578b\u516c\u5171\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5728\u79c1\u6709\u6570\u636e\u96c6\u4e0a\u5e94\u7528\u65f6\uff0c\u5e38\u9762\u4e34\u7ed3\u6784\u5dee\u5f02\u3001\u9886\u57df\u504f\u79fb\u548c\u6807\u7b7e\u7f3a\u5931\u7684\u6311\u6218\u3002", "method": "\u901a\u8fc7\u8bc6\u522b\u7edf\u8ba1\u76f8\u4f3c\u7684\u516c\u5171\u6570\u636e\u96c6\uff0c\u5e76\u5229\u7528\u5171\u4eab\u81ea\u7f16\u7801\u5668\u5c06\u79c1\u6709\u6570\u636e\u8f6c\u6362\u4e3a\u4e0e\u516c\u5171\u6a21\u578b\u517c\u5bb9\u7684\u683c\u5f0f\uff0c\u751f\u6210\u5f31\u6807\u7b7e\u3002", "result": "\u572850\u4e2a\u4e0d\u540c\u9886\u57df\u7684\u8868\u683c\u6570\u636e\u96c6\u4e0a\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u63d0\u4f9b\u66f4\u51c6\u786e\u7684\u6807\u6ce8\uff0c\u540c\u65f6\u51cf\u5c11\u8ba1\u7b97\u65f6\u95f4\u3002", "conclusion": "TDA\u4e3a\u516c\u5171\u7814\u7a76\u6a21\u578b\u4e0e\u5de5\u4e1a\u5e94\u7528\u4e4b\u95f4\u7684\u5dee\u8ddd\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u3001\u9ad8\u6548\u4e14\u7ecf\u6d4e\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2504.20869", "pdf": "https://arxiv.org/pdf/2504.20869", "abs": "https://arxiv.org/abs/2504.20869", "authors": ["Junyuan Fang", "Han Yang", "Haixian Wen", "Jiajing Wu", "Zibin Zheng", "Chi K. Tse"], "title": "Quantifying the Noise of Structural Perturbations on Graph Adversarial Attacks", "categories": ["cs.LG", "cs.AI", "cs.CR"], "comment": "Ubder Review", "summary": "Graph neural networks have been widely utilized to solve graph-related tasks\nbecause of their strong learning power in utilizing the local information of\nneighbors. However, recent studies on graph adversarial attacks have proven\nthat current graph neural networks are not robust against malicious attacks.\nYet much of the existing work has focused on the optimization objective based\non attack performance to obtain (near) optimal perturbations, but paid less\nattention to the strength quantification of each perturbation such as the\ninjection of a particular node/link, which makes the choice of perturbations a\nblack-box model that lacks interpretability. In this work, we propose the\nconcept of noise to quantify the attack strength of each adversarial link.\nFurthermore, we propose three attack strategies based on the defined noise and\nclassification margins in terms of single and multiple steps optimization.\nExtensive experiments conducted on benchmark datasets against three\nrepresentative graph neural networks demonstrate the effectiveness of the\nproposed attack strategies. Particularly, we also investigate the preferred\npatterns of effective adversarial perturbations by analyzing the corresponding\nproperties of the selected perturbation nodes.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u566a\u58f0\u7684\u653b\u51fb\u5f3a\u5ea6\u91cf\u5316\u65b9\u6cd5\uff0c\u5e76\u8bbe\u8ba1\u4e86\u4e09\u79cd\u653b\u51fb\u7b56\u7565\uff0c\u4ee5\u589e\u5f3a\u56fe\u795e\u7ecf\u7f51\u7edc\u5bf9\u6297\u653b\u51fb\u7684\u53ef\u89e3\u91ca\u6027\u548c\u6709\u6548\u6027\u3002", "motivation": "\u5f53\u524d\u56fe\u795e\u7ecf\u7f51\u7edc\u5bf9\u6297\u653b\u51fb\u7814\u7a76\u591a\u5173\u6ce8\u653b\u51fb\u6027\u80fd\u4f18\u5316\uff0c\u800c\u5ffd\u7565\u4e86\u653b\u51fb\u5f3a\u5ea6\u7684\u91cf\u5316\uff0c\u5bfc\u81f4\u653b\u51fb\u9009\u62e9\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027\u3002", "method": "\u63d0\u51fa\u566a\u58f0\u6982\u5ff5\u91cf\u5316\u653b\u51fb\u5f3a\u5ea6\uff0c\u5e76\u57fa\u4e8e\u566a\u58f0\u548c\u5206\u7c7b\u8fb9\u754c\u8bbe\u8ba1\u5355\u6b65\u548c\u591a\u6b65\u4f18\u5316\u7684\u4e09\u79cd\u653b\u51fb\u7b56\u7565\u3002", "result": "\u5728\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u5bf9\u4e09\u79cd\u4ee3\u8868\u6027\u56fe\u795e\u7ecf\u7f51\u7edc\u7684\u5b9e\u9a8c\u8bc1\u660e\u4e86\u6240\u63d0\u653b\u51fb\u7b56\u7565\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u901a\u8fc7\u5206\u6790\u6240\u9009\u6270\u52a8\u8282\u70b9\u7684\u7279\u6027\uff0c\u63ed\u793a\u4e86\u6709\u6548\u5bf9\u6297\u6270\u52a8\u7684\u504f\u597d\u6a21\u5f0f\u3002"}}
{"id": "2504.20887", "pdf": "https://arxiv.org/pdf/2504.20887", "abs": "https://arxiv.org/abs/2504.20887", "authors": ["Harry Mead", "Clarissa Costen", "Bruno Lacerda", "Nick Hawes"], "title": "Return Capping: Sample-Efficient CVaR Policy Gradient Optimisation", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "When optimising for conditional value at risk (CVaR) using policy gradients\n(PG), current methods rely on discarding a large proportion of trajectories,\nresulting in poor sample efficiency. We propose a reformulation of the CVaR\noptimisation problem by capping the total return of trajectories used in\ntraining, rather than simply discarding them, and show that this is equivalent\nto the original problem if the cap is set appropriately. We show, with\nempirical results in an number of environments, that this reformulation of the\nproblem results in consistently improved performance compared to baselines.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdb\u7684\u6761\u4ef6\u98ce\u9669\u4ef7\u503c\uff08CVaR\uff09\u4f18\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u9650\u5236\u8bad\u7ec3\u8f68\u8ff9\u7684\u603b\u56de\u62a5\u800c\u975e\u4e22\u5f03\u5b83\u4eec\uff0c\u63d0\u9ad8\u4e86\u6837\u672c\u6548\u7387\u3002", "motivation": "\u5f53\u524d\u65b9\u6cd5\u5728\u4f18\u5316CVaR\u65f6\u4e22\u5f03\u5927\u91cf\u8f68\u8ff9\uff0c\u5bfc\u81f4\u6837\u672c\u6548\u7387\u4f4e\u4e0b\u3002", "method": "\u91cd\u65b0\u5b9a\u4e49CVaR\u4f18\u5316\u95ee\u9898\uff0c\u9650\u5236\u8bad\u7ec3\u8f68\u8ff9\u7684\u603b\u56de\u62a5\u800c\u975e\u4e22\u5f03\uff0c\u5e76\u8bc1\u660e\u5728\u9002\u5f53\u8bbe\u7f6e\u4e0b\u4e0e\u539f\u95ee\u9898\u7b49\u4ef7\u3002", "result": "\u5728\u591a\u4e2a\u73af\u5883\u4e2d\u5b9e\u9a8c\u8bc1\u660e\uff0c\u8be5\u65b9\u6cd5\u6027\u80fd\u4f18\u4e8e\u57fa\u7ebf\u3002", "conclusion": "\u901a\u8fc7\u9650\u5236\u56de\u62a5\u800c\u975e\u4e22\u5f03\u8f68\u8ff9\uff0c\u663e\u8457\u63d0\u9ad8\u4e86CVaR\u4f18\u5316\u7684\u6837\u672c\u6548\u7387\u548c\u6027\u80fd\u3002"}}
{"id": "2504.20903", "pdf": "https://arxiv.org/pdf/2504.20903", "abs": "https://arxiv.org/abs/2504.20903", "authors": ["Prothit Sen", "Sai Mihir Jakkaraju"], "title": "Modeling AI-Human Collaboration as a Multi-Agent Adaptation", "categories": ["cs.MA", "cs.AI", "cs.HC"], "comment": "Manuscript under review for the Special Issue: 'Can AI Do Strategy?'\n  at Strategy Science (May 1, 2025)", "summary": "We develop an agent-based simulation to formalize AI-human collaboration as a\nfunction of task structure, advancing a generalizable framework for strategic\ndecision-making in organizations. Distinguishing between heuristic-based human\nadaptation and rule-based AI search, we model interactions across modular\n(parallel) and sequenced (interdependent) tasks using an NK model. Our results\nreveal that in modular tasks, AI often substitutes for humans - delivering\nhigher payoffs unless human expertise is very high, and the AI search space is\neither narrowly focused or extremely broad. In sequenced tasks, interesting\ncomplementarities emerge. When an expert human initiates the search and AI\nsubsequently refines it, aggregate performance is maximized. Conversely, when\nAI leads, excessive heuristic refinement by the human can reduce payoffs. We\nalso show that even \"hallucinatory\" AI - lacking memory or structure - can\nimprove outcomes when augmenting low-capability humans by helping escape local\noptima. These results yield a robust implication: the effectiveness of AI-human\ncollaboration depends less on context or industry, and more on the underlying\ntask structure. By elevating task decomposition as the central unit of\nanalysis, our model provides a transferable lens for strategic decision-making\ninvolving humans and an agentic AI across diverse organizational settings.", "AI": {"tldr": "\u8bba\u6587\u901a\u8fc7\u57fa\u4e8e\u4ee3\u7406\u7684\u6a21\u62df\uff0c\u7814\u7a76\u4e86AI\u4e0e\u4eba\u7c7b\u534f\u4f5c\u5728\u4e0d\u540c\u4efb\u52a1\u7ed3\u6784\u4e0b\u7684\u8868\u73b0\uff0c\u63d0\u51fa\u4efb\u52a1\u7ed3\u6784\u662f\u51b3\u5b9a\u534f\u4f5c\u6548\u679c\u7684\u5173\u952e\u56e0\u7d20\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u63a2\u7d22AI\u4e0e\u4eba\u7c7b\u534f\u4f5c\u5728\u4e0d\u540c\u4efb\u52a1\u7ed3\u6784\u4e0b\u7684\u8868\u73b0\u5dee\u5f02\uff0c\u4e3a\u7ec4\u7ec7\u4e2d\u7684\u6218\u7565\u51b3\u7b56\u63d0\u4f9b\u901a\u7528\u6846\u67b6\u3002", "method": "\u4f7f\u7528\u57fa\u4e8e\u4ee3\u7406\u7684\u6a21\u62df\u548cNK\u6a21\u578b\uff0c\u533a\u5206\u542f\u53d1\u5f0f\u4eba\u7c7b\u9002\u5e94\u548c\u57fa\u4e8e\u89c4\u5219\u7684AI\u641c\u7d22\uff0c\u5206\u6790\u6a21\u5757\u5316\u548c\u5e8f\u5217\u5316\u4efb\u52a1\u4e2d\u7684\u4ea4\u4e92\u3002", "result": "\u6a21\u5757\u5316\u4efb\u52a1\u4e2d\uff0cAI\u901a\u5e38\u66ff\u4ee3\u4eba\u7c7b\uff1b\u5e8f\u5217\u5316\u4efb\u52a1\u4e2d\uff0c\u4eba\u7c7b\u4e13\u5bb6\u542f\u52a8\u641c\u7d22\u5e76\u7531AI\u4f18\u5316\u65f6\u6548\u679c\u6700\u4f73\u3002AI\u5e7b\u89c9\u4e5f\u80fd\u5e2e\u52a9\u4f4e\u80fd\u529b\u4eba\u7c7b\u7a81\u7834\u5c40\u90e8\u6700\u4f18\u3002", "conclusion": "AI\u4e0e\u4eba\u7c7b\u534f\u4f5c\u7684\u6548\u679c\u4e3b\u8981\u53d6\u51b3\u4e8e\u4efb\u52a1\u7ed3\u6784\uff0c\u800c\u975e\u4e0a\u4e0b\u6587\u6216\u884c\u4e1a\u3002\u4efb\u52a1\u5206\u89e3\u662f\u5206\u6790\u7684\u6838\u5fc3\u5355\u5143\uff0c\u6a21\u578b\u9002\u7528\u4e8e\u591a\u6837\u5316\u7ec4\u7ec7\u73af\u5883\u3002"}}
{"id": "2504.20910", "pdf": "https://arxiv.org/pdf/2504.20910", "abs": "https://arxiv.org/abs/2504.20910", "authors": ["Sachin R. Pendse", "Darren Gergle", "Rachel Kornfield", "Jonah Meyerhoff", "David Mohr", "Jina Suh", "Annie Wescott", "Casey Williams", "Jessica Schleider"], "title": "When Testing AI Tests Us: Safeguarding Mental Health on the Digital Frontlines", "categories": ["cs.CY", "cs.AI", "cs.HC"], "comment": "Accepted to ACM Conference on Fairness, Accountability, and\n  Transparency (FAccT 2025)", "summary": "Red-teaming is a core part of the infrastructure that ensures that AI models\ndo not produce harmful content. Unlike past technologies, the black box nature\nof generative AI systems necessitates a uniquely interactional mode of testing,\none in which individuals on red teams actively interact with the system,\nleveraging natural language to simulate malicious actors and solicit harmful\noutputs. This interactional labor done by red teams can result in mental health\nharms that are uniquely tied to the adversarial engagement strategies necessary\nto effectively red team. The importance of ensuring that generative AI models\ndo not propagate societal or individual harm is widely recognized -- one less\nvisible foundation of end-to-end AI safety is also the protection of the mental\nhealth and wellbeing of those who work to keep model outputs safe. In this\npaper, we argue that the unmet mental health needs of AI red-teamers is a\ncritical workplace safety concern. Through analyzing the unique mental health\nimpacts associated with the labor done by red teams, we propose potential\nindividual and organizational strategies that could be used to meet these\nneeds, and safeguard the mental health of red-teamers. We develop our proposed\nstrategies through drawing parallels between common red-teaming practices and\ninteractional labor common to other professions (including actors, mental\nhealth professionals, conflict photographers, and content moderators),\ndescribing how individuals and organizations within these professional spaces\nsafeguard their mental health given similar psychological demands. Drawing on\nthese protective practices, we describe how safeguards could be adapted for the\ndistinct mental health challenges experienced by red teaming organizations as\nthey mitigate emerging technological risks on the new digital frontlines.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86AI\u7ea2\u961f\u6d4b\u8bd5\u5458\u7684\u5fc3\u7406\u5065\u5eb7\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4fdd\u62a4\u63aa\u65bd\u3002", "motivation": "\u751f\u6210\u5f0fAI\u7684\u9ed1\u7bb1\u7279\u6027\u5bfc\u81f4\u7ea2\u961f\u6d4b\u8bd5\u5458\u9700\u4e0e\u7cfb\u7edf\u4e92\u52a8\u4ee5\u6a21\u62df\u6076\u610f\u884c\u4e3a\uff0c\u53ef\u80fd\u5f15\u53d1\u5fc3\u7406\u5065\u5eb7\u95ee\u9898\uff0c\u9700\u5173\u6ce8\u5176\u804c\u4e1a\u5b89\u5168\u3002", "method": "\u901a\u8fc7\u5206\u6790\u7ea2\u961f\u6d4b\u8bd5\u7684\u72ec\u7279\u5fc3\u7406\u5f71\u54cd\uff0c\u5e76\u501f\u9274\u5176\u4ed6\u804c\u4e1a\uff08\u5982\u6f14\u5458\u3001\u5fc3\u7406\u5065\u5eb7\u4e13\u5bb6\u7b49\uff09\u7684\u4fdd\u62a4\u63aa\u65bd\uff0c\u63d0\u51fa\u5e94\u5bf9\u7b56\u7565\u3002", "result": "\u63d0\u51fa\u4e86\u9488\u5bf9\u7ea2\u961f\u6d4b\u8bd5\u5458\u7684\u4e2a\u4f53\u548c\u7ec4\u7ec7\u5c42\u9762\u7684\u5fc3\u7406\u5065\u5eb7\u4fdd\u62a4\u7b56\u7565\u3002", "conclusion": "\u7ea2\u961f\u6d4b\u8bd5\u5458\u7684\u5fc3\u7406\u5065\u5eb7\u662fAI\u5b89\u5168\u7684\u91cd\u8981\u7ec4\u6210\u90e8\u5206\uff0c\u9700\u91c7\u53d6\u5177\u4f53\u63aa\u65bd\u4fdd\u62a4\u5176\u798f\u7949\u3002"}}
{"id": "2504.20922", "pdf": "https://arxiv.org/pdf/2504.20922", "abs": "https://arxiv.org/abs/2504.20922", "authors": ["Miguel Nogales", "Matteo Gambella", "Manuel Roveri"], "title": "DYNAMAX: Dynamic computing for Transformers and Mamba based architectures", "categories": ["cs.CL", "cs.AI", "cs.LG", "68T50 (Primary), 68T07 (Secondary)"], "comment": "Accepted to IJCNN 2025", "summary": "Early exits (EEs) offer a promising approach to reducing computational costs\nand latency by dynamically terminating inference once a satisfactory prediction\nconfidence on a data sample is achieved. Although many works integrate EEs into\nencoder-only Transformers, their application to decoder-only architectures and,\nmore importantly, Mamba models, a novel family of state-space architectures in\nthe LLM realm, remains insufficiently explored. This work introduces DYNAMAX,\nthe first framework to exploit the unique properties of Mamba architectures for\nearly exit mechanisms. We not only integrate EEs into Mamba but also repurpose\nMamba as an efficient EE classifier for both Mamba-based and transformer-based\nLLMs, showcasing its versatility. Our experiments employ the Mistral 7B\ntransformer compared to the Codestral 7B Mamba model, using data sets such as\nTruthfulQA, CoQA, and TriviaQA to evaluate computational savings, accuracy, and\nconsistency. The results highlight the adaptability of Mamba as a powerful EE\nclassifier and its efficiency in balancing computational cost and performance\nquality across NLP tasks. By leveraging Mamba's inherent design for dynamic\nprocessing, we open pathways for scalable and efficient inference in embedded\napplications and resource-constrained environments. This study underscores the\ntransformative potential of Mamba in redefining dynamic computing paradigms for\nLLMs.", "AI": {"tldr": "DYNAMAX\u6846\u67b6\u9996\u6b21\u5c06\u65e9\u671f\u9000\u51fa\u673a\u5236\u5e94\u7528\u4e8eMamba\u67b6\u6784\uff0c\u5e76\u5c55\u793a\u5176\u4f5c\u4e3a\u9ad8\u6548\u5206\u7c7b\u5668\u7684\u6f5c\u529b\uff0c\u5e73\u8861\u8ba1\u7b97\u6210\u672c\u4e0e\u6027\u80fd\u3002", "motivation": "\u63a2\u7d22\u65e9\u671f\u9000\u51fa\u673a\u5236\u5728Mamba\u67b6\u6784\u4e2d\u7684\u5e94\u7528\uff0c\u586b\u8865\u5176\u5728\u89e3\u7801\u5668\u6a21\u578b\u4e2d\u7684\u7814\u7a76\u7a7a\u767d\u3002", "method": "\u5c06\u65e9\u671f\u9000\u51fa\u673a\u5236\u96c6\u6210\u5230Mamba\u4e2d\uff0c\u5e76\u5229\u7528Mamba\u4f5c\u4e3a\u5206\u7c7b\u5668\uff0c\u5bf9\u6bd4Mistral 7B\u548cCodestral 7B\u6a21\u578b\u3002", "result": "Mamba\u5728\u8ba1\u7b97\u8282\u7701\u3001\u51c6\u786e\u6027\u548c\u4e00\u81f4\u6027\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u9002\u5408\u8d44\u6e90\u53d7\u9650\u73af\u5883\u3002", "conclusion": "Mamba\u7684\u52a8\u6001\u5904\u7406\u80fd\u529b\u4e3a\u9ad8\u6548\u63a8\u7406\u63d0\u4f9b\u4e86\u65b0\u9014\u5f84\uff0c\u5177\u6709\u53d8\u9769\u6027\u6f5c\u529b\u3002"}}
{"id": "2504.20946", "pdf": "https://arxiv.org/pdf/2504.20946", "abs": "https://arxiv.org/abs/2504.20946", "authors": ["Tyler McDonald", "Ali Emami"], "title": "Trace-of-Thought: Enhanced Arithmetic Problem Solving via Reasoning Distillation From Large to Small Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "As Large Language Models (LLMs) continue to be leveraged for daily tasks,\nprompt engineering remains an active field of contribution within computational\nlinguistics, particularly in domains requiring specialized knowledge such as\narithmetic reasoning. While these LLMs are optimized for a variety of tasks,\ntheir exhaustive employment may become computationally or financially\ncumbersome for small teams. Additionally, complete reliance on proprietary,\nclosed-source models often limits customization and adaptability, posing\nsignificant challenges in research and application scalability. Instead, by\nleveraging open-source models at or below 7 billion parameters, we can optimize\nour resource usage while still observing remarkable gains over standard\nprompting approaches. To cultivate this notion, we introduce Trace-of-Thought\nPrompting, a simple, zero-shot prompt engineering method that instructs LLMs to\ncreate observable subproblems using critical problem-solving, specifically\ndesigned to enhance arithmetic reasoning capabilities. When applied to\nopen-source models in tandem with GPT-4, we observe that Trace-of-Thought not\nonly allows novel insight into the problem-solving process but also introduces\nperformance gains as large as 125% on language models at or below 7 billion\nparameters. This approach underscores the potential of open-source initiatives\nin democratizing AI research and improving the accessibility of high-quality\ncomputational linguistics applications.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aTrace-of-Thought Prompting\u7684\u96f6\u6837\u672c\u63d0\u793a\u5de5\u7a0b\u65b9\u6cd5\uff0c\u65e8\u5728\u901a\u8fc7\u5f00\u6e90\u6a21\u578b\uff08\u53c2\u6570\u22647B\uff09\u4f18\u5316\u7b97\u672f\u63a8\u7406\u80fd\u529b\uff0c\u6027\u80fd\u63d0\u5347\u9ad8\u8fbe125%\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u4e13\u4e1a\u9886\u57df\uff08\u5982\u7b97\u672f\u63a8\u7406\uff09\u7684\u5e94\u7528\u5b58\u5728\u8ba1\u7b97\u6210\u672c\u9ad8\u3001\u4f9d\u8d56\u95ed\u6e90\u6a21\u578b\u7b49\u95ee\u9898\uff0c\u9650\u5236\u4e86\u53ef\u5b9a\u5236\u6027\u548c\u53ef\u6269\u5c55\u6027\u3002", "method": "\u63d0\u51faTrace-of-Thought Prompting\u65b9\u6cd5\uff0c\u6307\u5bfcLLMs\u901a\u8fc7\u53ef\u89c2\u5bdf\u7684\u5b50\u95ee\u9898\u89e3\u51b3\u8fc7\u7a0b\u589e\u5f3a\u7b97\u672f\u63a8\u7406\u80fd\u529b\u3002", "result": "\u5728\u5f00\u6e90\u6a21\u578b\uff08\u53c2\u6570\u22647B\uff09\u4e0eGPT-4\u7ed3\u5408\u4f7f\u7528\u65f6\uff0c\u6027\u80fd\u63d0\u5347\u9ad8\u8fbe125%\u3002", "conclusion": "\u5f00\u6e90\u6a21\u578b\u7ed3\u5408Trace-of-Thought Prompting\u53ef\u663e\u8457\u63d0\u5347\u6027\u80fd\uff0c\u63a8\u52a8AI\u7814\u7a76\u7684\u6c11\u4e3b\u5316\u548c\u9ad8\u8d28\u91cf\u8ba1\u7b97\u8bed\u8a00\u5b66\u5e94\u7528\u7684\u53ef\u53ca\u6027\u3002"}}
{"id": "2504.20964", "pdf": "https://arxiv.org/pdf/2504.20964", "abs": "https://arxiv.org/abs/2504.20964", "authors": ["Shangyu Li", "Juyong Jiang", "Tiancheng Zhao", "Jiasi Shen"], "title": "OSVBench: Benchmarking LLMs on Specification Generation Tasks for Operating System Verification", "categories": ["cs.CL", "cs.AI", "cs.OS", "cs.PL", "cs.SE"], "comment": null, "summary": "We introduce OSVBench, a new benchmark for evaluating Large Language Models\n(LLMs) in generating complete specification code pertaining to operating system\nkernel verification tasks. The benchmark first defines the specification\ngeneration problem into a program synthesis problem within a confined scope of\nsyntax and semantics by providing LLMs with the programming model. The LLMs are\nrequired to understand the provided verification assumption and the potential\nsyntax and semantics space to search for, then generate the complete\nspecification for the potentially buggy operating system code implementation\nunder the guidance of the high-level functional description of the operating\nsystem. This benchmark is built upon a real-world operating system kernel,\nHyperkernel, and consists of 245 complex specification generation tasks in\ntotal, each is a long context task of about 20k-30k tokens. Our comprehensive\nevaluation of 12 LLMs exhibits the limited performance of the current LLMs on\nthe specification generation tasks for operating system verification.\nSignificant disparities in their performance on the benchmark highlight\ndifferences in their ability to handle long-context code generation tasks. The\nevaluation toolkit and benchmark are available at\nhttps://github.com/lishangyu-hkust/OSVBench.", "AI": {"tldr": "OSVBench\u662f\u4e00\u4e2a\u65b0\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u64cd\u4f5c\u7cfb\u7edf\u5185\u6838\u9a8c\u8bc1\u4efb\u52a1\u4e2d\u751f\u6210\u5b8c\u6574\u89c4\u8303\u4ee3\u7801\u7684\u80fd\u529b\u3002", "motivation": "\u5f53\u524dLLM\u5728\u64cd\u4f5c\u7cfb\u7edf\u9a8c\u8bc1\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u6709\u9650\uff0c\u9700\u8981\u8bc4\u4f30\u5176\u5728\u957f\u4e0a\u4e0b\u6587\u4ee3\u7801\u751f\u6210\u4efb\u52a1\u4e2d\u7684\u80fd\u529b\u3002", "method": "\u5c06\u89c4\u8303\u751f\u6210\u95ee\u9898\u5b9a\u4e49\u4e3a\u7a0b\u5e8f\u5408\u6210\u95ee\u9898\uff0c\u63d0\u4f9b\u7f16\u7a0b\u6a21\u578b\u548c\u9a8c\u8bc1\u5047\u8bbe\uff0c\u8981\u6c42LLM\u751f\u6210\u5b8c\u6574\u89c4\u8303\u3002", "result": "\u5bf912\u4e2aLLM\u7684\u7efc\u5408\u8bc4\u4f30\u663e\u793a\u5176\u5728\u64cd\u4f5c\u7cfb\u7edf\u9a8c\u8bc1\u4efb\u52a1\u4e2d\u8868\u73b0\u6709\u9650\uff0c\u4e14\u6027\u80fd\u5dee\u5f02\u663e\u8457\u3002", "conclusion": "OSVBench\u63ed\u793a\u4e86LLM\u5728\u957f\u4e0a\u4e0b\u6587\u4ee3\u7801\u751f\u6210\u4efb\u52a1\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u8bc4\u4f30\u5de5\u5177\u3002"}}
{"id": "2504.20988", "pdf": "https://arxiv.org/pdf/2504.20988", "abs": "https://arxiv.org/abs/2504.20988", "authors": ["Atul Sharma", "Kavindu Herath", "Saurabh Bagchi", "Chaoyue Liu", "Somali Chaterji"], "title": "Hubs and Spokes Learning: Efficient and Scalable Collaborative Machine Learning", "categories": ["cs.LG", "cs.AI", "cs.DC"], "comment": null, "summary": "We introduce the Hubs and Spokes Learning (HSL) framework, a novel paradigm\nfor collaborative machine learning that combines the strengths of Federated\nLearning (FL) and Decentralized Learning (P2PL). HSL employs a two-tier\ncommunication structure that avoids the single point of failure inherent in FL\nand outperforms the state-of-the-art P2PL framework, Epidemic Learning Local\n(ELL). At equal communication budgets (total edges), HSL achieves higher\nperformance than ELL, while at significantly lower communication budgets, it\ncan match ELL's performance. For instance, with only 400 edges, HSL reaches the\nsame test accuracy that ELL achieves with 1000 edges for 100 peers (spokes) on\nCIFAR-10, demonstrating its suitability for resource-constrained systems. HSL\nalso achieves stronger consensus among nodes after mixing, resulting in\nimproved performance with fewer training rounds. We substantiate these claims\nthrough rigorous theoretical analyses and extensive experimental results,\nshowcasing HSL's practicality for large-scale collaborative learning.", "AI": {"tldr": "HSL\u6846\u67b6\u7ed3\u5408\u4e86\u8054\u90a6\u5b66\u4e60\u548c\u53bb\u4e2d\u5fc3\u5316\u5b66\u4e60\u7684\u4f18\u52bf\uff0c\u901a\u8fc7\u53cc\u5c42\u901a\u4fe1\u7ed3\u6784\u907f\u514d\u5355\u70b9\u6545\u969c\uff0c\u5e76\u5728\u76f8\u540c\u6216\u66f4\u4f4e\u901a\u4fe1\u9884\u7b97\u4e0b\u4f18\u4e8e\u73b0\u6709P2PL\u6846\u67b6\u3002", "motivation": "\u89e3\u51b3\u8054\u90a6\u5b66\u4e60\u7684\u5355\u70b9\u6545\u969c\u95ee\u9898\uff0c\u540c\u65f6\u63d0\u5347\u53bb\u4e2d\u5fc3\u5316\u5b66\u4e60\u7684\u6548\u7387\u548c\u6027\u80fd\u3002", "method": "\u91c7\u7528\u53cc\u5c42\u901a\u4fe1\u7ed3\u6784\uff08Hubs\u548cSpokes\uff09\uff0c\u4f18\u5316\u901a\u4fe1\u6548\u7387\u548c\u8282\u70b9\u5171\u8bc6\u3002", "result": "\u5728\u76f8\u540c\u901a\u4fe1\u9884\u7b97\u4e0b\u6027\u80fd\u4f18\u4e8eELL\uff0c\u4f4e\u9884\u7b97\u65f6\u4e5f\u80fd\u5339\u914d\u5176\u6027\u80fd\uff0c\u8282\u70b9\u5171\u8bc6\u66f4\u5f3a\u3002", "conclusion": "HSL\u9002\u5408\u8d44\u6e90\u53d7\u9650\u7cfb\u7edf\uff0c\u5177\u6709\u5927\u89c4\u6a21\u534f\u4f5c\u5b66\u4e60\u7684\u5b9e\u7528\u6027\u3002"}}
{"id": "2504.20997", "pdf": "https://arxiv.org/pdf/2504.20997", "abs": "https://arxiv.org/abs/2504.20997", "authors": ["Dilip Arumugam", "Thomas L. Griffiths"], "title": "Toward Efficient Exploration by Large Language Model Agents", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "A burgeoning area within reinforcement learning (RL) is the design of\nsequential decision-making agents centered around large language models (LLMs).\nWhile autonomous decision-making agents powered by modern LLMs could facilitate\nnumerous real-world applications, such successes demand agents that are capable\nof data-efficient RL. One key obstacle to achieving data efficiency in RL is\nexploration, a challenge that we demonstrate many recent proposals for LLM\nagent designs struggle to contend with. Meanwhile, classic algorithms from the\nRL literature known to gracefully address exploration require technical\nmachinery that can be challenging to operationalize in purely natural language\nsettings. In this work, rather than relying on finetuning or in-context\nlearning to coax LLMs into implicitly imitating a RL algorithm, we illustrate\nhow LLMs can be used to explicitly implement an existing RL algorithm\n(Posterior Sampling for Reinforcement Learning) whose capacity for\nstatistically-efficient exploration is already well-studied. We offer empirical\nresults demonstrating how our LLM-based implementation of a known,\ndata-efficient RL algorithm can be considerably more effective in natural\nlanguage tasks that demand prudent exploration.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u5982\u4f55\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5b9e\u73b0\u6570\u636e\u9ad8\u6548\u7684\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u663e\u5f0f\u5b9e\u73b0\u73b0\u6709RL\u7b97\u6cd5\u7684\u65b9\u6cd5\uff0c\u4ee5\u89e3\u51b3\u63a2\u7d22\u6548\u7387\u95ee\u9898\u3002", "motivation": "\u73b0\u4ee3LLM\u9a71\u52a8\u7684\u81ea\u4e3b\u51b3\u7b56\u4ee3\u7406\u5728\u73b0\u5b9e\u5e94\u7528\u4e2d\u6f5c\u529b\u5de8\u5927\uff0c\u4f46\u9700\u8981\u6570\u636e\u9ad8\u6548\u7684RL\u65b9\u6cd5\uff0c\u800c\u73b0\u6709LLM\u4ee3\u7406\u8bbe\u8ba1\u5728\u63a2\u7d22\u6548\u7387\u4e0a\u8868\u73b0\u4e0d\u4f73\u3002", "method": "\u901a\u8fc7\u663e\u5f0f\u5b9e\u73b0\u4e00\u79cd\u5df2\u77e5\u7684RL\u7b97\u6cd5\uff08\u540e\u9a8c\u91c7\u6837\u5f3a\u5316\u5b66\u4e60\uff09\uff0c\u800c\u975e\u4f9d\u8d56\u5fae\u8c03\u6216\u4e0a\u4e0b\u6587\u5b66\u4e60\uff0c\u6765\u63d0\u5347LLM\u5728\u81ea\u7136\u8bed\u8a00\u4efb\u52a1\u4e2d\u7684\u63a2\u7d22\u6548\u7387\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u9700\u8981\u8c28\u614e\u63a2\u7d22\u7684\u81ea\u7136\u8bed\u8a00\u4efb\u52a1\u4e2d\u663e\u8457\u63d0\u5347\u4e86\u6548\u679c\u3002", "conclusion": "\u663e\u5f0f\u5b9e\u73b0\u73b0\u6709RL\u7b97\u6cd5\u662f\u63d0\u5347LLM\u4ee3\u7406\u6570\u636e\u6548\u7387\u7684\u6709\u6548\u9014\u5f84\u3002"}}
