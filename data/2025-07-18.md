[[toc]]

## cs.CV

### [1] [Spatially Grounded Explanations in Vision Language Models for Document Visual Question Answering](https://arxiv.org/abs/2507.12490)
*Maximiliano Hormazábal Lagos,Héctor Cerezo-Costas,Dimosthenis Karatzas*

Main category: cs.CV

TL;DR: 本文提出了EaGERS，一个无需训练的模型无关管道，通过生成自然语言推理、将推理定位到空间子区域、并限制从相关区域生成回答来改进文档视觉问答(DocVQA)的性能和可解释性。

- Motivation: 现有的文档视觉问答系统缺乏透明度和可解释性，需要一种能够提供推理过程并定位到具体文档区域的方法，同时避免额外的模型微调成本。
- Method: EaGERS采用三步管道：(1)通过视觉语言模型生成自然语言推理；(2)通过计算可配置网格上的多模态嵌入相似性并采用多数投票将推理定位到空间子区域；(3)限制仅从掩码图像中选定的相关区域生成回答。
- Result: 在DocVQA数据集上的实验表明，最佳配置在精确匹配准确率和平均标准化编辑距离相似性指标上都优于基础模型，同时增强了DocVQA的透明度和可重现性。
- Conclusion: EaGERS成功实现了无需额外模型微调就能提升文档视觉问答性能和可解释性的目标，为该领域提供了一个实用且透明的解决方案。


### [2] [MindJourney: Test-Time Scaling with World Models for Spatial Reasoning](https://arxiv.org/abs/2507.12508)
*Yuncong Yang,Jiageng Liu,Zheyuan Zhang,Siyuan Zhou,Reuben Tan,Jianwei Yang,Yilun Du,Chuang Gan*

Main category: cs.CV

TL;DR: MindJourney通过将视觉语言模型（VLM）与基于视频扩散的可控世界模型结合，提升了3D空间推理能力，无需微调即可在SAT基准上平均提升8%性能。

- Motivation: 现有视觉语言模型缺乏对3D动态的内部建模，导致在简单任务（如预测自我运动后的场景）中表现不佳。
- Method: MindJourney框架通过VLM迭代生成相机轨迹，世界模型合成对应视图，VLM基于多视图证据进行推理。
- Result: 在SAT基准上平均性能提升8%，优于基于强化学习的测试时推理VLM。
- Conclusion: 结合世界模型进行测试时扩展是提升3D推理能力的简单有效方法。


### [3] [Mono-InternVL-1.5: Towards Cheaper and Faster Monolithic Multimodal Large Language Models](https://arxiv.org/abs/2507.12566)
*Gen Luo,Wenhan Dou,Wenhao Li,Zhaokai Wang,Xue Yang,Changyao Tian,Hao Li,Weiyun Wang,Wenhai Wang,Xizhou Zhu,Yu Qiao,Jifeng Dai*

Main category: cs.CV

TL;DR: 该论文提出了一种新型的单模态多模态大语言模型（MLLM）Mono-InternVL及其改进版Mono-InternVL-1.5，通过嵌入视觉参数空间和创新的预训练策略，解决了现有模型的优化不稳定和灾难性遗忘问题。

- Motivation: 现有单模态MLLM在视觉编码和语言解码集成时存在优化不稳定和灾难性遗忘问题，需要一种更稳定的方法。
- Method: 提出Mono-InternVL，通过多模态专家混合架构嵌入视觉参数空间，并设计Endogenous Visual Pre-training（EViP）策略；进一步改进为Mono-InternVL-1.5，采用EViP++优化预训练过程。
- Result: Mono-InternVL在15个基准测试中优于现有模型，如OCRBench上提升114分；Mono-InternVL-1.5在保持性能的同时显著降低训练和推理成本。
- Conclusion: Mono-InternVL及其改进版通过创新的视觉参数嵌入和预训练策略，显著提升了单模态MLLM的性能和效率。


### [4] [Best Practices for Large-Scale, Pixel-Wise Crop Mapping and Transfer Learning Workflows](https://arxiv.org/abs/2507.12590)
*Judy Long,Tao Liu,Sean Alexander Woznicki,Miljana Marković,Oskar Marko,Molly Sears*

Main category: cs.CV

TL;DR: 本文综述了大规模像素级作物分类的工作流程，比较了传统监督方法和新兴迁移学习方法，并提出了最佳实践。

- Motivation: 研究旨在通过系统实验确定最优的监督作物分类工作流程，并评估迁移学习在不同领域偏移下的效果。
- Method: 比较了六种卫星图像预处理方法和十一种监督分类模型，同时评估了不同训练样本量和变量组合的影响，以及迁移学习技术。
- Result: 发现精细预处理与Transformer模型表现最佳；迁移学习提高了适应性，UDA适用于同质作物类，微调适用于多样场景；工作流程选择取决于样本量。
- Conclusion: 监督学习在样本充足时更优，样本不足时迁移学习是可行替代方案。


### [5] [CT-ScanGaze: A Dataset and Baselines for 3D Volumetric Scanpath Modeling](https://arxiv.org/abs/2507.12591)
*Trong-Thang Pham,Akash Awasthi,Saba Khan,Esteban Duran Marti,Tien-Phat Nguyen,Khoa Vo,Minh Tran,Ngoc Son Nguyen,Cuong Tran Van,Yuki Ikebe,Anh Totti Nguyen,Anh Nguyen,Zhigang Deng,Carol C. Wu,Hien Van Nguyen,Ngan Le*

Main category: cs.CV

TL;DR: 论文提出了首个公开的CT眼动数据集CT-ScanGaze，并开发了3D扫描路径预测模型CT-Searcher，解决了现有模型仅支持2D输入的问题。通过将2D眼动数据转换为3D数据进行预训练，提升了模型性能。

- Motivation: 理解放射科医生在CT阅读中的眼动行为对开发可解释的计算机辅助诊断系统至关重要，但现有研究因缺乏公开数据集和CT三维复杂性而受限。
- Method: 提出CT-ScanGaze数据集和CT-Searcher模型，利用2D眼动数据转换为3D数据进行预训练，生成放射科医生类似的3D注视序列。
- Result: 在CT-ScanGaze数据集上的定性和定量评估证明了方法的有效性，并提供了医学影像中3D扫描路径预测的评估框架。
- Conclusion: CT-Searcher和CT-ScanGaze为医学影像中的3D眼动研究提供了重要工具和数据集，推动了可解释辅助诊断系统的发展。


### [6] [MS-DGCNN++: A Multi-Scale Fusion Dynamic Graph Neural Network with Biological Knowledge Integration for LiDAR Tree Species Classification](https://arxiv.org/abs/2507.12602)
*Said Ohamouddou,Abdellatif El Afia,Hanaa El Afia,Raddouane Chiheb*

Main category: cs.CV

TL;DR: MS-DGCNN++是一种改进的多尺度动态图卷积网络，通过层次化多尺度融合和语义特征提取，显著提升了树种类分类和3D物体识别的准确性。

- Motivation: 解决现有方法在树种类分类中未能捕捉树结构层次语义关系的问题。
- Method: 采用层次化多尺度融合动态图卷积网络，结合局部、分支和冠层尺度的语义特征提取与跨尺度信息传播。
- Result: 在STPCTLS上达到94.96%准确率，FOR-species20K上提升6.1%，ModelNet40和ModelNet10上也表现优异。
- Conclusion: MS-DGCNN++在资源受限条件下保持高精度，适用于多种点云处理任务。


### [7] [Predicting Soccer Penalty Kick Direction Using Human Action Recognition](https://arxiv.org/abs/2507.12617)
*David Freire-Obregón,Oliverio J. Santana,Javier Lorenzo-Navarro,Daniel Hernández-Sosa,Modesto Castrillón-Santana*

Main category: cs.CV

TL;DR: 提出一个手动标注的足球点球数据集，结合深度学习模型预测射门方向，准确率63.9%，优于真实守门员表现。

- Motivation: 现有动作预测数据集在真实体育场景中应用受限，需专门数据集支持。
- Method: 提出深度学习分类器，整合动作识别特征与上下文数据，评估22种骨干模型。
- Result: 模型预测射门方向准确率达63.9%，优于守门员决策。
- Conclusion: 数据集对动作预测有价值，模型适用于体育预测任务。


### [8] [Funnel-HOI: Top-Down Perception for Zero-Shot HOI Detection](https://arxiv.org/abs/2507.12628)
*Sandipan Sarma,Agney Talwarr,Arijit Sur*

Main category: cs.CV

TL;DR: Funnel-HOI提出了一种新的编码器阶段关注HOI特定线索的框架，通过非对称共注意力机制和新型损失函数，在完全监督和零样本设置下显著提升了性能。

- Motivation: 解决HOID中长尾分布问题，现有方法主要关注解码器改进，而忽视了编码器阶段对HOI特定线索的利用。
- Method: 提出Funnel-HOI框架，先探测物体再关联动作，利用非对称共注意力机制和多模态信息，设计新型损失函数。
- Result: 在HICO-DET和V-COCO数据集上，Funnel-HOI在完全监督和零样本设置下均达到SOTA性能，未见和罕见HOI类别分别提升12.4%和8.4%。
- Conclusion: 编码器阶段关注HOI特定线索能显著提升性能，Funnel-HOI框架为HOID任务提供了新思路。


### [9] [Reconstruct, Inpaint, Finetune: Dynamic Novel-view Synthesis from Monocular Videos](https://arxiv.org/abs/2507.12646)
*Kaihua Chen,Tarasha Khurana,Deva Ramanan*

Main category: cs.CV

TL;DR: 提出了一种基于3D重建和2D视频扩散模型的新视角合成方法CogNVS，无需昂贵优化即可处理动态场景。

- Motivation: 解决现有方法在动态场景新视角合成中依赖昂贵优化或无法保持几何一致性的问题。
- Method: 结合3D重建（处理可见像素）和2D视频扩散模型（填充隐藏像素），并通过自监督训练。
- Result: CogNVS在单目视频动态场景新视角合成中表现优于现有方法。
- Conclusion: CogNVS通过自监督训练和零样本适应，实现了高效且高质量的动态场景新视角合成。


### [10] [Integrated Oculomics and Lipidomics Reveal Microvascular Metabolic Signatures Associated with Cardiovascular Health in a Healthy Cohort](https://arxiv.org/abs/2507.12663)
*Inamullah,Ernesto Elias Vidal Rosas,Imran Razzak,Shoaib Jameel*

Main category: cs.CV

TL;DR: 该研究通过结合视网膜微血管特征与血清脂质组学数据，开发了一种创新的成像组学框架，用于早期心血管疾病（CVD）风险的无创生物标志物识别。

- Motivation: 心血管疾病是全球主要死因，但现有风险分层方法常无法检测早期亚临床变化。研究旨在填补视网膜微血管特征与脂质组学数据关联的空白。
- Method: 采用深度学习图像处理技术量化视网膜表型，结合超高效液相色谱-电喷雾电离高分辨质谱（UHPLC ESI HRMS）分析血清脂质谱，进行大规模协变量调整和分层相关性分析。
- Result: 研究发现视网膜动脉平均宽度、血管密度与特定脂质亚类（如TAGs、DAGs和Cers）之间存在强相关性，揭示了代谢压力下微血管重塑的机制。
- Conclusion: 该研究为早期CVD发病机制提供了新见解，并展示了无创生物标志物在早期检测和个性化预防中的潜力。


### [11] [FORTRESS: Function-composition Optimized Real-Time Resilient Structural Segmentation via Kolmogorov-Arnold Enhanced Spatial Attention Networks](https://arxiv.org/abs/2507.12675)
*Christina Thrainer,Md Meftahul Ferdaus,Mahdi Abdelguerfi,Christian Guetl,Steven Sloan,Kendall N. Niles,Ken Pathak*

Main category: cs.CV

TL;DR: FORTRESS是一种新型架构，通过结合深度可分离卷积和自适应Kolmogorov-Arnold网络，实现了高精度和计算效率的平衡，显著提升了结构缺陷分割的性能。

- Motivation: 解决在实时部署中保持高精度和计算效率的挑战。
- Method: 采用深度可分离卷积框架、自适应TiKAN集成和多尺度注意力融合技术。
- Result: 参数减少91%，计算复杂度降低91%，推理速度提升3倍，F1分数0.771，平均IoU 0.677。
- Conclusion: FORTRESS在资源受限环境中表现出色，是结构缺陷分割的实用解决方案。


### [12] [NeuraLeaf: Neural Parametric Leaf Models with Shape and Deformation Disentanglement](https://arxiv.org/abs/2507.12714)
*Yang Yang,Dongni Mao,Hiroaki Santo,Yasuyuki Matsushita,Fumio Okura*

Main category: cs.CV

TL;DR: 提出了一种名为NeuraLeaf的神经参数化模型，用于3D植物叶子的建模与重建，解决了叶子形状多样和变形灵活的问题。

- Motivation: 植物叶子的多样形状和灵活变形对建模提出了独特挑战，而现有神经参数化模型主要针对人类和动物。
- Method: NeuraLeaf将叶子几何解耦为2D基础形状和3D变形，利用2D图像数据集学习基础形状，并提出无骨架蒙皮模型处理3D变形。
- Result: NeuraLeaf能生成多种变形叶子形状，并准确拟合深度图和点云等3D观测数据。
- Conclusion: NeuraLeaf为植物建模提供了高效解决方案，并公开了实现和数据集。


### [13] [SOD-YOLO: Enhancing YOLO-Based Detection of Small Objects in UAV Imagery](https://arxiv.org/abs/2507.12727)
*Peijun Wang,Jinhua Zhao*

Main category: cs.CV

TL;DR: 提出SOD-YOLO模型，通过ASF机制、P2层和Soft-NMS改进小目标检测性能，在VisDrone2019-DET数据集上显著提升mAP。

- Motivation: 小目标检测是目标检测领域的挑战性问题，需要更高效的解决方案。
- Method: 集成ASF机制增强多尺度特征融合，添加P2层提供高分辨率特征图，使用Soft-NMS优化置信度评分。
- Result: mAP$_{50:95}$提升36.1%，mAP$_{50}$提升20.6%。
- Conclusion: SOD-YOLO是无人机图像中小目标检测的高效解决方案。


### [14] [A Privacy-Preserving Semantic-Segmentation Method Using Domain-Adaptation Technique](https://arxiv.org/abs/2507.12730)
*Homare Sueyoshi,Kiyoshi Nishikawa,Hitoshi Kiya*

Main category: cs.CV

TL;DR: 提出了一种隐私保护的语义分割方法，通过感知加密技术保护训练和测试图像，同时保持与未加密模型相近的准确性。

- Motivation: 保护图像隐私的同时，确保语义分割模型的准确性不受影响。
- Method: 采用Vision Transformer（ViT）的嵌入结构进行领域自适应技术，结合Segmentation Transformer模型。
- Result: 实验证明该方法在语义分割准确性上与未加密模型几乎一致。
- Conclusion: 该方法在隐私保护和模型性能之间取得了平衡，具有实际应用潜力。


### [15] [Transformer-based Spatial Grounding: A Comprehensive Survey](https://arxiv.org/abs/2507.12739)
*Ijazul Haq,Muhammad Saqib,Yingjie Zhang*

Main category: cs.CV

TL;DR: 本文系统综述了2018至2025年间基于Transformer的空间定位方法，总结了主流架构、数据集、评估指标及工业应用。

- Motivation: 尽管Transformer模型在空间定位领域取得显著进展，但缺乏对方法、数据集、评估指标及工业应用的综合分析。
- Method: 通过系统性文献综述，分析主流模型架构、数据集、评估指标及方法论趋势。
- Result: 研究总结了关键方法论趋势和最佳实践，为开发稳健、可靠的工业级模型提供指导。
- Conclusion: 本文为研究者和从业者提供了结构化指导，推动基于Transformer的空间定位模型的发展。


### [16] [Domain-Enhanced Dual-Branch Model for Efficient and Interpretable Accident Anticipation](https://arxiv.org/abs/2507.12755)
*Yanchen Guan,Haicheng Liao,Chengyue Wang,Bonan Wang,Jiaxun Zhang,Jia Hu,Zhenning Li*

Main category: cs.CV

TL;DR: 提出了一种双分支架构的交通事故预测框架，结合视觉和文本数据，通过特征聚合和提示工程提升预测性能。

- Motivation: 开发精确且计算高效的交通事故预测系统，以支持自动驾驶技术的及时干预和损失预防。
- Method: 采用双分支架构整合视觉（车载摄像头视频）和文本（事故报告）数据，结合特征聚合方法和大模型（GPT-4o、Long-CLIP）进行多模态输入融合。
- Result: 在基准数据集（DAD、CCD、A3D）上验证了方法的优越性，包括更高的预测准确性、响应速度、计算效率和可解释性。
- Conclusion: 该方法为交通事故预测领域设定了新的性能基准。


### [17] [HairShifter: Consistent and High-Fidelity Video Hair Transfer via Anchor-Guided Animation](https://arxiv.org/abs/2507.12758)
*Wangzheng Shi,Yinglin Zheng,Yuxin Lin,Jianmin Bao,Ming Zeng,Dong Chen*

Main category: cs.CV

TL;DR: HairShifter提出了一种新颖的“锚帧+动画”框架，用于高质量视频发型转移，结合图像发型转移模块和多尺度门控SPADE解码器，实现时空一致性和动态适应性。

- Motivation: 视频发型转移因时空一致性和动态适应性需求而具有挑战性，HairShifter旨在解决这些问题。
- Method: 采用“锚帧+动画”框架，集成图像发型转移模块和多尺度门控SPADE解码器，确保时空一致性和高质量转移。
- Result: 实验表明HairShifter在视频发型转移中达到最先进性能，兼具视觉质量、时空一致性和可扩展性。
- Conclusion: HairShifter为视频发型转移开辟了新途径，并建立了该领域的强基线。


### [18] [Unified Medical Image Segmentation with State Space Modeling Snake](https://arxiv.org/abs/2507.12760)
*Ruicheng Zhang,Haowei Guo,Kanghui Tian,Jun Zhou,Mingliang Yan,Zeyu Zhang,Shen Zhao*

Main category: cs.CV

TL;DR: Mamba Snake是一种基于状态空间建模的新型深度蛇形框架，用于统一医学图像分割（UMIS），通过层次化状态空间图谱和多尺度信息聚合提升分割性能。

- Motivation: 传统像素级方法缺乏对象级解剖洞察和器官间关系建模，难以处理UMIS中的形态复杂性和特征冲突。
- Method: 提出Mamba Snake框架，结合状态空间建模和蛇形演化，引入Mamba Evolution Block（MEB）和能量图形状先验，优化复杂形态的分割。
- Result: 在五个临床数据集上，Mamba Snake的平均Dice分数比现有方法提高3%。
- Conclusion: Mamba Snake通过多尺度建模和双分类协同机制，显著提升了UMIS的分割效果。


### [19] [Think-Before-Draw: Decomposing Emotion Semantics & Fine-Grained Controllable Expressive Talking Head Generation](https://arxiv.org/abs/2507.12761)
*Hanlei Shi,Leyuan Qu,Yu Liu,Di Gao,Yuhua Zheng,Taihao Li*

Main category: cs.CV

TL;DR: 本文提出Think-Before-Draw框架，通过语义解析和细粒度优化，提升文本驱动的情感说话头生成的自然性和表现力。

- Motivation: 当前基于文本的情感说话头生成方法依赖离散情感标签，简化了面部肌肉运动的复杂性，导致表现不自然。
- Method: 引入Chain-of-Thought（CoT）将情感标签转化为面部肌肉运动描述，并提出渐进引导去噪策略优化微表情动态。
- Result: 在MEAD和HDTF基准测试中达到最优性能，并展示了零样本生成能力。
- Conclusion: Think-Before-Draw框架显著提升了情感说话头生成的自然性和表现力。


### [20] [World Model-Based End-to-End Scene Generation for Accident Anticipation in Autonomous Driving](https://arxiv.org/abs/2507.12762)
*Yanchen Guan,Haicheng Liao,Chengyue Wang,Xingcheng Liu,Jiaxun Zhang,Zhenning Li*

Main category: cs.CV

TL;DR: 提出了一种结合生成场景增强和自适应时序推理的框架，用于提高交通事故预测的准确性和提前时间。

- Motivation: 解决交通事故预测中数据稀缺和关键物体线索缺失的问题，以提升自动驾驶系统的安全性。
- Method: 开发了基于领域提示的视频生成管道和动态预测模型，结合图卷积和扩张时序操作。
- Result: 实验证明该框架在公开和新发布的数据集上提高了事故预测的准确性和提前时间。
- Conclusion: 该框架为自动驾驶安全应用中的数据与建模限制提供了稳健解决方案。


### [21] [Continuous Marine Tracking via Autonomous UAV Handoff](https://arxiv.org/abs/2507.12763)
*Heegyeong Kim,Alice James,Avishkar Seth,Endrowednes Kuantama,Jane Williamson,Yimeng Feng,Richard Han*

Main category: cs.CV

TL;DR: 提出了一种自主无人机视觉系统，用于实时跟踪海洋动物（如鲨鱼），通过多无人机协作扩展跟踪范围。

- Motivation: 解决单无人机电池限制及动态海洋环境中跟踪的挑战。
- Method: 集成机载计算机、RGB-D相机和自定义OSTrack管道，支持多无人机无缝交接。
- Result: 在5200帧鲨鱼数据集上，实时跟踪成功率达81.9%，多无人机交接覆盖率达82.9%。
- Conclusion: 验证了协作无人机系统在海洋监测中的可行性，为规模化自主监控奠定了基础。


### [22] [AnyPos: Automated Task-Agnostic Actions for Bimanual Manipulation](https://arxiv.org/abs/2507.12768)
*Hengkai Tan,Yao Feng,Xinyi Mao,Shuhe Huang,Guodong Liu,Zhongkai Hao,Hang Su,Jun Zhu*

Main category: cs.CV

TL;DR: 提出了一种任务无关的动作范式ATARA和AnyPos模型，显著提升数据收集效率和任务成功率。

- Motivation: 解决传统VLA模型对任务特定人类演示的依赖，提高泛化能力和降低成本。
- Method: 引入ATARA框架加速数据收集，结合AnyPos逆动力学模型和视频验证模块。
- Result: 测试准确率提升51%，下游任务成功率提高30-40%。
- Conclusion: ATARA-AnyPos框架在任务无关动作范式中表现出高效性和有效性。


### [23] [Local Representative Token Guided Merging for Text-to-Image Generation](https://arxiv.org/abs/2507.12771)
*Min-Jeong Lee,Hee-Dong Kim,Seong-Whan Lee*

Main category: cs.CV

TL;DR: 提出了一种名为ReToM的局部代表令牌合并策略，用于提升稳定扩散模型的效率，同时保持图像生成质量。

- Motivation: 稳定扩散模型在文本到图像生成中表现优异，但注意力操作的二次复杂度导致生成过程耗时。现有的令牌合并方法效率提升有限，未能充分考虑注意力机制的特性。
- Method: ReToM通过定义局部边界窗口并调整窗口大小，合并令牌。引入代表令牌，选择每个窗口中最具代表性的令牌，以保留关键局部特征。
- Result: 实验显示，ReToM在FID上提升了6.2%，CLIP分数更高，同时保持推理时间与基线相当。
- Conclusion: ReToM有效平衡了视觉质量和计算效率，适用于任何注意力机制的图像生成模型。


### [24] [Compact Vision Transformer by Reduction of Kernel Complexity](https://arxiv.org/abs/2507.12780)
*Yancheng Wang,Yingzhen Yang*

Main category: cs.CV

TL;DR: KCR-Transformer通过可微分通道选择和理论泛化边界，减少计算成本，同时保持或提升性能。

- Motivation: 将Transformer块集成到紧凑的视觉架构中，以降低计算成本并保持性能。
- Method: 在MLP层进行输入/输出通道选择，基于理论泛化边界进行剪枝。
- Result: KCR-Transformer在减少FLOPs和参数的同时，性能优于原始模型。
- Conclusion: KCR-Transformer是一种高效且理论支持的视觉Transformer改进方法。


### [25] [City-VLM: Towards Multidomain Perception Scene Understanding via Multimodal Incomplete Learning](https://arxiv.org/abs/2507.12795)
*Penglei Sun,Yaoxian Song,Xiangru Zhu,Xiang Liu,Qiang Wang,Yue Liu,Changqun Xia,Tiefeng Li,Yang Yang,Xiaowen Chu*

Main category: cs.CV

TL;DR: 论文提出了一种名为SVM-City的多域感知户外场景理解数据集，并设计了City-VLM模型，通过不完全多模态学习解决现有LVLMs在户外场景中的局限性。

- Motivation: 现有大型视觉语言模型（LVLMs）主要针对室内场景，难以处理户外大规模场景的多视角、多模态数据融合问题。
- Method: 构建SVM-City数据集，包含多尺度、多视角和多模态数据；设计City-VLM模型，通过联合概率分布空间实现多模态融合。
- Result: City-VLM在问答任务中平均性能提升18.14%，表现优于现有LVLMs。
- Conclusion: 该方法在多种户外场景中展现出实用性和泛化能力。


### [26] [DeQA-Doc: Adapting DeQA-Score to Document Image Quality Assessment](https://arxiv.org/abs/2507.12796)
*Junjie Gao,Runze Liu,Yingzhe Peng,Shujian Yang,Jin Zhang,Kai Yang,Zhiyuan You*

Main category: cs.CV

TL;DR: 本文提出DeQA-Doc框架，利用多模态大语言模型（MLLM）和软标签策略，显著提升了文档质量评估的准确性和鲁棒性。

- Motivation: 现有文档质量评估方法在准确性和鲁棒性上表现不足，限制了实际应用。MLLM在图像质量评估中的成功启发了将其扩展到文档领域。
- Method: 通过改进DeQA-Score模型，提出DeQA-Doc框架，采用软标签策略和分辨率调整，并引入集成方法提升性能。
- Result: 实验表明DeQA-Doc在多种退化类型下显著优于现有基线，提供了准确且通用的文档质量评估。
- Conclusion: DeQA-Doc为文档质量评估提供了一种高效且通用的解决方案，代码和模型权重已开源。


### [27] [ATL-Diff: Audio-Driven Talking Head Generation with Early Landmarks-Guide Noise Diffusion](https://arxiv.org/abs/2507.12804)
*Hoang-Son Vo,Quang-Vinh Nguyen,Seungwon Kim,Hyung-Jeong Yang,Soonja Yeom,Soo-Hyung Kim*

Main category: cs.CV

TL;DR: ATL-Diff是一种新的音频驱动说话头生成方法，通过三个关键组件解决同步问题，降低噪声和计算成本，并在实验中表现优于现有方法。

- Motivation: 解决音频与面部动画同步的局限性，同时减少噪声和计算成本。
- Method: 包括三个关键组件：地标生成模块、地标引导噪声方法和3D身份扩散网络。
- Result: 在MEAD和CREMA-D数据集上表现优于现有方法，实现近实时处理和高保真动画。
- Conclusion: ATL-Diff在虚拟助手、教育等领域具有广泛应用前景。


### [28] [Semantic-guided Fine-tuning of Foundation Model for Long-tailed Visual Recognition](https://arxiv.org/abs/2507.12807)
*Yufei Peng,Yonggang Zhang,Yiu-ming Cheung*

Main category: cs.CV

TL;DR: 论文提出了一种名为Sage的新方法，通过结合文本模态的语义指导来优化视觉编码器的微调，以解决长尾学习中类别不平衡导致的性能下降问题。

- Motivation: 长尾场景中类别样本数量的不平衡导致低频类别性能下降，而预训练基础模型具有通用表示能力，但现有方法忽略了视觉与文本模态的对齐。
- Method: 提出Sage方法，引入SG-Adapter将类别描述作为语义指导，通过注意力机制增强视觉与文本模态的对齐，并提出分布不匹配感知补偿因子来纠正预测偏差。
- Result: 在基准数据集上的实验表明，Sage能有效提升长尾学习中的性能。
- Conclusion: Sage通过语义指导和分布补偿因子，显著改善了长尾学习中的视觉识别性能。


### [29] [FIQ: Fundamental Question Generation with the Integration of Question Embeddings for Video Question Answering](https://arxiv.org/abs/2507.12816)
*Ju-Young Oh,Ho-Joong Kim,Seong-Whan Lee*

Main category: cs.CV

TL;DR: 提出了一种名为FIQ的新方法，通过生成基于视频描述的基础问答对，增强视频问答模型的推理能力和泛化能力。

- Motivation: 现有VQA方法主要依赖事件中心的问答对，缺乏对视频更广泛上下文的理解，限制了模型的泛化和推理能力。
- Method: FIQ通过生成基础问答对和集成问题嵌入，结合VQ-CAlign模块，增强模型对视频的理解和任务适应性。
- Result: 在SUTD-TrafficQA数据集上，FIQ实现了最先进的性能。
- Conclusion: FIQ通过丰富训练数据中的基础场景信息，显著提升了视频问答模型的推理和泛化能力。


### [30] [MCoT-RE: Multi-Faceted Chain-of-Thought and Re-Ranking for Training-Free Zero-Shot Composed Image Retrieval](https://arxiv.org/abs/2507.12819)
*Jeong-Woo Park,Seong-Whan Lee*

Main category: cs.CV

TL;DR: 提出了一种无需训练的多模态思维链重排序方法（MCoT-RE），用于组合图像检索任务，显著提升了性能。

- Motivation: 现有零样本方法在组合图像检索中存在信息丢失或视觉上下文利用不足的问题，需要一种更平衡的方法。
- Method: 采用多模态思维链生成两种描述，结合多粒度重排序，分两阶段进行检索。
- Result: 在FashionIQ和CIRR数据集上分别提升6.24%和8.58%的召回率。
- Conclusion: MCoT-RE通过平衡文本修改和视觉上下文，实现了无需训练的高效检索。


### [31] [FAR-Net: Multi-Stage Fusion Network with Enhanced Semantic Alignment and Adaptive Reconciliation for Composed Image Retrieval](https://arxiv.org/abs/2507.12823)
*Jeong-Woo Park,Young-Eun Kim,Seong-Whan Lee*

Main category: cs.CV

TL;DR: FAR-Net提出了一种多阶段融合框架，通过增强语义对齐和自适应协调模块，解决了早期和晚期融合在组合图像检索中的局限性，显著提升了性能。

- Motivation: 现有方法在组合图像检索中采用早期或晚期融合，分别存在忽视视觉上下文和难以捕捉细粒度语义对齐的问题。
- Method: FAR-Net包含增强语义对齐模块（ESAM）和自适应协调模块（ARM），分别采用晚期和早期融合策略，结合交叉注意力和不确定性嵌入。
- Result: 在CIRR和FashionIQ数据集上，FAR-Net显著提升了Recall@1和Recall@50，优于现有方法。
- Conclusion: FAR-Net为组合图像检索任务提供了鲁棒且可扩展的解决方案。


### [32] [Feature-Enhanced TResNet for Fine-Grained Food Image Classification](https://arxiv.org/abs/2507.12828)
*Lulu Liu,Zhiyong Xiao*

Main category: cs.CV

TL;DR: 本文提出了一种名为FE-TResNet的创新方法，用于解决细粒度食品图像分类问题，通过结合StyleRM和DCA技术显著提高了分类准确率。

- Motivation: 食品图像分类在技术发展中需求增长，但现有CNN在处理形状相似但细节微妙的细粒度食品图像时面临挑战。
- Method: 基于TResNet模型，集成StyleRM和DCA技术，增强特征提取能力。
- Result: 在ChineseFoodNet和CNFOOD-241数据集上，分类准确率分别达到81.37%和80.29%。
- Conclusion: FE-TResNet方法在细粒度食品图像分类中表现出高效性和优越性。


### [33] [MVA 2025 Small Multi-Object Tracking for Spotting Birds Challenge: Dataset, Methods, and Results](https://arxiv.org/abs/2507.12832)
*Yuki Kondo,Norimichi Ukita,Riku Kanayama,Yuki Yoshida,Takayuki Yamaguchi,Xiang Yu,Guang Liang,Xinyao Liu,Guan-Zhang Wang,Wei-Ta Chu,Bing-Cheng Chuang,Jia-Hua Lee,Pin-Tseng Kuo,I-Hsuan Chu,Yi-Shein Hsiao,Cheng-Han Wu,Po-Yi Wu,Jui-Chien Tsou,Hsuan-Chi Liu,Chun-Yi Lee,Yuan-Fu Yang,Kosuke Shigematsu,Asuka Shin,Ba Tran*

Main category: cs.CV

TL;DR: SMOT4SB挑战赛通过引入新数据集、新评估指标和竞赛，推动了小目标多目标跟踪（SMOT）在无人机场景中的应用。

- Motivation: 解决小目标（仅几十像素）在多目标跟踪中检测和外观关联不可靠的问题，特别是在无人机（UAV）场景中。
- Method: 1. 发布SMOT4SB数据集，包含211个无人机视频序列和108,192帧标注数据；2. 提出SO-HOTA评估指标，结合Dot Distance和HOTA以减少IoU对小位移的敏感性；3. 举办MVA2025挑战赛，吸引78名参与者提交308份方案。
- Result: 获胜方法比基线提升了5.1倍，验证了方法的有效性。
- Conclusion: 该工作为无人机场景中的SMOT奠定了基础，适用于鸟击避免、农业、渔业和生态监测等领域。


### [34] [AnyCap Project: A Unified Framework, Dataset, and Benchmark for Controllable Omni-modal Captioning](https://arxiv.org/abs/2507.12841)
*Yiming Ren,Zhiqiang Lin,Yu Li,Gao Meng,Weiyun Wang,Junjie Wang,Zicheng Lin,Jifeng Dai,Yujiu Yang,Wenhai Wang,Ruihang Chu*

Main category: cs.CV

TL;DR: AnyCap项目提出了一种轻量级即插即用框架（ACM），通过结合用户指令和多模态特征提升现有基础模型的可控性，无需重新训练。同时构建了AnyCapDataset（ACD）和AnyCapEval评测基准，显著提升了多模态可控描述的质量。

- Motivation: 现有模型在可控描述任务中缺乏细粒度控制和可靠评测方法，AnyCap项目旨在填补这一空白。
- Method: 提出ACM框架，复用基础模型的原始描述，结合用户指令和多模态特征生成改进描述；构建ACD数据集和AnyCapEval评测基准。
- Result: ACM显著提升了多种基础模型在AnyCapEval上的表现，例如ACM-8B将GPT-4o的内容得分提升45%，风格得分提升12%。
- Conclusion: AnyCap项目通过模型、数据集和评测基准的整合，有效提升了多模态可控描述的质量和可靠性。


### [35] [SEMT: Static-Expansion-Mesh Transformer Network Architecture for Remote Sensing Image Captioning](https://arxiv.org/abs/2507.12845)
*Khang Truong,Lam Pham,Hieu Tang,Jasmin Lampert,Martin Boyer,Son Phan,Truong Nguyen*

Main category: cs.CV

TL;DR: 本文提出了一种基于Transformer的遥感图像描述生成（RSIC）网络架构，结合了多种技术，并在两个基准数据集上验证了其优越性。

- Motivation: 遥感图像描述生成在环境监测、灾害评估和城市规划中具有重要应用价值，但现有方法存在不足，因此需要更高效的解决方案。
- Method: 采用Transformer架构，结合静态扩展、记忆增强自注意力和网格Transformer等技术。
- Result: 在两个基准数据集（UCM-Caption和NWPU-Caption）上，提出的模型在多数评估指标上优于现有最佳系统。
- Conclusion: 该模型展示了在实际遥感图像系统中应用的潜力。


### [36] [Simulate, Refocus and Ensemble: An Attention-Refocusing Scheme for Domain Generalization](https://arxiv.org/abs/2507.12851)
*Ziyi Wang,Zhi Gao,Jin Chen,Qingjie Zhao,Xinxiao Wu,Jiebo Luo*

Main category: cs.CV

TL;DR: 论文提出了一种名为SRE的注意力重聚焦方法，用于解决CLIP在领域泛化中难以关注任务相关区域的问题。

- Motivation: CLIP在领域泛化中表现不佳，因其难以聚焦于跨领域的任务相关区域（即领域不变区域）。
- Method: SRE通过模拟领域偏移、注意力重聚焦和集成学习，减少领域差异并提升性能。
- Result: 在多个数据集上的实验表明，SRE优于现有方法。
- Conclusion: SRE通过注意力重聚焦有效提升了CLIP在领域泛化中的表现。


### [37] [SCORE: Scene Context Matters in Open-Vocabulary Remote Sensing Instance Segmentation](https://arxiv.org/abs/2507.12857)
*Shiqi Huang,Shuting He,Huaiyuan Qin,Bihan Wen*

Main category: cs.CV

TL;DR: SCORE框架通过整合多粒度场景上下文（区域和全局），提升开放词汇遥感实例分割的性能，解决了现有方法在多样性和泛化性上的限制。

- Motivation: 现有遥感实例分割方法局限于封闭词汇预测，难以识别新类别或跨数据集泛化，限制了其在地球观测多样化场景中的应用。
- Method: 提出SCORE框架，结合区域感知集成（优化类嵌入）和全局上下文适应（丰富文本嵌入），增强视觉和文本表示。
- Result: 实验表明，SCORE在多个数据集上达到SOTA性能，为大规模地理空间分析提供稳健解决方案。
- Conclusion: SCORE通过多粒度上下文整合，显著提升了开放词汇遥感实例分割的性能和适应性。


### [38] [WhoFi: Deep Person Re-Identification via Wi-Fi Channel Signal Encoding](https://arxiv.org/abs/2507.12869)
*Danilo Avola,Daniele Pannone,Dario Montagnini,Emad Emam*

Main category: cs.CV

TL;DR: WhoFi利用Wi-Fi信号进行行人重识别，通过提取CSI生物特征并使用Transformer编码器的DNN，在NTU-Fi数据集上取得竞争性结果。

- Motivation: 传统视觉方法在光照差、遮挡或角度不佳时性能受限，Wi-Fi信号提供了一种补充方案。
- Method: 从CSI提取生物特征，通过模块化DNN（含Transformer编码器）处理，使用批内负损失函数训练。
- Result: 在NTU-Fi数据集上表现优异，与先进方法竞争。
- Conclusion: WhoFi证实了Wi-Fi信号在行人重识别中的有效性。


### [39] [HRSeg: High-Resolution Visual Perception and Enhancement for Reasoning Segmentation](https://arxiv.org/abs/2507.12883)
*Weihuang Lin,Yiwei Ma,Xiaoshuai Sun,Shuting He,Jiayi Ji,Liujuan Cao,Rongrong Ji*

Main category: cs.CV

TL;DR: HRSeg模型通过高分辨率细粒度感知解决推理分割任务中的低感知分辨率问题，提出HRP和HRE模块，显著提升性能。

- Motivation: 现有方法因视觉编码器预训练分辨率低导致感知分辨率受限，简单插值方法效果有限且计算成本高。
- Method: HRSeg包含HRP模块（处理高分辨率图像，整合局部和全局特征）和HRE模块（增强掩码特征，优化与文本特征对齐）。
- Result: 在多个基准数据集上验证了模块有效性，HRSeg表现优异。
- Conclusion: HRSeg通过高分辨率感知和增强模块，显著提升了推理分割任务的性能。


### [40] [From Neck to Head: Bio-Impedance Sensing for Head Pose Estimation](https://arxiv.org/abs/2507.12884)
*Mengxi Liu,Lala Shakti Swarup Ray,Sizhen Bian,Ko Watanabe,Ankur Bhatt,Joanna Sorysz,Russel Torah,Bo Zhou,Paul Lukowicz*

Main category: cs.CV

TL;DR: NeckSense是一种新型可穿戴系统，通过多通道生物阻抗传感和深度学习框架实现头部姿态跟踪。

- Motivation: 开发一种轻便、无需视线约束的可穿戴设备，用于准确跟踪头部姿态。
- Method: 利用嵌入轻质项链式设备的软干电极捕捉颈部组织阻抗变化，结合深度学习框架和先验解剖知识。
- Result: 在7名参与者中验证，平均顶点误差为25.9毫米，性能与当前视觉方法相当。
- Conclusion: NeckSense展示了生物阻抗可穿戴设备在头部跟踪中的潜力，性能接近视觉方法。


### [41] [Camera-based implicit mind reading by capturing higher-order semantic dynamics of human gaze within environmental context](https://arxiv.org/abs/2507.12889)
*Mengke Song,Yuge Xie,Qi Cui,Luming Li,Xinyu Liu,Guotao Wang,Chenglizhao Chen,Shanchen Pang*

Main category: cs.CV

TL;DR: 提出了一种基于摄像头、用户无感知的情绪识别方法，结合注视模式与环境语义及时态动态，实现高通用性和低成本部署。

- Motivation: 现有情绪识别方法依赖显式信号（如面部表情、语音或手势），易被掩盖且忽略环境背景；生理信号方法虽直接但设备复杂；注视分析方法静态且忽略动态交互。
- Method: 利用标准高清摄像头无干扰捕捉用户眼部外观和头部运动，估计时空注视轨迹，建模注视行为的空间、语义和时态维度。
- Result: 揭示了情绪不仅是生理反应，更是人与环境交互的复杂结果，实现了用户无感知、实时连续的情绪识别。
- Conclusion: 该方法为情绪识别提供了更自然、通用且低成本的解决方案。


### [42] [LanePerf: a Performance Estimation Framework for Lane Detection](https://arxiv.org/abs/2507.12894)
*Yin Wu,Daniel Slieter,Ahmed Abouelazm,Christian Hubschneider,J. Marius Zöllner*

Main category: cs.CV

TL;DR: 本文提出了一种新的车道性能估计框架（LanePerf），用于在无标签情况下评估车道检测模型的性能，解决了领域偏移问题。

- Motivation: 车道检测在ADAS和ADS中至关重要，但领域偏移会降低模型可靠性。传统方法需要大量标注数据，而性能估计方法在车道检测中尚未充分探索。
- Method: 本文首先将五种图像分类的性能估计方法适配到车道检测任务中，然后提出LanePerf框架，结合图像和车道特征，采用预训练图像编码器和DeepSets架构。
- Result: 在OpenLane数据集上的实验表明，LanePerf优于所有基线方法，MAE为0.117，Spearman相关系数为0.727。
- Conclusion: LanePerf为ADAS提供了无标签的性能估计方法，支持在复杂驾驶场景中更高效的测试和安全性提升。


### [43] [Federated Learning for Commercial Image Sources](https://arxiv.org/abs/2507.12903)
*Shreyansh Jain,Koteswar Rao Jerripothula*

Main category: cs.CV

TL;DR: 本文提出了一种新的联邦学习数据集和两种算法（Fed-Cyclic和Fed-Star），在隐私保护下提升图像分类性能。

- Motivation: 解决联邦学习中缺乏专用数据集的问题，并优化算法以应对统计异质性。
- Method: 提出Fed-Cyclic（循环拓扑）和Fed-Star（星形拓扑）两种联邦学习算法，并在新数据集上测试。
- Result: 两种算法在新数据集上表现优于现有基线方法。
- Conclusion: 新数据集和算法为联邦学习的图像分类任务提供了有效支持。


### [44] [AthleticsPose: Authentic Sports Motion Dataset on Athletic Field and Evaluation of Monocular 3D Pose Estimation Ability](https://arxiv.org/abs/2507.12905)
*Tomohiro Suzuki,Ryota Tanaka,Calvin Yeung,Keisuke Fujii*

Main category: cs.CV

TL;DR: 论文介绍了AthleticsPose数据集，用于单目3D姿态估计在体育分析中的应用，并验证了其优于模仿运动数据集的性能。

- Motivation: 解决体育分析中缺乏真实运动数据集和单目3D姿态估计可靠性不明确的问题。
- Method: 使用AthleticsPose数据集训练3D姿态估计模型，并进行全面评估。
- Result: 模型在真实运动数据上表现显著优于基线模型（MPJPE降低75%），但对相机视角和主体尺度敏感。
- Conclusion: 真实运动数据对模型性能至关重要，单目3D姿态估计在体育分析中具有潜力但存在局限性。


### [45] [Argus: Leveraging Multiview Images for Improved 3-D Scene Understanding With Large Language Models](https://arxiv.org/abs/2507.12916)
*Yifan Xu,Chao Zhang,Hanqi Jiang,Xiaoyan Wang,Ruifei Ma,Yiwei Li,Zihao Wu,Zeju Li,Xiangde Liu*

Main category: cs.CV

TL;DR: 提出了一种名为Argus的多模态3D框架，利用多视角图像增强LLMs的3D场景理解能力，弥补3D点云重建中的信息丢失问题。

- Motivation: 3D点云重建常导致信息丢失，尤其是纹理缺失或重复模式的部分，而多视角图像能提供更详细的场景表示，弥补这些不足。
- Method: 提出Argus框架，整合多视角图像和相机姿态为场景特征，与3D特征交互生成全面的3D感知场景嵌入。
- Result: 实验表明，Argus在多种下游任务中优于现有3D-LMMs。
- Conclusion: Argus通过多模态输入扩展了LLMs的3D任务能力，有效解决了3D点云重建中的信息丢失问题。


### [46] [DMQ: Dissecting Outliers of Diffusion Models for Post-Training Quantization](https://arxiv.org/abs/2507.12933)
*Dongyeun Lee,Jiwan Hur,Hyounguk Shon,Jae Young Lee,Junmo Kim*

Main category: cs.CV

TL;DR: 提出DMQ方法，结合LES和PTS技术，优化扩散模型的量化性能，显著提升低比特宽度下的图像生成质量。

- Motivation: 扩散模型计算成本高，现有量化方法忽视异常值，导致低比特宽度性能下降。
- Method: 结合LES优化通道缩放因子，引入PTS处理高方差通道，并采用自适应时间步加权方案。
- Result: 在W4A6和W4A8等低比特宽度下显著优于现有方法，保持高质量图像生成。
- Conclusion: DMQ有效解决了扩散模型量化中的挑战，提升了资源受限环境下的部署可行性。


### [47] [A Deep-Learning Framework for Land-Sliding Classification from Remote Sensing Image](https://arxiv.org/abs/2507.12939)
*Hieu Tang,Truong Vo,Dong Pham,Toan Nguyen,Lam Pham,Truong Nguyen*

Main category: cs.CV

TL;DR: 提出了一种结合在线和离线数据增强、EfficientNet_Large模型和后处理SVM分类器的深度学习框架，用于滑坡检测，F1得分为0.8938。

- Motivation: 解决滑坡检测中深度学习架构选择和过拟合问题。
- Method: 结合在线和离线数据增强处理不平衡数据，使用EfficientNet_Large提取特征，后接SVM分类器优化性能。
- Result: 在Zindi挑战赛公共测试集上F1得分为0.8938。
- Conclusion: 该框架在滑坡检测中表现出色，有效解决了数据不平衡和过拟合问题。


### [48] [Weakly Supervised Visible-Infrared Person Re-Identification via Heterogeneous Expert Collaborative Consistency Learning](https://arxiv.org/abs/2507.12942)
*Yafei Zhang,Lingqi Kong,Huafeng Li,Jie Wen*

Main category: cs.CV

TL;DR: 提出一种弱监督跨模态行人重识别方法，仅使用单模态样本标签，解决跨模态标签缺失问题。

- Motivation: 减少对标记跨模态样本的依赖，解决跨模态身份标签不可用的情况。
- Method: 提出异构专家协作一致性学习框架，利用单模态标签训练分类专家，通过跨模态关系融合机制整合预测。
- Result: 在两个挑战性数据集上验证了方法的有效性。
- Conclusion: 框架显著提升了模型提取模态不变特征和跨模态身份识别的能力。


### [49] [Analysis of Image-and-Text Uncertainty Propagation in Multimodal Large Language Models with Cardiac MR-Based Applications](https://arxiv.org/abs/2507.12945)
*Yucheng Tang,Yunguan Fu,Weixi Yi,Yipei Wang,Daniel C. Alexander,Rhodri Davies,Yipeng Hu*

Main category: cs.CV

TL;DR: 提出了基于不确定性传播的多模态不确定性传播模型（MUPM），用于分析多模态大语言模型（MLLMs）中输入模态间的不确定性关系，并在临床数据中验证其鲁棒性和可迁移性。

- Motivation: 研究多模态输入中不确定性关系及其在临床任务中的应用尚未充分探索。
- Method: 提出MUPM模型，通过不确定性传播分析图像、文本及联合模态的不确定性关系，并在心脏MR扫描和数字健康记录数据上进行优化和验证。
- Result: MUPM在少量样本下鲁棒优化，可迁移至不同数据分布和下游任务，并直接应用于临床疾病预测任务。
- Conclusion: MUPM通过共享预训练和轻量微调实现不确定性关系的量化，为临床提供了高效且实用的工具。


### [50] [LoViC: Efficient Long Video Generation with Context Compression](https://arxiv.org/abs/2507.12952)
*Jiaxiu Jiang,Wenbo Li,Jingjing Ren,Yuping Qiu,Yong Guo,Xiaogang Xu,Han Wu,Wangmeng Zuo*

Main category: cs.CV

TL;DR: LoViC是一个基于扩散变换器（DiT）的框架，通过分段生成过程生成长且连贯的视频，解决了长视频生成的挑战。

- Motivation: 长视频生成因自注意力的二次复杂度而困难，现有方法如稀疏注意力和时间自回归模型在时间连贯性或可扩展性上有所妥协。
- Method: LoViC采用FlexFormer，一种联合压缩视频和文本的统一潜在表示的自动编码器，支持可变长度输入和线性可调压缩率。
- Result: 实验验证了LoViC在预测、回溯、插值和多镜头生成等任务中的有效性和多功能性。
- Conclusion: LoViC通过统一范式解决了长视频生成的挑战，展示了其高效性和灵活性。


### [51] [cIDIR: Conditioned Implicit Neural Representation for Regularized Deformable Image Registration](https://arxiv.org/abs/2507.12953)
*Sidaty El Hadramy,Oumeymah Cherkaoui,Philippe C. Cattin*

Main category: cs.CV

TL;DR: 提出了一种基于隐式神经表示（INRs）的新框架cIDIR，通过条件化正则化超参数优化变形图像配准（DIR）过程，避免了传统方法中需要多次训练的问题。

- Motivation: 传统DIR框架中正则化参数调优计算成本高，需多次训练迭代，限制了效率。
- Method: cIDIR基于INRs，通过先验分布训练正则化超参数，并利用分割掩码优化，实现连续可微的变形向量场（DVF）。
- Result: 在DIR-LAB数据集上，cIDIR表现出高精度和鲁棒性。
- Conclusion: cIDIR为DIR提供了一种高效且灵活的正则化参数优化方法。


### [52] [FantasyPortrait: Enhancing Multi-Character Portrait Animation with Expression-Augmented Diffusion Transformers](https://arxiv.org/abs/2507.12956)
*Qiang Wang,Mengchao Wang,Fan Jiang,Yaqi Fan,Yonggang Qi,Mu Xu*

Main category: cs.CV

TL;DR: FantasyPortrait提出了一种基于扩散变换器的框架，用于生成高质量、情感丰富的单人和多人面部动画，解决了现有方法在跨重演和多角色场景中的问题。

- Motivation: 现有方法依赖显式几何先验，易产生伪影且难以捕捉细微情感，同时缺乏多角色动画支持。
- Method: 采用扩散变换器框架，引入表达增强学习策略和掩码交叉注意力机制，以隐式表示捕捉面部动态并防止特征干扰。
- Result: 在定量和定性评估中显著优于现有方法，尤其在跨重演和多角色场景中表现优异。
- Conclusion: FantasyPortrait为高质量面部动画生成提供了有效解决方案，并推动了多角色动画研究的发展。


### [53] [Demographic-aware fine-grained classification of pediatric wrist fractures](https://arxiv.org/abs/2507.12964)
*Ammar Ahmed,Ali Shariq Imran,Zenun Kastrati,Sher Muhammad Daudpota*

Main category: cs.CV

TL;DR: 研究提出了一种结合细粒度识别和患者元数据融合的方法，用于在极有限的数据集上识别手腕病理，显著提高了诊断准确性。

- Motivation: 手腕病理诊断耗时且需专业知识，计算机视觉虽有潜力但受限于数据不足，单一模态（如图像）效果不佳。
- Method: 采用细粒度识别任务、融合患者元数据与X光图像，并使用细粒度数据集预训练权重。
- Result: 细粒度策略和元数据融合在有限数据集上提高诊断准确性2%，在更大骨折数据集上提高超过10%。
- Conclusion: 多模态方法和细粒度策略可有效提升手腕病理诊断的准确性，尤其在数据有限的情况下。


### [54] [RGB Pre-Training Enhanced Unobservable Feature Latent Diffusion Model for Spectral Reconstruction](https://arxiv.org/abs/2507.12967)
*Keli Deng,Jie Nie,Yuntao Qian*

Main category: cs.CV

TL;DR: 该论文提出了一种基于预训练RGB潜在扩散模型（RGB-LDM）的两阶段方法，用于从RGB图像重建高光谱图像（HSI），通过分离不可观测特征并学习其联合分布，实现了优异的性能。

- Motivation: 高光谱图像重建（SR）的关键挑战在于从RGB图像中估计未被传感器捕获的不可观测光谱特征。
- Method: 扩展RGB-LDM为不可观测特征LDM（ULDM），通过两阶段流程（光谱结构表示学习和光谱-空间联合分布学习）实现。
- Result: 实验证明该方法在SR和下游重光照任务中达到最优性能。
- Conclusion: 通过分离不可观测特征并利用预训练模型的优势，该方法有效提升了高光谱图像重建的精度。


### [55] [Variance-Based Pruning for Accelerating and Compressing Trained Networks](https://arxiv.org/abs/2507.12988)
*Uranik Berisha,Jens Mehnert,Alexandru Paul Condurache*

Main category: cs.CV

TL;DR: 论文提出了一种基于方差的剪枝方法（Variance-Based Pruning），用于高效压缩模型，减少计算和内存需求，同时通过少量微调保持性能。

- Motivation: 大型模型（如Vision Transformers）的训练成本高，部署时面临延迟、计算和内存问题。现有结构化剪枝方法需要昂贵重训练，难以保持性能。
- Method: 通过收集激活统计信息选择剪枝神经元，同时将平均激活整合回模型以减少性能损失。
- Result: 在ImageNet-1k任务中，剪枝后的DeiT-Base模型直接保留70%性能，仅需10轮微调恢复99%精度，计算量减少35%，模型大小减少36%，速度提升1.44倍。
- Conclusion: Variance-Based Pruning是一种高效的结构化剪枝方法，显著减少资源需求且性能损失小。


### [56] [Differential-informed Sample Selection Accelerates Multimodal Contrastive Learning](https://arxiv.org/abs/2507.12998)
*Zihua Zhao,Feng Hong,Mengxi Chen,Pengyi Chen,Benyuan Liu,Jiangchao Yao,Ya Zhang,Yanfeng Wang*

Main category: cs.CV

TL;DR: 论文提出了一种名为DISSect的新方法，通过差分信息选择样本，解决了对比学习中噪声对应问题，显著提升了训练效率。

- Motivation: 现有样本选择方法在冷启动场景下依赖离线模型，或在线选择时未充分处理噪声对应问题，亟需一种更高效的方法。
- Method: 提出差分信息样本选择（DISSect），利用当前模型与历史模型预测相关性的差异来表征样本质量。
- Result: 在三个基准数据集和多种下游任务上的实验表明，DISSect优于现有方法。
- Conclusion: DISSect通过差分信息有效解决了噪声对应问题，显著提升了训练效率，具有理论和实践优势。


### [57] [Beyond Fully Supervised Pixel Annotations: Scribble-Driven Weakly-Supervised Framework for Image Manipulation Localization](https://arxiv.org/abs/2507.13018)
*Songlin Li,Guofeng Yu,Zhiqing Guo,Yunfeng Diao,Dan Ma,Gaobo Yang,Liejun Wang*

Main category: cs.CV

TL;DR: 该研究提出了一种基于涂鸦标注的弱监督图像篡改定位方法，通过自监督训练和动态特征调整模块提升性能。

- Motivation: 解决传统方法依赖大量像素级标注的问题，探索涂鸦标注以提高标注效率和检测性能。
- Method: 采用自监督训练、先验感知特征调制模块（PFMM）、门控自适应融合模块（GAFM）和置信感知熵最小化损失（${\mathcal{L}}_{ {CEM }}$）。
- Result: 实验结果表明，该方法在分布内和分布外数据上的平均性能优于现有全监督方法。
- Conclusion: 涂鸦标注监督是一种高效且有效的弱监督形式，显著提升了图像篡改定位的性能。


### [58] [Resurrect Mask AutoRegressive Modeling for Efficient and Scalable Image Generation](https://arxiv.org/abs/2507.13032)
*Yi Xin,Le Zhuo,Qi Qin,Siqi Luo,Yuewen Cao,Bin Fu,Yangfan He,Hongsheng Li,Guangtao Zhai,Xiaohong Liu,Peng Gao*

Main category: cs.CV

TL;DR: MaskGIL改进MAR架构，通过双向注意力和2D RoPE提升图像生成质量，性能媲美AR模型，推理步骤大幅减少。

- Motivation: 传统MAR模型在图像生成上表现不如AR模型，研究旨在改进MAR架构以提升生成质量。
- Method: 评估图像分词器，引入双向注意力和2D RoPE，构建MaskGIL模型，参数规模从111M到1.4B。
- Result: MaskGIL在ImageNet 256x256上FID达3.71，仅需8步推理，支持文本驱动生成和实时语音转图像。
- Conclusion: MaskGIL显著提升MAR模型性能，扩展了应用场景，代码和模型已开源。


### [59] [Advancing Complex Wide-Area Scene Understanding with Hierarchical Coresets Selection](https://arxiv.org/abs/2507.13061)
*Jingyao Wang,Yiming Chen,Lingyu Si,Changwen Zheng*

Main category: cs.CV

TL;DR: 论文提出了一种分层核心集选择（HCS）机制，用于提升视觉语言模型（VLMs）在复杂广域场景理解中的适应性，无需额外微调即可快速理解未见场景。

- Motivation: 现有视觉语言模型在适应未见复杂广域场景时面临挑战，需改进其适应性。
- Method: 提出分层核心集选择（HCS）机制，基于理论保证的重要性函数逐步优化区域选择，考虑效用、代表性、鲁棒性和协同性。
- Result: 实验表明，HCS在多种任务中表现出优越性能和通用性。
- Conclusion: HCS是一种即插即用的方法，适用于任何VLM，能有效提升复杂场景理解能力。


### [60] [Label-Consistent Dataset Distillation with Detector-Guided Refinement](https://arxiv.org/abs/2507.13074)
*Yawen Zou,Guang Li,Zi Wang,Chunzhi Gu,Chao Zhang*

Main category: cs.CV

TL;DR: 提出了一种基于检测器引导的数据集蒸馏框架，通过预训练检测器优化合成样本，提升标签一致性和图像质量。

- Motivation: 解决现有扩散模型在数据集蒸馏中生成的样本标签不一致或结构细节不足的问题。
- Method: 利用预训练检测器识别异常样本，生成多个候选图像，并通过置信度和多样性选择最优样本。
- Result: 实验表明，该方法能生成高质量、细节丰富的图像，在验证集上达到最优性能。
- Conclusion: 检测器引导的框架有效提升了数据集蒸馏的质量和性能。


### [61] [Channel-wise Motion Features for Efficient Motion Segmentation](https://arxiv.org/abs/2507.13082)
*Riku Inoue,Masamitsu Tsuchiya,Yuji Yasui*

Main category: cs.CV

TL;DR: 提出了一种基于成本体积的运动特征表示方法，显著提高了实时性能。

- Motivation: 在安全关键的机器人应用中，实时准确检测动态物体至关重要。现有方法因多子网络联合使用导致计算成本高，影响实时性。
- Method: 提出Channel-wise Motion Features，仅使用Pose Network提取深度特征和3D运动信息，无需其他子网络。
- Result: 在KITTI和Cityscapes数据集上，FPS提升约4倍，参数减少至25%，同时保持同等精度。
- Conclusion: 新方法在高效性和准确性上均优于现有技术，适用于实时应用。


### [62] [Decoupled PROB: Decoupled Query Initialization Tasks and Objectness-Class Learning for Open World Object Detection](https://arxiv.org/abs/2507.13085)
*Riku Inoue,Masamitsu Tsuchiya,Yuji Yasui*

Main category: cs.CV

TL;DR: Decoupled PROB通过ETOP和TDQI解决了PROB模型中的学习冲突问题，显著提升了开放世界目标检测的性能。

- Motivation: 开放世界目标检测（OWOD）任务中，未知对象的无监督检测和增量学习是主要挑战。PROB模型虽无需伪标签，但存在对象性和类别预测的学习冲突问题。
- Method: 提出Decoupled PROB，引入ETOP终止对象性预测，并通过TDQI高效提取已知和未知对象的特征。
- Result: 在OWOD基准测试中，Decoupled PROB在多项指标上超越现有方法，性能显著提升。
- Conclusion: Decoupled PROB通过解耦学习冲突和优化特征提取，成为开放世界目标检测的先进方法。


### [63] [DiffOSeg: Omni Medical Image Segmentation via Multi-Expert Collaboration Diffusion Model](https://arxiv.org/abs/2507.13087)
*Han Zhang,Xiangde Luo,Yong Chen,Kang Li*

Main category: cs.CV

TL;DR: DiffOSeg是一个两阶段扩散框架，旨在同时实现共识驱动和偏好驱动的医学图像分割，优于现有方法。

- Motivation: 医学图像分割中的标注变异性问题源于模糊的边界和专家多样性，传统方法难以捕捉这些偏差。
- Method: DiffOSeg采用两阶段扩散框架：第一阶段通过概率共识策略建立群体共识，第二阶段通过自适应提示捕捉专家偏好。
- Result: 在LIDC-IDRI和NPC-170数据集上，DiffOSeg在所有评估指标上优于现有方法。
- Conclusion: DiffOSeg成功解决了共识和偏好驱动的分割问题，为医学图像分割提供了更全面的解决方案。


### [64] [GLAD: Generalizable Tuning for Vision-Language Models](https://arxiv.org/abs/2507.13089)
*Yuqi Peng,Pengfei Wang,Jianzhuang Liu,Shifeng Chen*

Main category: cs.CV

TL;DR: GLAD框架通过LoRA和梯度正则化技术，解决了少样本学习中的过拟合问题，提升了模型的泛化能力。

- Motivation: 现有提示调优方法在少样本场景下易过拟合且依赖复杂架构，限制了通用性。
- Method: 提出GLAD框架，结合LoRA和梯度正则化技术，优化模型参数稳定性。
- Result: 在15个基准数据集上，GLAD在基类到新类泛化、图像域泛化和跨数据集泛化方面表现优异。
- Conclusion: GLAD简化了提示调优，提升了泛化能力，适用于多种下游任务。


### [65] [Deep Learning-Based Fetal Lung Segmentation from Diffusion-weighted MRI Images and Lung Maturity Evaluation for Fetal Growth Restriction](https://arxiv.org/abs/2507.13106)
*Zhennan Xiao,Katharine Brudkiewicz,Zhen Yuan,Rosalind Aughwane,Magdalena Sokolska,Joanna Chappell,Trevor Gaunt,Anna L. David,Andrew P. King,Andrew Melbourne*

Main category: cs.CV

TL;DR: 提出了一种基于深度学习的自动化胎儿肺成熟度评估流程，结合3D nnU-Net分割和模型拟合，性能与手动分割相当。

- Motivation: 胎儿肺成熟度对预测新生儿结局至关重要，但现有方法依赖耗时的手动分割，限制了临床应用。
- Method: 使用3D nnU-Net模型自动分割胎儿肺部，并进行模型拟合评估成熟度。
- Result: 分割模型平均Dice系数达82.14%，自动与手动分割结果无显著差异。
- Conclusion: 自动化流程可用于支持胎儿肺成熟度评估和临床决策。


### [66] [R^2MoE: Redundancy-Removal Mixture of Experts for Lifelong Concept Learning](https://arxiv.org/abs/2507.13107)
*Xiaohan Guo,Yusong Cai,Zejia Liu,Zhengning Wang,Lili Pan,Hongliang Li*

Main category: cs.CV

TL;DR: 提出了一种参数高效框架R²MoE，用于持续学习新视觉概念，解决了灾难性遗忘和参数膨胀问题。

- Motivation: 大规模生成模型需要持续学习新视觉概念以满足用户个性化需求，但现有方法面临灾难性遗忘和参数膨胀的挑战。
- Method: 提出R²MoE框架，包括路由蒸馏机制、冗余专家消除策略和分层局部注意力引导推理。
- Result: 在CustomConcept 101数据集上，遗忘率降低87.8%，参数减少63.3%，生成图像概念保真度优于SOTA方法。
- Conclusion: R²MoE是一种高效且参数节省的持续视觉概念学习框架，显著提升了生成模型的适应性和性能。


### [67] [3DKeyAD: High-Resolution 3D Point Cloud Anomaly Detection via Keypoint-Guided Point Clustering](https://arxiv.org/abs/2507.13110)
*Zi Wang,Katsuya Hotta,Koichiro Kamide,Yawen Zou,Chao Zhang,Jun Yu*

Main category: cs.CV

TL;DR: 提出了一种基于配准的异常检测框架，结合多原型对齐和聚类差异分析，实现高精度3D异常定位。

- Motivation: 高分辨率3D点云在工业检测中能有效发现细微结构异常，但其密集和不规则特性带来计算成本高、空间对齐敏感等问题。
- Method: 测试样本先与多个正常原型配准，进行直接结构比较；通过聚类和关键点引导策略进行局部异常分析。
- Result: 在Real3D-AD基准测试中，该方法在物体级和点级异常检测上均达到最优性能。
- Conclusion: 该方法通过多原型对齐和关键点引导聚类，显著提升了3D异常检测的精度和稳定性。


### [68] [Leveraging Language Prior for Infrared Small Target Detection](https://arxiv.org/abs/2507.13113)
*Pranav Singh,Pravendra Singh*

Main category: cs.CV

TL;DR: 提出一种结合语言先验的多模态红外小目标检测框架，显著提升检测性能。

- Motivation: 现有红外小目标检测方法仅依赖图像模态，限制了性能提升。
- Method: 利用语言先验生成文本描述，结合GPT-4视觉模型和注意力机制增强检测能力。
- Result: 在NUAA-SIRST和IRSTD-1k数据集上，IoU、nIoU、Pd和Fa等指标显著提升。
- Conclusion: 多模态方法有效提升了红外小目标检测性能，填补了现有数据集的不足。


### [69] [RS-TinyNet: Stage-wise Feature Fusion Network for Detecting Tiny Objects in Remote Sensing Images](https://arxiv.org/abs/2507.13120)
*Xiaozheng Jiang,Wei Zhang,Xuerui Mao*

Main category: cs.CV

TL;DR: RS-TinyNet提出了一种针对遥感图像中小目标检测的多阶段特征融合与增强模型，通过显著性建模和特征完整性重建提升检测性能。

- Motivation: 遥感图像中小目标检测因空间信息有限、特征表示弱和背景复杂而具有挑战性，现有检测器表现不佳。
- Method: 设计了多维度协作注意力模块（MDCA）、辅助可逆分支（ARB）和渐进融合检测头（PFDH）来增强特征表示和融合。
- Result: 在AI-TOD数据集上AP提升4.0%，AP75提升6.5%，DIOR数据集验证了其广泛适用性。
- Conclusion: 多阶段特征融合策略为复杂遥感环境中的小目标检测提供了有效解决方案。


### [70] [DINO-VO: A Feature-based Visual Odometry Leveraging a Visual Foundation Model](https://arxiv.org/abs/2507.13145)
*Maulana Bisyir Azhari,David Hyunchul Shim*

Main category: cs.CV

TL;DR: DINO-VO是一种基于DINOv2视觉基础模型的单目视觉里程计系统，通过改进特征匹配和几何特征，提高了鲁棒性和泛化能力。

- Motivation: 解决基于学习的单目视觉里程计在鲁棒性、泛化性和效率方面的挑战，尤其是DINOv2模型在VO中因特征粒度粗而受限的问题。
- Method: 提出了一种针对DINOv2粗粒度特征的关键点检测器，结合几何特征增强定位能力，并使用基于Transformer的匹配器和可微分位姿估计层。
- Result: DINO-VO在TartanAir和KITTI数据集上优于现有帧间VO方法，在EuRoC数据集上表现竞争性，运行效率达72 FPS，内存占用小于1GB。
- Conclusion: DINO-VO展示了在复杂环境中的鲁棒性和泛化能力，性能接近视觉SLAM系统。


### [71] [SE-VLN: A Self-Evolving Vision-Language Navigation Framework Based on Multimodal Large Language Models](https://arxiv.org/abs/2507.13152)
*Xiangyu Dong,Haoran Zhao,Jiang Gao,Haozhou Li,Xiaoguang Ma,Yaoming Zhou,Fuhai Chen,Juan Liu*

Main category: cs.CV

TL;DR: 提出了一种自进化的视觉语言导航框架（SE-VLN），通过分层记忆、检索增强推理和反思模块，实现了在测试中持续进化的能力，显著提升了导航成功率。

- Motivation: 现有基于大语言模型的视觉语言导航方法受限于固定知识库和推理能力，无法有效整合经验知识，缺乏进化能力。
- Method: SE-VLN包含三个核心模块：分层记忆模块（存储成功与失败案例）、检索增强推理模块（多步决策）、反思模块（持续进化）。
- Result: 在未见环境中导航成功率分别达到57%和35.2%，比现有最优方法提升了23.9%和15.0%。
- Conclusion: SE-VLN展示了自进化框架在视觉语言导航中的潜力，性能随经验库增长而提升。


### [72] [Orbis: Overcoming Challenges of Long-Horizon Prediction in Driving World Models](https://arxiv.org/abs/2507.13162)
*Arian Mousakhan,Sudhanshu Mittal,Silvio Galesso,Karim Farid,Thomas Brox*

Main category: cs.CV

TL;DR: 论文提出了一种简单设计的世界模型，无需额外监督或传感器，在长时程生成和复杂场景泛化上表现优异，尤其擅长转弯和城市交通场景。

- Motivation: 现有自动驾驶世界模型在长时程生成和复杂场景泛化上表现不佳，需要改进。
- Method: 采用简单设计，无需额外监督或传感器，提出混合分词器比较离散与连续模型。
- Result: 模型仅用469M参数和280h视频数据训练，表现优于现有方法，连续自回归模型优于离散模型。
- Conclusion: 连续自回归模型在鲁棒性和性能上优于离散模型，代码和模型已公开。


### [73] [Synthesizing Reality: Leveraging the Generative AI-Powered Platform Midjourney for Construction Worker Detection](https://arxiv.org/abs/2507.13221)
*Hongyang Zhao,Tianyu Liang,Sina Davari,Daeho Kim*

Main category: cs.CV

TL;DR: 该研究提出了一种基于生成AI（Midjourney）的图像合成方法，用于解决建筑工人检测中的数据不足问题，生成了12,000张合成图像，并在真实数据集上取得了较高的检测精度。

- Motivation: 深度神经网络在视觉AI中表现优异，但在建筑领域，数据多样性和数量不足仍是挑战。
- Method: 利用Midjourney生成12,000张合成图像，强调真实性和多样性，并手动标注后用于DNN训练。
- Result: 在真实数据集上，模型在IoU阈值为0.5和0.5-0.95时的平均精度分别为0.937和0.642；在合成数据集上表现更优，分别为0.994和0.919。
- Conclusion: 生成AI在解决DNN训练数据稀缺方面具有潜力，但也存在局限性。


### [74] [Leveraging Pre-Trained Visual Models for AI-Generated Video Detection](https://arxiv.org/abs/2507.13224)
*Keerthi Veeramachaneni,Praveen Tirupattur,Amrit Singh Bedi,Mubarak Shah*

Main category: cs.CV

TL;DR: 提出了一种利用预训练视觉模型检测AI生成视频的新方法，无需额外训练即可实现高检测准确率，并在公开数据集上验证了其有效性。

- Motivation: 随着AI生成视频质量的提升，检测此类内容以应对虚假信息、隐私和安全威胁变得至关重要。现有方法主要针对DeepFakes，而通用视频生成领域的需求尚未满足。
- Method: 利用预训练视觉模型提取特征，通过简单线性分类层区分真实与生成视频，无需额外训练。
- Result: 在包含10,000个AI生成视频和4,000个真实视频的数据集上，平均检测准确率超过90%。
- Conclusion: 该方法高效且实用，未来将公开代码、模型和数据集以推动相关研究。


### [75] [$S^2M^2$: Scalable Stereo Matching Model for Reliable Depth Estimation](https://arxiv.org/abs/2507.13229)
*Junhong Min,Youngpil Jeon,Jimin Kim,Minyong Choi*

Main category: cs.CV

TL;DR: 论文提出了一种名为$S^2M^2$的全局匹配架构，解决了立体匹配模型在泛化性、计算效率和全局一致性之间的权衡问题。

- Motivation: 追求一个无需数据集特定微调即可适应不同分辨率和视差范围的通用立体匹配模型，揭示了局部搜索方法与全局匹配架构之间的固有矛盾。
- Method: 设计了一种结合多分辨率变换器的全局匹配架构$S^2M^2$，采用新型损失函数集中概率于可行匹配，实现了视差、遮挡和置信度的联合估计。
- Result: $S^2M^2$在Middlebury v3和ETH3D基准测试中取得了最先进的精度和高效性，显著优于现有方法。
- Conclusion: $S^2M^2$通过创新的全局匹配架构和损失函数设计，成功解决了立体匹配中的泛化性和效率问题，为未来研究提供了新方向。


### [76] [VITA: Vision-to-Action Flow Matching Policy](https://arxiv.org/abs/2507.13231)
*Dechen Gao,Boqi Zhao,Andrew Lee,Ian Chuang,Hanchu Zhou,Hang Wang,Zhe Zhao,Junshan Zhang,Iman Soltani*

Main category: cs.CV

TL;DR: VITA是一种视觉到动作的流匹配策略，通过将潜在视觉表示转化为潜在动作，简化了传统流匹配和扩散策略的复杂条件机制，提高了效率。

- Motivation: 传统方法需要额外的条件机制来处理视觉信息，增加了时间和空间开销。VITA旨在消除这些模块，同时保留生成建模能力。
- Method: VITA使用潜在图像作为流源，通过自动编码器创建结构化的动作潜在空间，并通过流潜在解码监督流匹配。
- Result: 在ALOHA平台上，VITA在复杂双手机器人任务中表现优于或匹配现有生成策略，同时减少了50-130%的推理延迟。
- Conclusion: VITA是首个仅使用MLP层就能解决复杂双手机器人任务的流匹配策略，展示了其高效性和简单性。


### [77] [Efficient Adaptation of Pre-trained Vision Transformer underpinned by Approximately Orthogonal Fine-Tuning Strategy](https://arxiv.org/abs/2507.13260)
*Yiting Yang,Hao Luo,Yuan Sun,Qingsen Yan,Haokui Zhang,Wei Dong,Guoqing Wang,Peng Wang,Yang Yang,Hengtao Shen*

Main category: cs.CV

TL;DR: 论文提出了一种近似正交微调（AOFT）策略，通过生成近似正交向量来改进低秩适配矩阵，从而增强预训练视觉变换器（ViT）在下游任务中的泛化能力。

- Motivation: 观察到预训练ViT的权重矩阵具有近似正交性，而低秩适配矩阵（如LoRA和Adapter）缺乏这一特性，可能导致泛化能力不足。因此，研究如何通过改进低秩矩阵的正交性来提升模型性能。
- Method: 提出AOFT策略，利用单个可学习向量生成近似正交向量，构建低秩适配矩阵，使其与预训练主干的权重矩阵性质一致。
- Result: 实验表明，AOFT在多个下游图像分类任务中表现优异，验证了改进泛化能力的有效性。
- Conclusion: AOFT通过增强低秩适配矩阵的正交性，显著提升了ViT在下游任务中的泛化性能。


### [78] [DiffClean: Diffusion-based Makeup Removal for Accurate Age Estimation](https://arxiv.org/abs/2507.13292)
*Ekta Balkrishna Gavas,Chinmay Hegde,Nasir Memon,Sudipta Banerjee*

Main category: cs.CV

TL;DR: DiffClean利用文本引导扩散模型消除化妆痕迹，提升年龄验证和面部识别的准确性。

- Motivation: 保护未成年用户免受年龄限制服务的未经授权访问，但化妆可能干扰年龄估计。
- Method: 提出DiffClean，通过文本引导扩散模型消除化妆痕迹。
- Result: 在数字模拟和真实化妆图像上，年龄估计准确性提升4.8%，面部识别性能提升8.9%。
- Conclusion: DiffClean有效防御化妆攻击，提升年龄验证和面部识别的准确性。


### [79] [FashionPose: Text to Pose to Relight Image Generation for Personalized Fashion Visualization](https://arxiv.org/abs/2507.13311)
*Chuancheng Shi,Yixiang Chen,Burong Lei,Jichao Chen*

Main category: cs.CV

TL;DR: FashionPose是一个统一的文本到姿势到重新光照生成框架，用于时尚电子商务中的个性化服装预览。

- Motivation: 解决现有方法依赖预定义姿势、缺乏语义灵活性和光照适应性的问题。
- Method: 通过文本输入预测2D人体姿势，使用扩散模型生成高保真人物图像，并应用轻量级重新光照模块。
- Result: 实验表明，FashionPose实现了细粒度姿势合成和高效、一致的重新光照。
- Conclusion: FashionPose为个性化虚拟时尚展示提供了实用解决方案。


### [80] [Revisiting Reliability in the Reasoning-based Pose Estimation Benchmark](https://arxiv.org/abs/2507.13314)
*Junsu Kim,Naeun Kim,Jaeho Lee,Incheol Park,Dongyoon Han,Seungryul Baek*

Main category: cs.CV

TL;DR: 论文指出了RPE基准在可重复性和质量上的问题，并提供了改进的GT标注以促进公平评估。

- Motivation: RPE基准存在可重复性和质量问题，影响对姿态感知多模态大语言模型的公平评估。
- Method: 通过细致的视觉匹配改进GT标注，并公开这些标注。
- Result: 解决了图像索引不一致、图像冗余、场景不平衡等问题，提升了评估的可靠性。
- Conclusion: 改进的GT标注有助于促进姿态感知多模态推理的未来发展。


### [81] [A Real-Time System for Egocentric Hand-Object Interaction Detection in Industrial Domains](https://arxiv.org/abs/2507.13326)
*Antonio Finocchiaro,Alessandro Sebastiano Catinello,Michele Mazzamuto,Rosario Leonardi,Antonino Furnari,Giovanni Maria Farinella*

Main category: cs.CV

TL;DR: 提出了一种实时检测手与物体交互的高效方法，结合动作识别和物体检测模块，在30fps下达到较高性能。

- Motivation: 解决实时应用中手与物体交互检测的挑战，提升用户体验。
- Method: 采用级联架构，动作识别模块（Mamba模型+EfficientNetV2）触发物体检测模块（YOLOWorld）。
- Result: 动作识别模块在ENIGMA-51基准上达到38.52% p-AP，物体检测模块达到85.13% AP。
- Conclusion: 级联架构实现了实时高效的手与物体交互检测。


### [82] [Taming Diffusion Transformer for Real-Time Mobile Video Generation](https://arxiv.org/abs/2507.13343)
*Yushu Wu,Yanyu Li,Anil Kag,Ivan Skorokhodov,Willi Menapace,Ke Ma,Arpit Sahni,Ju Hu,Aliaksandr Siarohin,Dhritiman Sagar,Yanzhi Wang,Sergey Tulyakov*

Main category: cs.CV

TL;DR: 提出了一种针对Diffusion Transformers (DiT)的优化方法，使其能够在移动设备上实现实时高质量视频生成。

- Motivation: DiT在视频生成任务中表现优异，但计算成本高，难以在资源受限的设备（如智能手机）上实现实时生成。
- Method: 1. 使用高度压缩的变分自编码器（VAE）降低输入数据维度；2. 提出KD引导的三级剪枝策略缩小模型尺寸；3. 开发针对DiT的对抗性步数蒸馏技术，减少推理步骤。
- Result: 优化后的模型在iPhone 16 Pro Max上实现了超过10 FPS的生成速度。
- Conclusion: 证明了在移动设备上实现实时高质量视频生成的可行性。


### [83] [Diffuman4D: 4D Consistent Human View Synthesis from Sparse-View Videos with Spatio-Temporal Diffusion Models](https://arxiv.org/abs/2507.13344)
*Yudong Jin,Sida Peng,Xuan Wang,Tao Xie,Zhen Xu,Yifan Yang,Yujun Shen,Hujun Bao,Xiaowei Zhou*

Main category: cs.CV

TL;DR: 提出了一种滑动迭代去噪方法，提升4D扩散模型在稀疏视角视频输入下的时空一致性，显著提高了新视角视频合成的质量。

- Motivation: 解决现有4D扩散模型在稀疏视角视频输入下生成视频时空一致性不足的问题。
- Method: 定义潜在网格，交替沿空间和时间维度滑动窗口去噪，解码目标视角视频。
- Result: 在DNA-Rendering和ActorsHQ数据集上表现优异，生成高质量且一致的新视角视频。
- Conclusion: 滑动迭代去噪方法有效提升了4D扩散模型的时空一致性，显著优于现有方法。


### [84] [Imbalance in Balance: Online Concept Balancing in Generation Models](https://arxiv.org/abs/2507.13345)
*Yukai Shi,Jiarong Ou,Rui Chen,Haotian Yang,Jiahao Wang,Xin Tao,Pengfei Wan,Di Zhang,Kun Gai*

Main category: cs.CV

TL;DR: 论文提出了一种在线概念均衡损失函数（IMBA损失），用于解决视觉生成任务中复杂概念响应不稳定的问题，无需离线数据处理，代码改动少。

- Motivation: 视觉生成任务中复杂概念的响应和组合缺乏稳定性且易出错，这一问题尚未被充分研究。
- Method: 设计了概念均衡损失函数（IMBA损失），该方法在线运行，无需离线数据处理，且代码改动极少。
- Result: 在新提出的复杂概念基准Inert-CompBench和两个公共测试集上，该方法显著提升了基线模型的概念响应能力，并取得了极具竞争力的结果。
- Conclusion: IMBA损失函数有效解决了复杂概念响应不稳定的问题，且实现简单高效。


### [85] [AutoPartGen: Autogressive 3D Part Generation and Discovery](https://arxiv.org/abs/2507.13346)
*Minghao Chen,Jianyuan Wang,Roman Shapovalov,Tom Monnier,Hyunyoung Jung,Dilin Wang,Rakesh Ranjan,Iro Laina,Andrea Vedaldi*

Main category: cs.CV

TL;DR: AutoPartGen是一个自回归模型，用于生成由3D部件组成的物体，支持多种输入（图像、2D掩码或现有3D物体），并自动生成部件数量和类型。

- Motivation: 现有的3D生成方法在部件级生成任务上表现不足，而3DShape2VecSet的潜在空间具有强大的组合性，适合此类任务。
- Method: 基于3DShape2VecSet的潜在空间，AutoPartGen自回归生成部件，每次预测一个部件并考虑先前生成的部件和其他输入（如图像或3D物体）。
- Result: AutoPartGen在3D部件生成任务上达到最先进性能，生成的部件无需额外优化即可组装成完整物体。
- Conclusion: AutoPartGen展示了在3D部件生成任务中的高效性和优越性，为组合式3D生成提供了新思路。


### [86] [$π^3$: Scalable Permutation-Equivariant Visual Geometry Learning](https://arxiv.org/abs/2507.13347)
*Yifan Wang,Jianjun Zhou,Haoyi Zhu,Wenzheng Chang,Yang Zhou,Zizun Li,Junyi Chen,Jiangmiao Pang,Chunhua Shen,Tong He*

Main category: cs.CV

TL;DR: Error

- Motivation: Error
- Method: Error
- Result: Error
- Conclusion: Error


### [87] [VisionThink: Smart and Efficient Vision Language Model via Reinforcement Learning](https://arxiv.org/abs/2507.13348)
*Senqiao Yang,Junyi Li,Xin Lai,Bei Yu,Hengshuang Zhao,Jiaya Jia*

Main category: cs.CV

TL;DR: VisionThink提出了一种动态视觉令牌压缩方法，通过自适应分辨率处理不同任务，显著节省计算资源。

- Motivation: 现有视觉语言模型通常使用过多视觉令牌，而实际场景中并不需要如此高的分辨率。
- Method: 采用动态分辨率处理，结合强化学习和LLM-as-Judge策略，智能决定是否请求高分辨率图像。
- Result: 在OCR任务中表现优异，同时在简单任务中大幅节省视觉令牌。
- Conclusion: VisionThink在性能和效率上均优于现有方法，代码已开源。


### [88] [Hierarchical Rectified Flow Matching with Mini-Batch Couplings](https://arxiv.org/abs/2507.13350)
*Yichi Zhang,Yici Yan,Alex Schwing,Zhizhen Zhao*

Main category: cs.CV

TL;DR: 论文研究了如何通过mini-batch couplings在分层流匹配中逐步调整分布复杂度，展示了在合成和图像数据上的优势。

- Motivation: 分层流匹配虽然能建模多模态速度分布，但各层分布的复杂度相同，限制了模型的表达能力。
- Method: 提出使用mini-batch couplings在分层流匹配中逐步调整分布复杂度。
- Result: 在合成和图像数据上取得了显著效果。
- Conclusion: mini-batch couplings能有效提升分层流匹配的性能。


### [89] [VideoITG: Multimodal Video Understanding with Instructed Temporal Grounding](https://arxiv.org/abs/2507.13353)
*Shihao Wang,Guo Chen,De-an Huang,Zhiqi Li,Minghan Li,Guilin Li,Jose M. Alvarez,Lei Zhang,Zhiding Yu*

Main category: cs.CV

TL;DR: VideoITG提出了一种基于用户指令的视频帧选择方法，通过VidThinker流程生成详细标注，显著提升了视频大语言模型的性能。

- Motivation: 现有方法在长视频理解中难以处理复杂场景，需要一种更有效的帧选择策略。
- Method: 采用VidThinker流程，包括生成指令相关的剪辑级描述、检索相关视频片段和精细帧选择。
- Result: 构建了VideoITG-40K数据集，并在多模态视频理解基准测试中表现优异。
- Conclusion: VideoITG展示了在视频理解中的优越性和潜力。
## cs.GR

### [90] [HairFormer: Transformer-Based Dynamic Neural Hair Simulation](https://arxiv.org/abs/2507.12600)
*Joy Xiaoji Zhang,Jingsen Zhu,Hanyu Chen,Steve Marschner*

Main category: cs.GR

TL;DR: 提出了一种基于Transformer的两阶段神经网络方法，用于模拟任意发型、体型和动作的头发动态效果。

- Motivation: 解决头发动态模拟在广泛场景下的通用性问题，包括发型、体型和动作的多样性。
- Method: 采用两阶段方法：静态网络预测发型与身体的静态交互，动态网络通过跨注意力机制融合静态特征与运动输入，生成动态效果。
- Result: 实现了高保真、通用的头发动态模拟，支持实时推理，并能处理复杂发型和突然动作。
- Conclusion: 该方法在广泛场景下表现出优异的通用性和保真度，解决了头发与身体的穿透问题。
## eess.IV

### [91] [Pathology-Guided Virtual Staining Metric for Evaluation and Training](https://arxiv.org/abs/2507.12624)
*Qiankai Wang,James E. D. Tweel,Parsin Haji Reza,Anita Layton*

Main category: eess.IV

TL;DR: 提出了一种名为PaPIS的新型FR-IQA指标，专门用于虚拟染色评估，优于传统方法。

- Motivation: 传统评估方法（如FR-IQA指标）无法捕捉病理相关特征，专家评审又主观且耗时。
- Method: PaPIS结合深度学习特征和Retinex分解，更贴合组织学感知质量。
- Result: PaPIS更准确反映病理相关视觉线索，提升虚拟染色的组织学保真度。
- Conclusion: 病理感知评估框架对虚拟染色技术的发展至关重要。


### [92] [InSight: AI Mobile Screening Tool for Multiple Eye Disease Detection using Multimodal Fusion](https://arxiv.org/abs/2507.12669)
*Ananya Raghu,Anisha Raghu,Alice S. Tang,Yannis M. Paulus,Tyson N. Kim,Tomiko T. Oskotsky*

Main category: eess.IV

TL;DR: InSight是一款基于AI的应用，结合患者元数据和眼底图像，用于准确诊断五种常见眼病，提高筛查可及性。

- Motivation: 解决低收入和资源有限地区眼病早期筛查的医疗资源不足问题。
- Method: 采用三阶段流程：实时图像质量评估、疾病诊断模型和DR分级模型，结合多模态融合技术和多任务模型。
- Result: 图像质量检查器准确率近100%，多模态疾病诊断模型性能优于仅使用图像的模型。
- Conclusion: InSight流程在不同图像条件下表现稳健，诊断准确率高，计算效率高。


### [93] [TRIQA: Image Quality Assessment by Contrastive Pretraining on Ordered Distortion Triplets](https://arxiv.org/abs/2507.12687)
*Rajesh Sureddi,Saman Zadtootaghaj,Nabajeet Barman,Alan C. Bovik*

Main category: eess.IV

TL;DR: 提出了一种基于对比三元组学习的无参考图像质量评估方法，通过构建自定义数据集和结合内容与质量特征，解决了数据稀缺问题。

- Motivation: 无参考图像质量评估（NR-IQA）因缺乏参考图像和主观标注数据有限而具有挑战性，现有方法依赖大规模预训练数据。
- Method: 使用少量参考图像构建自定义数据集，结合内容和质量特征，采用对比三元组学习方法训练模型。
- Result: 模型在少量样本下表现出色，并在公开数据集上展现出强大的泛化性能。
- Conclusion: 该方法为NR-IQA提供了一种高效且数据友好的解决方案，代码已开源。


### [94] [Pixel Perfect MegaMed: A Megapixel-Scale Vision-Language Foundation Model for Generating High Resolution Medical Images](https://arxiv.org/abs/2507.12698)
*Zahra TehraniNasab,Amar Kumar,Tal Arbel*

Main category: eess.IV

TL;DR: Pixel Perfect MegaMed 是一种基于视觉-语言对齐的高分辨率医学图像生成模型，能够在1024x1024分辨率下生成保留全局解剖结构和局部细节的医学图像。

- Motivation: 传统生成模型（如GANs和VAEs）在医学图像生成中难以保留对诊断至关重要的细粒度细节，因此需要一种新方法来解决这一问题。
- Method: 采用多尺度Transformer架构，结合视觉-语言对齐技术，专门用于超高分辨率医学图像生成。
- Result: 在CheXpert数据集上验证了模型能够生成临床可信的胸部X光图像，并在数据增强任务中提升了分类性能。
- Conclusion: Pixel Perfect MegaMed 为高分辨率医学图像合成提供了有效解决方案，并在下游任务中表现出实用价值。


### [95] [Unleashing Vision Foundation Models for Coronary Artery Segmentation: Parallel ViT-CNN Encoding and Variational Fusion](https://arxiv.org/abs/2507.12938)
*Caixia Dong,Duwei Dai,Xinyi Han,Fan Liu,Xu Yang,Zongfang Li,Songhua Xu*

Main category: eess.IV

TL;DR: 提出了一种基于视觉基础模型（VFM）的并行编码架构，结合ViT和CNN编码器，通过交叉分支变分融合（CVF）和证据学习不确定性细化（EUR）模块，显著提升了冠状动脉分割的准确性。

- Motivation: 冠状动脉分割对CAD诊断至关重要，但由于血管小、形态复杂且与周围组织对比度低，现有方法效果有限。
- Method: 采用并行编码架构，ViT捕获全局特征，CNN提取局部细节，通过CVF模块自适应融合特征，EUR模块量化不确定性并优化分割结果。
- Result: 在多个数据集上验证，性能显著优于现有方法，展示了强泛化能力。
- Conclusion: 提出的框架在冠状动脉分割任务中表现出色，代码已开源。


### [96] [Improving Diagnostic Accuracy of Pigmented Skin Lesions With CNNs: an Application on the DermaMNIST Dataset](https://arxiv.org/abs/2507.12961)
*Nerma Kadric,Amila Akagic,Medina Kapo*

Main category: eess.IV

TL;DR: 研究评估了ResNet-50和EfficientNetV2L模型在DermaMNIST数据集上的多分类性能，结果表明某些配置优于现有方法，支持CNN在生物医学图像分析中的应用。

- Motivation: 色素性皮肤病变可能预示严重疾病如黑色素瘤，需要高效准确的诊断方法。
- Method: 使用ResNet-50和EfficientNetV2L模型，结合迁移学习和不同层配置，对DermaMNIST数据集进行多分类。
- Result: 某些模型配置的表现优于现有方法，显著提高了诊断准确性。
- Conclusion: 卷积神经网络（CNN）在生物医学图像分析中具有潜力，可推动诊断技术的进步。


### [97] [From Variability To Accuracy: Conditional Bernoulli Diffusion Models with Consensus-Driven Correction for Thin Structure Segmentation](https://arxiv.org/abs/2507.12985)
*Jinseo An,Min Jin Lee,Kyu Won Shim,Helen Hong*

Main category: eess.IV

TL;DR: 提出了一种基于多扩散模型共识的框架，用于改进面部CT图像中眼眶骨的分割，解决了模糊边界和薄结构的分割问题。

- Motivation: 眼眶骨分割在定制植入物中至关重要，但现有方法在模糊区域（如眼眶内侧壁和底部）常产生不连续或欠分割结果。
- Method: 使用条件伯努利扩散模型生成多种可能的分割结果，并通过共识驱动校正（结合位置邻近性、共识水平和梯度方向相似性）修正模糊区域。
- Result: 实验表明，该方法优于现有方法，显著提高了模糊区域的召回率，同时保持了薄结构的连续性。
- Conclusion: 该方法自动化了分割结果的修正过程，可应用于图像引导的手术规划和手术。


### [98] [fastWDM3D: Fast and Accurate 3D Healthy Tissue Inpainting](https://arxiv.org/abs/2507.13146)
*Alicia Durrer,Florentin Bieder,Paul Friedrich,Bjoern Menze,Philippe C. Cattin,Florian Kofler*

Main category: eess.IV

TL;DR: 论文提出了一种结合DDPMs和GANs的2D图像生成方法，用于3D修复任务，并通过改进的噪声调度和重构损失实现了高质量快速修复。

- Motivation: 解决DDPMs在健康组织修复中采样速度慢的问题，同时保持高质量结果。
- Method: 结合DDPMs与GANs，采用方差保持噪声调度和特定重构损失，无需对抗训练。
- Result: fastWDM3D模型在BraTS测试集上SSIM为0.8571，MSE为0.0079，PSNR为22.26，仅需2步完成修复。
- Conclusion: fastWDM3D是一种快速且准确的健康组织修复方法，速度提升800倍且性能优越。


### [99] [SpectraLift: Physics-Guided Spectral-Inversion Network for Self-Supervised Hyperspectral Image Super-Resolution](https://arxiv.org/abs/2507.13339)
*Ritik Shah,Marco F. Duarte*

Main category: eess.IV

TL;DR: SpectraLift是一种自监督框架，通过融合低分辨率高光谱图像（LR-HSI）和高分辨率多光谱图像（HR-MSI）生成高分辨率高光谱图像（HR-HSI），无需PSF校准或地面真实数据。

- Motivation: 高分辨率高光谱图像（HSI）在遥感等领域至关重要，但现有方法需要PSF校准或地面真实数据，实际应用中难以获取。
- Method: SpectraLift使用轻量级多层感知机（MLP）网络，通过合成低分辨率多光谱图像（LR-MSI）和LR-HSI进行训练，优化目标是光谱重建损失。推理时，将HR-MSI映射为HR-HSI。
- Result: SpectraLift在PSNR、SAM、SSIM和RMSE等指标上优于现有方法，且收敛速度快，对空间模糊和分辨率不敏感。
- Conclusion: SpectraLift提供了一种高效、自监督的HSI-MSI融合方法，无需额外校准数据，适用于实际场景。
## cs.RO

### [100] [Physically Based Neural LiDAR Resimulation](https://arxiv.org/abs/2507.12489)
*Richard Marcus,Marc Stamminger*

Main category: cs.RO

TL;DR: 该论文提出了一种改进的LiDAR模拟方法，通过显式建模传感器特性（如滚动快门、激光功率变化和强度衰减），实现了比现有技术更准确的LiDAR模拟。

- Motivation: 现有方法在LiDAR特定效果（如传感器特性）的模拟上表现不足，因此需要一种更精确的LiDAR模拟方法。
- Method: 显式建模传感器特性（如滚动快门、激光功率变化和强度衰减），并通过定量、定性比较及消融研究验证方法的有效性。
- Result: 实验表明，该方法在LiDAR模拟精度上优于现有技术，并展示了高级重模拟能力（如生成相机视角的高分辨率LiDAR扫描）。
- Conclusion: 该方法显著提升了LiDAR模拟的准确性，并具备高级重模拟能力，为LiDAR仿真领域提供了新的解决方案。


### [101] [Rethinking the Embodied Gap in Vision-and-Language Navigation: A Holistic Study of Physical and Visual Disparities](https://arxiv.org/abs/2507.13019)
*Liuyi Wang,Xinyuan Xia,Hui Zhao,Hanqing Wang,Tai Wang,Yilun Chen,Chengju Liu,Qijun Chen,Jiangmiao Pang*

Main category: cs.RO

TL;DR: VLN-PE是一个物理现实的VLN平台，支持人形、四足和轮式机器人，评估了多种视觉与语言导航方法在物理环境中的表现，揭示了性能下降的原因，并提供了扩展性强的工具。

- Motivation: 当前VLN方法在物理机器人部署中的理想化假设与实际挑战存在差距，需要更真实的评估平台。
- Method: 引入VLN-PE平台，评估了分类模型、扩散模型和基于地图的LLM方法在物理环境中的表现。
- Result: 发现机器人观察空间有限、环境光照变化和物理挑战导致性能显著下降，同时暴露了腿式机器人在复杂环境中的运动限制。
- Conclusion: VLN-PE为改进跨具身适应性提供了新途径，并鼓励社区重新思考VLN的局限性。
## math.OC

### [102] [Tensor-Tensor Products, Group Representations, and Semidefinite Programming](https://arxiv.org/abs/2507.12729)
*Alex Dunbar,Elizabeth Newman*

Main category: math.OC

TL;DR: Error

- Motivation: Error
- Method: Error
- Result: Error
- Conclusion: Error
## eess.SY

### [103] [Dual LiDAR-Based Traffic Movement Count Estimation at a Signalized Intersection: Deployment, Data Collection, and Preliminary Analysis](https://arxiv.org/abs/2507.13073)
*Saswat Priyadarshi Nayak,Guoyuan Wu,Kanok Boriboonsomsin,Matthew Barth*

Main category: eess.SY

TL;DR: 该论文提出了一种基于双LiDAR系统的交通流量计数方法，以解决传统摄像头技术在恶劣天气和夜间条件下的不准确问题，并在实际场景中进行了部署和评估。

- Motivation: 传统交通流量计数方法（如摄像头）在恶劣天气和夜间条件下表现不佳，而LiDAR技术因其成本降低和应用扩展成为更优选择。
- Method: 作者开发并部署了一个双LiDAR系统，利用3D边界框检测对车辆进行分类和计数，分析交通方向、车辆运动和类型。
- Result: 论文展示了双LiDAR系统在交通流量计数中的实际效果，并分析了其趋势和不规则性。
- Conclusion: 双LiDAR系统在交通流量计数中具有潜力，未来可进一步优化以提升轨迹预测和意图预测能力。
## cs.LG

### [104] [Multimodal-Guided Dynamic Dataset Pruning for Robust and Efficient Data-Centric Learning](https://arxiv.org/abs/2507.12750)
*Suorong Yang,Peijia Li,Yujie Liu,Zhiming Xu,Peng Ye,Wanli Ouyang,Furao Shen,Dongzhan Zhou*

Main category: cs.LG

TL;DR: 提出了一种动态数据集剪枝框架，结合任务驱动难度和跨模态语义一致性，提升训练效率和模型性能。

- Motivation: 现有数据集剪枝方法依赖静态启发式或任务特定指标，缺乏跨域鲁棒性和通用性。
- Method: 利用预训练多模态基础模型的监督，动态选择训练样本，过滤无信息样本。
- Result: 框架能有效捕捉训练动态，提升样本选择的鲁棒性。
- Conclusion: 跨模态对齐的集成推动了数据为中心学习的高效和鲁棒实践。


### [105] [WaveletInception Networks for Drive-by Vibration-Based Infrastructure Health Monitoring](https://arxiv.org/abs/2507.12969)
*Reza Riahi Samani,Alfredo Nunez,Bart De Schutter*

Main category: cs.LG

TL;DR: 提出了一种基于深度学习的框架WaveletInception-BiLSTM，用于通过振动信号监测基础设施健康状态，显著优于现有方法。

- Motivation: 基础设施健康监测需要高效且自动化的方法，传统方法依赖预处理且无法捕捉多尺度信息。
- Method: 结合Learnable Wavelet Packet Transform (LWPT)和1D Inception网络提取多尺度特征，再用BiLSTM整合时序信息。
- Result: 在铁路轨道刚度估计案例中，模型显著优于现有方法，实现了高分辨率、自动化的健康评估。
- Conclusion: 该方法为基础设施健康监测提供了准确、局部化和自动化的解决方案。


### [106] [DASViT: Differentiable Architecture Search for Vision Transformer](https://arxiv.org/abs/2507.13079)
*Pengjin Wu,Ferrante Neri,Zhenhua Feng*

Main category: cs.LG

TL;DR: DASViT是一种可微分架构搜索方法，专为Vision Transformers设计，解决了传统方法的局限性，实现了高效且创新的架构设计。

- Motivation: 传统NAS方法在ViT架构搜索中存在创新性不足、计算资源消耗大和时间长的问题，DASViT旨在解决这些问题。
- Method: DASViT采用可微分架构搜索方法，专注于ViT的架构设计，避免了离散搜索的高成本。
- Result: 实验表明，DASViT设计的架构突破了传统Transformer编码器设计，性能优于ViT-B/16，且参数和计算量更少。
- Conclusion: DASViT为ViT提供了一种高效且创新的架构搜索方法，具有显著优势。


### [107] [MUPAX: Multidimensional Problem Agnostic eXplainable AI](https://arxiv.org/abs/2507.13090)
*Vincenzo Dentamaro,Felice Franchini,Giuseppe Pirlo,Irina Voiculescu*

Main category: cs.LG

TL;DR: MUPAX是一种确定性、模型无关且保证收敛的XAI技术，通过结构化扰动分析提供特征重要性，适用于多种数据模态和任务。

- Motivation: 现有XAI方法通常缺乏确定性、模型无关性或收敛保证，且可能降低模型性能，因此需要一种更稳健的XAI技术。
- Method: MUPAX采用测度理论框架，通过结构化扰动分析发现输入模式并消除虚假关系。
- Result: MUPAX在多种任务中表现优异，不仅保持模型准确性，还能提升性能，生成精确且一致的解释。
- Conclusion: MUPAX为构建可解释和可信赖的AI系统提供了重要工具，其广泛适用性和收敛性使其成为XAI领域的突破。
