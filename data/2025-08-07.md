[[toc]]

## cs.CV

### [1] [Text2VR: Automated instruction Generation in Virtual Reality using Large language Models for Assembly Task](https://arxiv.org/abs/2508.03699)
*Subin Raj Peter*

Main category: cs.CV

TL;DR: 提出了一种利用大型语言模型（LLM）自动从文本生成虚拟指令的新方法，以简化VR培训内容的开发。

- Motivation: VR培训内容开发耗时且需要专业知识，限制了其广泛应用。
- Method: 结合LLM模块提取任务信息，智能模块将其转化为VR环境中的动画演示和视觉提示。
- Result: 提高了培训效果，降低了开发成本，使VR培训更具可扩展性和适应性。
- Conclusion: 该方法为VR培训提供了一种高效、自动化的解决方案，适合工业需求的变化。


### [2] [Outlier Detection Algorithm for Circle Fitting](https://arxiv.org/abs/2508.03720)
*Ahmet Gökhan Poyraz*

Main category: cs.CV

TL;DR: 论文提出了一种基于极坐标的异常值检测算法（PCOD），用于提高圆拟合的准确性，尤其在噪声点集的情况下。通过局部和全局标准差比较识别异常值，并在工业垫圈的高精度直径测量中验证了其优越性。

- Motivation: 圆拟合算法在噪声点集下效果不佳，现有异常值检测方法需改进以提高精度。
- Method: 将点集转换为极坐标，计算局部和全局标准差，通过比较识别异常值，再进行圆拟合。
- Result: 提出的PCOD算法在十种圆拟合算法和五种异常值检测方法中表现最佳，准确性最高。
- Conclusion: PCOD算法能有效提升工业环境中圆拟合的精度，具有实际应用潜力。


### [3] [Enhancing Diameter Measurement Accuracy in Machine Vision Applications](https://arxiv.org/abs/2508.03721)
*Ahmet Gokhan Poyraz,Ahmet Emir Dirik,Hakan Gurkan,Mehmet Kacmaz*

Main category: cs.CV

TL;DR: 提出两种新方法（基于转换因子和基于像素）以提高相机测量系统的精度，显著减少误差。

- Motivation: 解决因机械和软件因素导致的测量误差，尤其是在测量不同直径零件时。
- Method: 使用已知参考零件，分别通过转换因子和像素信息估计未知零件的直径。
- Result: 误差从13-114微米降至1-2微米，显著提高测量精度。
- Conclusion: 该方法仅需少量参考零件即可实现高精度测量，显著提升测量可靠性。


### [4] [Multimodal Video Emotion Recognition with Reliable Reasoning Priors](https://arxiv.org/abs/2508.03722)
*Zhepeng Wang,Yingjian Zhu,Guanghao Dong,Hongzhu Yi,Feng Chen,Xinming Wang,Jun Xie*

Main category: cs.CV

TL;DR: 该研究通过将MLLMs的可信先验推理知识融入多模态情感识别，结合Gemini生成细粒度推理痕迹，并通过平衡双对比学习解决类别不平衡问题，显著提升了性能。

- Motivation: 探索如何将MLLMs的可信推理知识有效融入多模态情感识别，以提升模型的性能和鲁棒性。
- Method: 使用Gemini生成细粒度、模态可分离的推理痕迹作为先验知识，在融合阶段注入；提出平衡双对比学习以解决类别不平衡问题。
- Result: 在MER2024基准测试中，先验增强框架显著提升了性能，证明了MLLM推理与轻量级融合网络的协同效应。
- Conclusion: MLLM的可靠推理与轻量级融合网络的领域适应性可以协同作用，实现鲁棒且可扩展的情感识别。


### [5] [From Waveforms to Pixels: A Survey on Audio-Visual Segmentation](https://arxiv.org/abs/2508.03724)
*Jia Li,Yapeng Tian*

Main category: cs.CV

TL;DR: 本文综述了音频-视觉分割（AVS）领域，涵盖问题定义、数据集、评估指标和方法进展，分析了不同方法、训练范式，并比较了性能。最后提出当前挑战和未来方向。

- Motivation: AVS通过结合视觉和音频模态识别视频中的发声物体，是多模态感知的重要研究方向，旨在实现细粒度的物体级理解。
- Method: 综述了AVS的方法论，包括单模态和多模态编码架构、音频-视觉融合策略、解码器设计，以及从全监督到弱监督和无监督的训练范式。
- Result: 通过标准基准测试比较了不同AVS方法的性能，分析了架构选择、融合策略和训练范式的影响。
- Conclusion: 当前挑战包括时间建模不足、视觉模态偏置、复杂环境鲁棒性差和高计算需求。未来方向包括改进时间推理和多模态融合、利用基础模型、减少标注数据依赖及引入高级推理。


### [6] [A Large Language Model Powered Integrated Circuit Footprint Geometry Understanding](https://arxiv.org/abs/2508.03725)
*Yida Wang,Taiting Lu,Runze Liu,Lanqing Yang,Yifan Yang,Zhe Chen,Yuehai Wang,Yixin Liu,Kaiyuan Lin,Xiaomeng Chen,Dian Ding,Yijie Li,Yi-Chao Chen,Yincheng Jin,Mahanth Gowda*

Main category: cs.CV

TL;DR: 论文提出LLM4-IC8K框架，利用LLM从IC机械图纸中自动标注PCB封装几何，解决了当前LMM在几何感知上的不足，并通过实验验证其优越性。

- Motivation: 当前缺乏从IC机械图纸自动标注PCB封装几何的方法，且现有LMM在几何感知上表现不佳，亟需解决方案。
- Method: 提出LLM4-IC8K框架，将IC图纸视为图像，分三步任务（引脚数量感知、中心坐标计算、引脚尺寸估计），并分两阶段训练（合成数据预训练、真实数据微调）。
- Result: 实验表明，LLM4-IC8K在ICGeo8K数据集上优于现有LMM。
- Conclusion: LLM4-IC8K为IC封装几何标注提供了高效自动化解决方案，填补了领域空白。


### [7] [TIR-Diffusion: Diffusion-based Thermal Infrared Image Denoising via Latent and Wavelet Domain Optimization](https://arxiv.org/abs/2508.03727)
*Tai Hyoung Rhee,Dong-guw Lee,Ayoung Kim*

Main category: cs.CV

TL;DR: 提出了一种基于扩散模型的热红外图像去噪框架，结合潜在空间和小波域优化，显著提升了去噪性能。

- Motivation: 热红外图像在低能见度或复杂光照条件下具有潜力，但存在固定模式噪声，影响目标检测和定位等任务。
- Method: 利用预训练的稳定扩散模型，通过潜在空间和小波变换损失函数进行微调，并采用级联细化阶段增强细节。
- Result: 在基准数据集上表现优于现有方法，且能零样本泛化到真实世界数据集。
- Conclusion: 该方法在热红外图像去噪中表现出高效性和实用性，适用于机器人部署。


### [8] [What is Beneath Misogyny: Misogynous Memes Classification and Explanation](https://arxiv.org/abs/2508.03732)
*Kushal Kanwar,Dushyant Singh Chauhan,Gopendra Vikram Singh,Asif Ekbal*

Main category: cs.CV

TL;DR: 论文提出了一种多模态方法MM-Misogyny，用于检测、分类和解释模因中的厌女内容，结合文本和图像处理，并通过跨注意力机制统一模态。

- Motivation: 现代社会中，看似无害的模因可能传播有害的意识形态（如厌女症），但其多模态性和社会背景的复杂性使其检测和理解成为研究挑战。
- Method: MM-Misogyny分别处理文本和图像模态，通过跨注意力机制统一为多模态上下文，再通过分类器和大型语言模型（LLM）进行标注、分类和解释。
- Result: 模型在新建数据集WBMS上评估，成功检测并分类厌女内容，提供对厌女症在生活领域中的细粒度理解，表现优于现有方法。
- Conclusion: MM-Misogyny方法在检测和解释厌女模因方面具有优势，代码和数据集已公开。


### [9] [StorySync: Training-Free Subject Consistency in Text-to-Image Generation via Region Harmonization](https://arxiv.org/abs/2508.03735)
*Gopalji Gaur,Mohammadreza Zolfaghari,Thomas Brox*

Main category: cs.CV

TL;DR: 提出一种无需训练的文本到图像扩散模型方法，通过跨图像注意力共享和区域特征协调，实现跨场景主题一致性的高效生成。

- Motivation: 现有方法依赖微调或重新训练模型，计算成本高且可能干扰模型原有能力，因此需要一种无需训练的高效解决方案。
- Method: 采用掩码跨图像注意力共享动态对齐主题特征，并通过区域特征协调优化视觉细节。
- Result: 实验表明，该方法能在多种场景下生成视觉一致的主题，同时保持模型的创造力。
- Conclusion: 无需训练的方法在主题一致性和模型能力之间取得了平衡，为视觉故事生成提供了高效解决方案。


### [10] [Fusion of Pervasive RF Data with Spatial Images via Vision Transformers for Enhanced Mapping in Smart Cities](https://arxiv.org/abs/2508.03736)
*Rafayel Mkrtchyan,Armen Manukyan,Hrant Khachatrian,Theofanis P. Raptis*

Main category: cs.CV

TL;DR: 本文提出了一种基于DINOv2架构的深度学习方法，结合开源地图和射频数据，提升建筑映射的准确性。

- Motivation: 传统智能城市映射技术（如卫星图像、LiDAR扫描）存在成本高、可访问性差和精度不足的问题，而开源地图数据因人为错误和环境变化引入偏差。
- Method: 采用视觉Transformer架构，统一处理射频数据和地图模态，利用空间依赖性和结构先验提升映射精度。
- Result: 在合成数据集上，模型在Jaccard指数（IoU）上达到65.3%，显著优于基线方法。
- Conclusion: 结合多模态数据的深度学习方法能有效提升环境映射的准确性。


### [11] [VQ-DeepISC: Vector Quantized-Enabled Digital Semantic Communication with Channel Adaptive Image Transmission](https://arxiv.org/abs/2508.03740)
*Jianqiao Chen,Tingting Zhu,Huishi Song,Nan Ma,Xiaodong Xu*

Main category: cs.CV

TL;DR: 论文提出了一种基于向量量化（VQ）的数字语义通信系统VQ-DeepISC，通过离散化语义特征实现高效传输，并在实验中表现出优越的重建性能。

- Motivation: 语义特征离散化是实现语义与数字通信系统互操作的关键，但需解决连续性和上下文保持的挑战。
- Method: 采用Swin Transformer提取分层语义特征，结合VQ模块将特征投影到离散空间，并通过注意力机制优化索引传输。训练中使用KLD正则化和EMA稳定代码书更新。
- Result: 实验表明，VQ-DeepISC在重建保真度上优于基准方法。
- Conclusion: VQ-DeepISC为数字语义通信提供了一种高效且鲁棒的解决方案。


### [12] [Tobler's First Law in GeoAI: A Spatially Explicit Deep Learning Model for Terrain Feature Detection Under Weak Supervision](https://arxiv.org/abs/2508.03745)
*Wenwen Li,Chia-Yu Hsu,Maosheng Hu*

Main category: cs.CV

TL;DR: 本文提出了一种弱监督的深度学习模型，用于自然特征的目标检测，结合了地理学第一定律和注意力机制，显著提升了GeoAI的性能。

- Motivation: GeoAI在解决地理空间问题时面临训练数据不足和忽视空间原则的挑战，阻碍了AI与地理空间研究的深度融合。
- Method: 开发了一种基于弱标签的目标检测方法，结合地理学第一定律和注意力机制，采用多阶段训练策略。
- Result: 模型成功应用于火星撞击坑检测，并推广到地球和其他行星的自然和人造特征。
- Conclusion: 该研究推动了GeoAI的理论和方法基础。


### [13] [Closed-Circuit Television Data as an Emergent Data Source for Urban Rail Platform Crowding Estimation](https://arxiv.org/abs/2508.03749)
*Riccardo Fiorista,Awad Abdelhalim,Anson F. Stewart,Gabriel L. Pincus,Ian Thistle,Jinhua Zhao*

Main category: cs.CV

TL;DR: 论文探讨了利用CCTV视频数据通过计算机视觉技术实时估计地铁站台拥挤度的方法，并验证了其有效性。

- Motivation: 准确估计地铁站台拥挤度可提升运营决策能力，改善安全和乘客体验，但现有方法依赖间接数据，实时性不足。
- Method: 比较了三种计算机视觉方法（目标检测、人群分类、语义分割），并提出了一种基于线性优化的计数方法。
- Result: 在600多小时视频数据上测试，证明计算机视觉方法能有效估计拥挤度。
- Conclusion: CCTV数据可独立支持实时拥挤度估计，为运营决策提供精准依据。


### [14] [Modular Transformer Architecture for Precision Agriculture Imaging](https://arxiv.org/abs/2508.03751)
*Brian Gopalan,Nathalia Nascimento,Vishal Monga*

Main category: cs.CV

TL;DR: 提出了一种基于质量感知的模块化深度学习框架，用于无人机视频中的杂草分割，通过动态路由输入到优化的预处理和变换器模型，显著提升了分割质量和计算效率。

- Motivation: 解决精准农业中无人机视频杂草分割的高效性和准确性需求，应对图像退化问题。
- Method: 使用Mean Absolute Deviation和Laplacian分析图像质量，动态路由到三种优化的变换器模型之一。
- Result: 在分割质量和计算效率上优于现有CNN方法。
- Conclusion: 该框架在农业深度学习应用中具有显著优势。


### [15] [Generating Synthetic Invoices via Layout-Preserving Content Replacement](https://arxiv.org/abs/2508.03754)
*Bevin V,Ananthakrishnan P V,Ragesh KR,Sanjay M,Vineeth S,Bibin Wilson*

Main category: cs.CV

TL;DR: 提出了一种生成合成发票文档的自动化流程，通过OCR提取内容、LLM生成替换文本，并结合图像修复技术，以解决数据隐私和标注成本问题。

- Motivation: 解决自动化发票处理中因隐私法规和标注成本高导致的大规模多样化数据集获取困难的问题。
- Method: 使用OCR提取原始发票的文本和布局，用LLM生成上下文相关的合成内容，再通过图像修复技术替换原始文本，生成新的发票图像和对应的结构化数据。
- Result: 生成了视觉逼真的合成发票图像和完美对齐的结构化数据文件，为训练文档智能模型提供了可扩展的解决方案。
- Conclusion: 该方法能够高效扩展小型私有数据集，为训练更鲁棒和准确的文档智能模型提供支持。


### [16] [Refine-IQA: Multi-Stage Reinforcement Finetuning for Perceptual Image Quality Assessment](https://arxiv.org/abs/2508.03763)
*Ziheng Jia,Jiaying Qian,Zicheng Zhang,Zijian Chen,Xiongkuo Min*

Main category: cs.CV

TL;DR: Refine-IQA提出了一种多阶段强化微调框架，通过增强低层次视觉质量感知和监督“思考”过程，显著提升了图像质量评估（IQA）的性能。

- Motivation: 现有基于RFT的IQA方法缺乏对“思考”过程的奖励监督，且未明确增强模型的低层次视觉感知能力，限制了性能上限。
- Method: 提出两阶段框架：第一阶段通过多任务奖励函数增强视觉感知；第二阶段引入概率差异奖励策略监督“思考”过程。
- Result: Refine-IQA系列模型在感知和评分任务中表现优异，并激活了强大的“思考”能力。
- Conclusion: Refine-IQA通过多阶段设计和奖励监督，显著提升了IQA任务的性能和模型的可解释性。


### [17] [4D-PreNet: A Unified Preprocessing Framework for 4D-STEM Data Analysis](https://arxiv.org/abs/2508.03775)
*Mingyu Liu,Zian Mao,Zhu Liu,Haoran Zhang,Jintao Guo,Xiaoya He,Xi Huang,Shufen Chu,Chun Cheng,Jun Ding,Yujun Xie*

Main category: cs.CV

TL;DR: 4D-PreNet是一个深度学习框架，用于解决4D-STEM数据预处理中的噪声、中心漂移和椭圆畸变问题，显著提升实时分析的可靠性。

- Motivation: 高吞吐量4D-STEM数据采集中的噪声、中心漂移和椭圆畸变导致定量测量偏差，传统算法缺乏通用性。
- Method: 结合注意力增强的U-Net和ResNet架构，通过大规模模拟数据集训练，实现去噪、中心校正和畸变校准。
- Result: 去噪均方误差降低50%，中心定位误差低于0.04像素，优于传统算法。
- Conclusion: 4D-PreNet为自动化4D-STEM实时分析提供了高效、通用的解决方案。


### [18] [HPSv3: Towards Wide-Spectrum Human Preference Score](https://arxiv.org/abs/2508.03789)
*Yuhang Ma,Xiaoshi Wu,Keqiang Sun,Hongsheng Li*

Main category: cs.CV

TL;DR: 论文提出了HPSv3，一种基于人类偏好的文本到图像生成评估方法，并发布了HPDv3数据集和CoHP迭代优化方法。

- Motivation: 现有的人类中心评估指标存在数据覆盖不足、特征提取不优和损失函数效率低的问题，需要更符合人类感知的评估方法。
- Method: 1. 发布HPDv3数据集，包含108万文本-图像对和117万标注的成对比较；2. 提出基于VLM的偏好模型，使用不确定性感知排序损失；3. 提出CoHP方法，通过迭代优化提升图像质量。
- Result: HPSv3成为广泛图像评估的鲁棒指标，CoHP提供了一种高效且符合人类偏好的图像生成优化方法。
- Conclusion: HPSv3和CoHP为文本到图像生成提供了更符合人类感知的评估和优化工具。


### [19] [Deep learning framework for crater detection and identification on the Moon and Mars](https://arxiv.org/abs/2508.03920)
*Yihan Ma,Zeyang Yu,Rohitash Chandra*

Main category: cs.CV

TL;DR: 论文提出了一种基于深度学习的陨石坑检测框架，结合CNN、ResNet-50和YOLO模型，分两阶段实现陨石坑的识别与定位，并在火星和月球遥感数据上验证了性能。

- Motivation: 陨石坑的空间分布和形态特征对行星科学研究至关重要，深度学习模型的快速发展为自动化陨石坑检测提供了新机遇。
- Method: 采用两阶段框架：第一阶段使用CNN、ResNet-50和YOLO进行陨石坑识别；第二阶段基于YOLO实现陨石坑定位。
- Result: YOLO在陨石坑检测中表现最均衡，ResNet-50在大型陨石坑识别中精度更高。
- Conclusion: 提出的框架有效提升了陨石坑检测与识别的自动化水平，为行星科学研究提供了新工具。


### [20] [Point-Based Shape Representation Generation with a Correspondence-Preserving Diffusion Model](https://arxiv.org/abs/2508.03925)
*Shen Zhu,Yinzhu Jin,Ifrah Zawar,P. Thomas Fletcher*

Main category: cs.CV

TL;DR: 提出了一种扩散模型，用于生成具有对应关系的点基形状表示，解决了现有深度学习方法忽略点对应关系的问题。

- Motivation: 传统统计形状模型关注点对应关系，而当前深度学习方法仅关注无序点云，无法生成具有对应关系的形状。
- Method: 设计了一种扩散模型，利用OASIS-3数据中的对应关系，生成具有点对应关系的形状表示。
- Result: 模型生成的形状表示高度逼真，优于现有方法，并能应用于条件生成和疾病形态变化预测。
- Conclusion: 该模型成功生成了具有对应关系的形状，为下游任务提供了实用工具。


### [21] [Policy to Assist Iteratively Local Segmentation: Optimising Modality and Location Selection for Prostate Cancer Localisation](https://arxiv.org/abs/2508.03953)
*Xiangcen Wu,Shaheer U. Saeed,Yipei Wang,Ester Bonmati Coll,Yipeng Hu*

Main category: cs.CV

TL;DR: 提出了一种推荐系统，通过动态选择最佳成像模态和图像区域，优化前列腺癌分割性能。

- Motivation: 放射科医生通常混合使用多种医学图像阅读策略，但现有机器学习模型缺乏动态选择能力，因此需要一种能模拟医生行为的推荐系统。
- Method: 训练一个策略网络，动态推荐最佳成像模态和感兴趣区域，结合预训练的分割网络迭代优化分割结果。
- Result: 在1325个多参数MRI图像数据集上验证，方法显著提高了分割准确性和标注效率，尤其在复杂病理情况下表现优异。
- Conclusion: 该方法不仅超越了标准分割网络，还独立开发出可能与现有放射科指南不同的策略，展示了与人类医生互动的潜力。


### [22] [Scaling Up Audio-Synchronized Visual Animation: An Efficient Training Paradigm](https://arxiv.org/abs/2508.03955)
*Lin Zhang,Zefan Cai,Yufan Zhou,Shentong Mo,Jinhong Lin,Cheng-En Wu,Yibing Wei,Yijing Zhang,Ruiyi Zhang,Wen Xiao,Tong Sun,Junjie Hu,Pedro Morgado*

Main category: cs.CV

TL;DR: 提出了一种两阶段训练范式，利用大量噪声视频扩展音频同步视觉动画，减少对高质量人工标注数据的依赖。

- Motivation: 现有方法依赖昂贵的高质量人工标注视频，难以扩展到开放世界中的多样化音频-视频类别。
- Method: 采用两阶段训练：第一阶段自动筛选大规模视频进行预训练，学习不完美的对齐；第二阶段在小规模高质量数据上微调。通过多特征条件和窗口注意力增强同步性。
- Result: 方法显著减少人工标注需求（超过10倍），并在48类视频基准测试中表现优异。
- Conclusion: 提出的方法高效且可扩展，适用于多样化音频-视频类别。


### [23] [RAVID: Retrieval-Augmented Visual Detection: A Knowledge-Driven Approach for AI-Generated Image Identification](https://arxiv.org/abs/2508.03967)
*Mamadou Keita,Wassim Hamidouche,Hessen Bougueffa Eutamene,Abdelmalik Taleb-Ahmed,Abdenour Hadid*

Main category: cs.CV

TL;DR: RAVID是一个基于视觉检索增强生成（RAG）的AI生成图像检测框架，通过动态检索相关图像提升检测性能，结合CLIP和视觉语言模型（VLM），在通用性和鲁棒性上表现优异。

- Motivation: 现有检测方法依赖低级特征和模型特定信息，通用性和鲁棒性不足，而RAG方法在视觉领域尚未充分探索。
- Method: RAVID结合微调的CLIP图像编码器（RAVID CLIP）和VLM，动态检索相关图像并融合查询图像，生成增强输入。
- Result: 在UniversalFakeDetect基准测试中，RAVID平均准确率达93.85%，在图像退化条件下仍保持80.27%的准确率，显著优于现有方法。
- Conclusion: RAVID通过RAG和VLM的结合，显著提升了AI生成图像检测的性能和鲁棒性，为视觉领域的RAG应用提供了新思路。


### [24] [Investigating the Impact of Large-Scale Pre-training on Nutritional Content Estimation from 2D Images](https://arxiv.org/abs/2508.03996)
*Michele Andrade,Guilherme A. L. Silva,Valéria Santos,Gladston Moreira,Eduardo Luz*

Main category: cs.CV

TL;DR: 研究探讨了大尺度预训练数据集对仅用2D图像进行营养估计的深度学习模型性能的影响，发现JFT-300M预训练模型显著优于公开数据集预训练模型。

- Motivation: 营养估计对健康监测至关重要，但仅依赖2D图像存在挑战，且现有方法依赖专有数据集，影响可复现性。
- Method: 使用ImageNet和COYO预训练的ViT模型，与CNN基线模型及JFT-300M预训练的SOTA方法对比，在Nutrition5k数据集上评估。
- Result: JFT-300M预训练模型表现最佳，COYO预训练模型表现意外差于ImageNet预训练模型。
- Conclusion: 预训练数据集的规模、领域相关性和质量对2D营养估计的迁移学习效果至关重要。


### [25] [JanusNet: Hierarchical Slice-Block Shuffle and Displacement for Semi-Supervised 3D Multi-Organ Segmentation](https://arxiv.org/abs/2508.03997)
*Zheng Zhang,Tianzhuzi Tan,Guanchun Yin,Bo Zhang,Xiuzhuang Zhou*

Main category: cs.CV

TL;DR: JanusNet是一种用于3D医学图像分割的数据增强框架，通过全局建模解剖连续性和局部关注难分割区域，显著提升了性能。

- Motivation: 解决现有数据增强方法在3D医学图像中破坏解剖连续性和忽略难分割区域的问题。
- Method: 提出双阶段框架：Slice-Block Shuffle保持解剖连续性，Confidence-Guided Displacement增强难分割区域信号。
- Result: 在Synapse和AMOS数据集上表现优异，例如在仅20%标注数据下，Synapse数据集的DSC提升4%。
- Conclusion: JanusNet通过结合全局和局部策略，显著提升了弱监督医学图像分割的效果。


### [26] [CAD-Judge: Toward Efficient Morphological Grading and Verification for Text-to-CAD Generation](https://arxiv.org/abs/2508.04002)
*Zheyuan Zhou,Jiayi Han,Liang Du,Naiyu Fang,Lemiao Qiu,Shuyou Zhang*

Main category: cs.CV

TL;DR: 提出了一种名为CAD-Judge的新系统，用于高效评估和验证文本到CAD模型的生成，结合了Compiler-as-a-Judge模块和Compiler-as-a-Review模块，显著提升了性能和效率。

- Motivation: 传统CAD模型生成和验证过程复杂且耗时，且现有方法存在奖励机制滥用问题，需要一种更高效、可靠的解决方案。
- Method: 采用Compiler-as-a-Judge模块作为快速奖励信号，结合前景理论优化模型对齐；引入Compiler-as-a-Review模块验证生成模型，并通过代理生成方法提升鲁棒性。
- Result: 在多个CAD数据集上的实验表明，该方法在性能和效率上均达到最优水平。
- Conclusion: CAD-Judge系统为文本到CAD模型的生成和验证提供了一种高效、可靠的解决方案，具有实际应用潜力。


### [27] [$\text{S}^2$Q-VDiT: Accurate Quantized Video Diffusion Transformer with Salient Data and Sparse Token Distillation](https://arxiv.org/abs/2508.04016)
*Weilun Feng,Haotong Qin,Chuanguang Yang,Xiangqi Li,Han Yang,Yuqi Li,Zhulin An,Libo Huang,Michele Magno,Yongjun Xu*

Main category: cs.CV

TL;DR: 提出了一种名为S²Q-VDiT的后训练量化框架，用于视频扩散模型，通过显著数据选择和稀疏令牌蒸馏解决高校准方差和学习挑战。

- Motivation: 视频扩散模型因联合建模时空信息导致长令牌序列，引发高校准方差和学习难题，需高效量化解决方案。
- Method: 采用Hessian-aware显著数据选择构建高质量校准数据集，并利用注意力引导的稀疏令牌蒸馏优化令牌分布。
- Result: 在W4A6量化下实现无损性能，模型压缩3.9倍，推理加速1.3倍。
- Conclusion: S²Q-VDiT有效解决了视频扩散模型量化中的挑战，显著提升了效率。


### [28] [Can Large Multimodal Models Actively Recognize Faulty Inputs? A Systematic Evaluation Framework of Their Input Scrutiny Ability](https://arxiv.org/abs/2508.04017)
*Haiqi Yang,Jinzhe Li,Gengxu Li,Yi Chang,Yuan Wu*

Main category: cs.CV

TL;DR: 论文提出了一个输入审查能力评估框架（ISEval），用于评估大型多模态模型（LMMs）主动检测和审查错误输入的能力，发现多数模型依赖显式提示，且在不同错误类型上表现不一。

- Motivation: 大型多模态模型（LMMs）在处理多模态任务时表现出色，但缺乏对错误输入的主动检测能力，这一问题尚未被充分研究。
- Method: 作者设计了ISEval框架，包含七类错误前提和三个评估指标，对十种先进LMMs进行了广泛评估。
- Result: 多数模型无法主动检测错误前提，依赖显式提示；在逻辑错误上表现较好，但在语言错误和条件错误上表现较差；不同模型对模态的信任度不同。
- Conclusion: 研究强调了增强LMMs主动验证输入有效性的迫切需求，并提供了缓解这一问题的新见解。


### [29] [Prototype-Driven Structure Synergy Network for Remote Sensing Images Segmentation](https://arxiv.org/abs/2508.04022)
*Junyi Wang,Jinjiang Li,Guodong Fan,Yakun Ju,Xiang Fang,Alex C. Kot*

Main category: cs.CV

TL;DR: 论文提出了一种原型驱动的结构协同网络（PDSSNet），用于解决遥感图像语义分割中的高类内方差和高类间相似性问题，通过三个关键模块实现语义和结构的协同优化。

- Motivation: 遥感图像语义分割中，完整地获取地物对象对精确分析至关重要，但高类内方差和高类间相似性严重阻碍了任务。传统方法无法有效统一类表示和区分相似特征，导致分割结果不完整。
- Method: 设计了三个模块：1) 自适应原型提取模块（APEM）确保语义准确性；2) 语义-结构协调模块（SSCM）遵循语义优先、结构其次的原则；3) 通道相似性调整模块（CSAM）动态调整步长以聚焦类间区分性特征。
- Result: 实验表明，PDSSNet优于现有方法。
- Conclusion: PDSSNet通过协同优化语义和结构信息，显著提升了遥感图像语义分割的完整性和准确性。


### [30] [Dual Prompt Learning for Adapting Vision-Language Models to Downstream Image-Text Retrieval](https://arxiv.org/abs/2508.04028)
*Yifan Wang,Tao Wang,Chenwei Tang,Caiyang Yu,Zhengqing Zang,Mengmi Zhang,Shudong Huang,Jiancheng Lv*

Main category: cs.CV

TL;DR: 论文提出了一种双提示学习框架DCAR，用于改进图像-文本检索任务，通过动态调整语义和视觉维度的提示向量，优化属性和类别特征。

- Motivation: 现有提示学习方法在图像分类任务中表现良好，但在图像-文本检索任务中面临挑战，尤其是区分细粒度属性和相似子类别。
- Method: DCAR框架通过动态更新属性描述权重和引入多视角负样本，联合优化属性和类别特征。
- Result: 在构建的FDRD数据集上，DCAR实现了最先进的性能。
- Conclusion: DCAR通过双提示学习和联合优化策略，显著提升了图像-文本检索任务的性能。


### [31] [Radar-Based NLoS Pedestrian Localization for Darting-Out Scenarios Near Parked Vehicles with Camera-Assisted Point Cloud Interpretation](https://arxiv.org/abs/2508.04033)
*Hee-Yeun Kim,Byeonggyu Park,Byonghyok Choi,Hansang Cho,Byungkwan Kim,Soomok Lee,Mingu Jeon,Seung-Woo Seo,Seong-Woo Kim*

Main category: cs.CV

TL;DR: 提出了一种结合单目摄像头和2D雷达点云数据的NLoS行人定位框架，用于解决路边停车导致的盲区问题，提升道路安全。

- Motivation: 路边停车导致的NLoS盲区对道路安全构成威胁，现有方法依赖预定义空间信息或简单反射假设，难以应对动态场景。
- Method: 通过图像分割检测停车车辆，结合深度估计和2D雷达点云数据，实现精确空间推理。
- Result: 实验表明，该方法能更早检测行人，提升道路安全。
- Conclusion: 提出的框架有效解决了动态NLoS场景中的行人定位问题，具有实际应用潜力。


### [32] [CORE-ReID V2: Advancing the Domain Adaptation for Object Re-Identification with Optimized Training and Ensemble Fusion](https://arxiv.org/abs/2508.04036)
*Trinh Quoc Nguyen,Oky Dicky Ardiansyah Prima,Syahid Al Irfan,Hindriyanto Dwi Purnomo,Radius Tanone*

Main category: cs.CV

TL;DR: CORE-ReID V2是CORE-ReID的增强版，通过CycleGAN合成数据和先进的融合机制（ECAB和SECAB）解决无监督域适应（UDA）问题，在Person/Vehicle ReID任务中表现优异。

- Motivation: 解决无监督域适应（UDA）在Person/Vehicle/Object ReID中的挑战，提升跨域特征表示和伪标签准确性。
- Method: 使用CycleGAN预训练合成数据，结合ECAB和SECAB的融合机制优化特征表示和伪标签。
- Result: 在UDA Person/Vehicle ReID数据集上取得最高mAP和Rank-k准确率，支持轻量级骨干网络。
- Conclusion: CORE-ReID V2不仅推动了UDA-based Object ReID的发展，还为后续研究提供了坚实基础。


### [33] [SPJFNet: Self-Mining Prior-Guided Joint Frequency Enhancement for Ultra-Efficient Dark Image Restoration](https://arxiv.org/abs/2508.04041)
*Tongshun Zhang,Pingling Liu,Zijian Zhang,Qiuzhan Zhou*

Main category: cs.CV

TL;DR: 论文提出SPJFNet，通过自挖掘轻量内源指导和双频处理框架，解决了暗图像恢复中的效率瓶颈问题。

- Motivation: 现有暗图像恢复方法依赖外部先验、多阶段冗余操作和全局频率处理，导致效率低下。
- Method: 引入自挖掘指导模块（SMGM）生成内源指导，利用无损小波分解和联合傅里叶增强优化频率处理，提出双频指导框架（DFGF）。
- Result: SPJFNet在多个基准测试中超越现有方法，显著降低计算复杂度和参数量。
- Conclusion: SPJFNet通过高效的自挖掘指导和双频处理，实现了性能和效率的双重提升。


### [34] [VisualTrans: A Benchmark for Real-World Visual Transformation Reasoning](https://arxiv.org/abs/2508.04043)
*Yuheng Ji,Yipu Wang,Yuyang Liu,Xiaoshuai Hao,Yue Liu,Yuting Zhao,Huaihai Lyu,Xiaolong Zheng*

Main category: cs.CV

TL;DR: 论文介绍了VisualTrans，首个针对真实世界人机交互场景的视觉变换推理（VTR）综合基准，解决了现有基准的局限性，并评估了多种推理维度。

- Motivation: 现有基准存在模拟与现实的差距、任务复杂度有限和推理覆盖不完整的问题，限制了其在实际场景中的应用。
- Method: 通过构建包含12种语义多样任务的数据集，结合空间、过程和定量三个推理维度，利用第一人称操作视频构建数据，并通过人工验证确保质量。
- Result: 评估显示现有视觉语言模型在静态空间任务表现良好，但在动态多步推理场景中存在明显不足。
- Conclusion: 研究揭示了现有模型在时间建模和因果推理上的弱点，为未来开发更强大和通用的VTR系统提供了方向。


### [35] [Iterative pseudo-labeling based adaptive copy-paste supervision for semi-supervised tumor segmentation](https://arxiv.org/abs/2508.04044)
*Qiangguo Jin,Hui Cui,Junbo Wang,Changming Sun,Yimiao He,Ping Xuan,Linlin Wang,Cong Cong,Leyi Wei,Ran Su*

Main category: cs.CV

TL;DR: 论文提出了一种名为IPA-CP的半监督学习方法，用于CT扫描中的肿瘤分割，通过自适应增强和迭代伪标签策略提升性能。

- Motivation: 现有半监督学习方法主要关注大器官分割，忽视了多肿瘤或小体积肿瘤的挑战性场景，且数据增强策略的潜力未充分挖掘。
- Method: IPA-CP结合了双向不确定性自适应增强机制和迭代伪标签过渡策略，以生成更鲁棒的伪标签。
- Result: 实验表明，IPA-CP在多个数据集上优于现有半监督学习方法，消融研究验证了其技术贡献的有效性。
- Conclusion: IPA-CP为医学图像分割中的挑战性场景提供了一种简单而有效的解决方案。


### [36] [Motion is the Choreographer: Learning Latent Pose Dynamics for Seamless Sign Language Generation](https://arxiv.org/abs/2508.04049)
*Jiayi He,Xu Wang,Shengeng Tang,Yaxiong Wang,Lechao Cheng,Dan Guo*

Main category: cs.CV

TL;DR: 提出了一种新的手语视频生成范式，通过两阶段合成框架将运动语义与手语者身份解耦，解决了数据需求高和泛化能力差的问题。

- Motivation: 手语视频生成面临数据需求高和泛化能力差的挑战，需要一种既能精确控制语义又能生成自然动作的方法。
- Method: 采用两阶段框架：1）构建手语者无关的多模态运动词典；2）通过离散到连续的运动合成和身份感知神经渲染生成视频。
- Result: 实验表明，解耦运动与身份不仅可行，还能实现高质量合成和前所未有的手语者个性化灵活性。
- Conclusion: 该方法为手语视频生成提供了高效且灵活的解决方案，显著提升了生成质量和泛化能力。


### [37] [DOMR: Establishing Cross-View Segmentation via Dense Object Matching](https://arxiv.org/abs/2508.04050)
*Jitong Liao,Yulu Gao,Shaofei Huang,Jialin Gao,Jie Lei,Ronghua Liang,Si Liu*

Main category: cs.CV

TL;DR: 提出DOMR框架，通过密集对象匹配和细化实现跨视角对象对应，性能优于现有方法。

- Motivation: 跨视角（第一人称和第三人称）对象对应是视觉理解中的关键但具有挑战性的任务。
- Method: 提出DOMR框架，包含密集对象匹配模块（DOM）和掩码细化头，联合建模对象的位置和语义关系。
- Result: 在Ego-Exo4D基准测试中，平均IoU为49.7%（Ego→Exo）和55.2%（Exo→Ego），分别比之前方法提升5.8%和4.3%。
- Conclusion: DOMR框架通过整合视觉、空间和语义线索，显著提升了跨视角对象对应的性能。


### [38] [Towards Globally Predictable k-Space Interpolation: A White-box Transformer Approach](https://arxiv.org/abs/2508.04051)
*Chen Luo,Qiyu Jin,Taofeng Xie,Xuemei Wang,Huayu Wang,Congcong Liu,Liming Tang,Guoqing Chen,Zhuo-Xu Cui,Dong Liang*

Main category: cs.CV

TL;DR: 提出了一种基于全局可预测插值（GPI）的白盒Transformer框架GPI-WT，用于k空间数据插值，显著提高了插值精度和可解释性。

- Motivation: 现有方法（如卷积神经网络）主要利用局部可预测性，忽略了k空间的全局依赖性。Transformer因其捕捉长距离依赖的能力而受到启发，但其缺乏可解释性。
- Method: 从湮灭角度提出GPI作为k空间结构低秩（SLR）模型，将全局湮灭滤波器作为可学习参数，并通过展开SLR的优化算法构建白盒Transformer。
- Result: 实验表明，GPI-WT在k空间插值精度上显著优于现有方法，同时提供更好的可解释性。
- Conclusion: GPI-WT是一种高效且可解释的k空间插值方法，为加速MRI提供了新思路。


### [39] [Uni-DocDiff: A Unified Document Restoration Model Based on Diffusion](https://arxiv.org/abs/2508.04055)
*Fangmin Zhao,Weichao Zeng,Zhenhang Li,Dongbao Yang,Binbin Li,Xiaojun Bi,Yu Zhou*

Main category: cs.CV

TL;DR: Uni-DocDiff是一个基于扩散模型的统一文档修复模型，通过可学习的任务提示和创新的Prior Pool机制，实现了多任务的高效协同与扩展性。

- Motivation: 解决现有文档修复方法独立建模导致系统复杂、扩展性差的问题，以及多任务协同不足的缺陷。
- Method: 提出Uni-DocDiff，采用可学习的任务提示设计，结合Prior Pool（局部高频与全局低频特征）和Prior Fusion Module（自适应选择任务相关先验信息）。
- Result: 实验表明，Uni-DocDiff性能与任务专用模型相当或更优，同时具备扩展性，可适应新任务。
- Conclusion: Uni-DocDiff为文档修复提供了一种高效、可扩展的统一解决方案。


### [40] [TCSAFormer: Efficient Vision Transformer with Token Compression and Sparse Attention for Medical Image Segmentation](https://arxiv.org/abs/2508.04058)
*Zunhui Xia,Hongxing Li,Libin Lan*

Main category: cs.CV

TL;DR: TCSAFormer是一种高效的医学图像分割网络，通过压缩注意力模块和双分支前馈网络模块解决了Transformer方法的高计算复杂性和局部上下文信息捕捉不足的问题。

- Motivation: Transformer方法在医学图像分割中表现出色，但存在计算复杂度高和局部上下文信息捕捉不足的局限性。
- Method: TCSAFormer采用压缩注意力模块（CA）减少计算复杂度，并引入双分支前馈网络模块（DBFFN）增强局部特征和多尺度信息捕捉。
- Result: 在ISIC-2018、CVC-ClinicDB和Synapse数据集上的实验表明，TCSAFormer性能优于现有方法，且计算开销更低。
- Conclusion: TCSAFormer在效率和准确性之间实现了最佳平衡，为医学图像分割提供了高效解决方案。


### [41] [Beyond the Visible: Benchmarking Occlusion Perception in Multimodal Large Language Models](https://arxiv.org/abs/2508.04059)
*Zhaochen Liu,Kaiwen Gao,Shuyi Liang,Bin Xiao,Limeng Qiao,Lin Ma,Tingting Jiang*

Main category: cs.CV

TL;DR: 论文介绍了O-Bench，首个专为遮挡感知设计的VQA基准，评估了22种MLLMs，发现其与人类表现存在显著差距，并识别了三种典型失败模式。

- Motivation: 遮挡感知是空间理解的关键，但现有MLLMs在此任务上的表现尚未充分探索，因此需要专门的评测工具。
- Method: 基于SA-1B构建1,365张语义连贯的遮挡图像，标注4,588个QA对，评估22种MLLMs。
- Result: MLLMs与人类表现差距显著，模型规模或思维过程无法完全弥补，识别出三种失败模式。
- Conclusion: O-Bench为遮挡感知提供了重要评测工具，并启发MLLMs在视觉智能上的发展。


### [42] [TNet: Terrace Convolutional Decoder Network for Remote Sensing Image Semantic Segmentation](https://arxiv.org/abs/2508.04061)
*Chengqian Dai,Yonghong Guo,Hongzhao Xiang,Yigui Luo*

Main category: cs.CV

TL;DR: 论文提出了一种名为TNet的新型解码器网络，通过卷积和加法操作逐步融合多分辨率特征，解决了现有方法忽略全局上下文依赖的问题。

- Motivation: 现有遥感图像分割网络（如UNet）通常关注局部特征交互，而忽略了多分辨率间的全局上下文依赖。
- Method: 提出TNet，利用卷积和加法操作逐步将低分辨率特征（全局上下文）融合到高分辨率特征（局部细节）中。
- Result: 在ISPRS Vaihingen（85.35% mIoU）、ISPRS Potsdam（87.05% mIoU）和LoveDA（52.19% mIoU）数据集上表现优异。
- Conclusion: TNet通过简单有效的设计，实现了全局与局部特征的融合，同时保持了高效计算。


### [43] [Bridging Diffusion Models and 3D Representations: A 3D Consistent Super-Resolution Framework](https://arxiv.org/abs/2508.04090)
*Yi-Ting Chen,Ting-Hsuan Liao,Pengsheng Guo,Alexander Schwing,Jia-Bin Huang*

Main category: cs.CV

TL;DR: 3DSR是一种基于3D高斯溅射的超分辨率框架，利用现有2D扩散超分辨率模型，确保3D一致性，无需额外微调即可提升视觉质量。

- Motivation: 现有方法（如图像上采样或视频超分辨率）未充分考虑3D一致性或仅隐式处理，3DSR旨在通过显式3D表示解决这一问题。
- Method: 结合3D高斯溅射场景表示和2D扩散超分辨率模型，确保多视角3D一致性。
- Result: 在MipNeRF360和LLFF数据上验证，3DSR生成高分辨率结果，视觉效果好且保持3D结构一致性。
- Conclusion: 3DSR通过显式3D表示提升超分辨率效果，代码将开源。


### [44] [DET-GS: Depth- and Edge-Aware Regularization for High-Fidelity 3D Gaussian Splatting](https://arxiv.org/abs/2508.04099)
*Zexu Huang,Min Xu,Stuart Perry*

Main category: cs.CV

TL;DR: DET-GS提出了一种统一的深度和边缘感知正则化框架，用于3D高斯溅射，显著提高了稀疏视图条件下的几何重建精度和视觉保真度。

- Motivation: 在稀疏视图条件下，现有方法难以准确重建几何结构，且传统平滑方法会破坏语义边界和纹理细节。
- Method: DET-GS结合了分层几何深度监督框架、边缘感知深度正则化和RGB引导的边缘保留总变差损失。
- Result: 实验表明，DET-GS在几何精度和视觉保真度上均优于现有方法。
- Conclusion: DET-GS为稀疏视图下的高质量重建提供了一种有效解决方案。


### [45] [NEARL-CLIP: Interacted Query Adaptation with Orthogonal Regularization for Medical Vision-Language Understanding](https://arxiv.org/abs/2508.04101)
*Zelin Peng,Yichen Zhao,Yu Huang,Piao Yang,Feilong Tang,Zhengqin Xu,Xiaokang Yang,Wei Shen*

Main category: cs.CV

TL;DR: NEARL-CLIP提出了一种基于跨模态交互的VLM框架，通过动态生成跨模态查询和正交技术，提升医学图像分析的性能。

- Motivation: 医学图像分析缺乏标注数据，现有VLM直接应用于医学领域存在域差距问题。
- Method: 提出NEARL-CLIP框架，包含USEformer和OCA模块，分别用于跨模态交互和正交知识解耦。
- Result: NEARL-CLIP以高效参数（1.46M）实现了性能提升。
- Conclusion: NEARL-CLIP通过跨模态交互和正交技术，有效释放了VLM在医学领域的潜力。


### [46] [AR as an Evaluation Playground: Bridging Metrics and Visual Perception of Computer Vision Models](https://arxiv.org/abs/2508.04102)
*Ashkan Ganj,Yiqin Zhao,Tian Guo*

Main category: cs.CV

TL;DR: ARCADE是一个基于增强现实（AR）的平台，旨在简化计算机视觉（CV）研究中的人类感知实验。

- Motivation: 人类感知研究对理解CV模型性能至关重要，但传统方法复杂且难以扩展。AR为简化这一过程提供了机会。
- Method: 设计ARCADE平台，支持跨平台AR数据收集、可插拔模型推理和AR流媒体，用于人类感知实验。
- Result: ARCADE成功用于深度和光照估计模型的感知评估，并展示了其灵活性和有效性。
- Conclusion: ARCADE是一个高效、灵活的人类中心评估平台，适用于CV研究。


### [47] [Unlocking the Potential of MLLMs in Referring Expression Segmentation via a Light-weight Mask Decode](https://arxiv.org/abs/2508.04107)
*Jingchao Wang,Zhijian Wu,Dingjiang Huang,Yefeng Zheng,Hong Wang*

Main category: cs.CV

TL;DR: MLLMSeg是一个新框架，利用MLLM视觉编码器的视觉细节特征，无需额外编码器，通过DSFF模块融合细节与语义特征，轻量级掩码解码器实现高效掩码预测。

- Motivation: 解决现有RES方法在性能与成本之间的权衡问题，避免使用参数繁重的SAM或牺牲准确性的轻量级方法。
- Method: 提出MLLMSeg框架，利用MLLM视觉编码器的细节特征，设计DSFF模块融合细节与语义特征，采用轻量级掩码解码器（34M参数）。
- Result: 实验表明MLLMSeg在性能与成本上优于SAM-based和SAM-free方法。
- Conclusion: MLLMSeg在RES任务中实现了性能与成本的良好平衡，优于现有方法。


### [48] [CLIPVehicle: A Unified Framework for Vision-based Vehicle Search](https://arxiv.org/abs/2508.04120)
*Likai Wang,Ruize Han,Xiangqun Zhang,Wei Feng*

Main category: cs.CV

TL;DR: 论文提出了一种名为CLIPVehicle的统一框架，用于联合车辆检测和重识别，解决了传统方法资源消耗大且不实用的问题。

- Motivation: 现有方法需要先检测并存储所有车辆图像，再应用重识别模型，资源密集且不实用。研究旨在实现联合检测和重识别。
- Method: 提出了CLIPVehicle框架，包含双粒度语义区域对齐模块和多层次车辆识别学习策略，利用视觉语言模型进行车辆区分建模。
- Result: 实验结果表明，该方法在车辆重识别和人物搜索任务中优于现有最先进方法。
- Conclusion: CLIPVehicle框架有效解决了联合检测和重识别的挑战，并在新构建的基准数据集上表现出色。


### [49] [Conditional Latent Diffusion Models for Zero-Shot Instance Segmentation](https://arxiv.org/abs/2508.04122)
*Maximilian Ulmer,Wout Boerdijk,Rudolph Triebel,Maximilian Durner*

Main category: cs.CV

TL;DR: OC-DiT是一种新型扩散模型，用于对象中心预测和零样本实例分割，通过条件潜在扩散框架生成实例掩码，并在多个真实世界基准测试中达到最先进性能。

- Motivation: 解决零样本实例分割的挑战，通过扩散模型有效分离对象实例。
- Method: 提出条件潜在扩散框架，结合对象模板和图像特征生成实例掩码；包括粗模型生成初始提案和细化模型并行优化提案。
- Result: 在多个真实世界基准测试中达到最先进性能，无需目标数据重新训练。
- Conclusion: 扩散模型在实例分割任务中具有潜力，OC-DiT展示了其有效性。


### [50] [Excavate the potential of Single-Scale Features: A Decomposition Network for Water-Related Optical Image Enhancement](https://arxiv.org/abs/2508.04123)
*Zheng Cheng,Wenri Wang,Guangyong Chen,Yakun Ju,Yihua Cheng,Zhisong Liu,Yanda Meng,Jintao Song*

Main category: cs.CV

TL;DR: 论文提出了一种单尺度分解网络（SSD-Net），用于水下图像增强，通过不对称分解机制分离图像为清洁层和退化层，结合CNN和Transformer的优势，性能优于多尺度方法。

- Motivation: 当前主流方法依赖多尺度特征融合，但实验表明单尺度特征提取同样有效且复杂度更低，因此探索单尺度特征在水下图像增强中的潜力。
- Method: 提出SSD-Net，采用不对称分解机制，结合CNN和Transformer，通过PFDB和BFCB模块实现特征分解与融合。
- Result: 单尺度方法性能匹配或超越多尺度方法，显著降低复杂度。
- Conclusion: SSD-Net通过单尺度特征分解和动态跨层信息交互，有效提升水下图像增强性能。


### [51] [Learning Using Privileged Information for Litter Detection](https://arxiv.org/abs/2508.04124)
*Matthias Bartolo,Konstantinos Makantasis,Dylan Seychell*

Main category: cs.CV

TL;DR: 提出了一种结合特权信息与深度学习目标检测的新方法，提升垃圾检测效果，同时保持模型效率。

- Motivation: 全球垃圾污染日益严重，开发高效的自动化垃圾检测工具具有重要挑战。
- Method: 首次结合特权信息与深度学习目标检测，并引入边界框信息编码为二进制掩码的方法。
- Result: 在多个数据集上验证了性能提升，且无需增加模型复杂度。
- Conclusion: 该方法在垃圾检测中实现了准确性与效率的平衡，具有实际应用潜力。


### [52] [SVC 2025: the First Multimodal Deception Detection Challenge](https://arxiv.org/abs/2508.04129)
*Xun Lin,Xiaobao Guo,Taorui Wang,Yingjie Ma,Jiajian Huang,Jiayu Zhang,Junzhe Cao,Zitong Yu*

Main category: cs.CV

TL;DR: 该论文介绍了SVC 2025多模态欺骗检测挑战赛，旨在评估跨域泛化能力，推动更适应性强、可解释性高的欺骗检测系统发展。

- Motivation: 现有研究多关注单域场景，忽略了域转移导致的性能下降问题，需开发能跨域泛化的模型。
- Method: 通过多模态数据（音频、视频、文本）设计模型，捕捉隐晦欺骗线索，并评估跨域性能。
- Result: 21支团队提交了最终结果，挑战赛促进了跨域欺骗检测模型的开发。
- Conclusion: 该挑战赛为多模态学习领域提供了新基准，推动了更实用的欺骗检测系统发展。


### [53] [DS$^2$Net: Detail-Semantic Deep Supervision Network for Medical Image Segmentation](https://arxiv.org/abs/2508.04131)
*Zhaohong Huang,Yuxin Zhang,Mingbao Lin,Taojian Zhou,Guorong Cai,Rongrong Ji*

Main category: cs.CV

TL;DR: DS²Net提出了一种多视角深度监督网络，结合细节和语义特征监督，通过DEM和SEM模块增强特征监督，并采用基于不确定性的损失函数，显著优于现有方法。

- Motivation: 现有方法仅单独监督粗粒度语义或细粒度细节特征，忽视了二者在医学图像分析中的关键关系。
- Method: 提出DS²Net，结合DEM和SEM模块分别监督低层细节和高层语义特征，并引入基于不确定性的监督损失。
- Result: 在六个基准测试中，DS²Net表现优于现有方法。
- Conclusion: DS²Net通过多视角深度监督和自适应损失设计，显著提升了医学图像分割性能。


### [54] [UniFGVC: Universal Training-Free Few-Shot Fine-Grained Vision Classification via Attribute-Aware Multimodal Retrieval](https://arxiv.org/abs/2508.04136)
*Hongyu Guo,Kuan Zhu,Xiangzhao Hao,Haiyun Guo,Ming Tang,Jinqiao Wang*

Main category: cs.CV

TL;DR: UniFGVC是一个无需训练的多模态检索框架，通过生成结构化文本描述和联合空间检索，显著提升少样本细粒度视觉分类性能。

- Motivation: 解决少样本细粒度视觉分类中预训练视觉语言模型的过拟合和泛化能力弱的问题。
- Method: 提出Category-Discriminative Visual Captioner生成结构化文本描述，结合多模态检索完成分类。
- Result: 在12个基准测试中表现优于现有少样本CLIP方法和部分全监督MLLM方法。
- Conclusion: UniFGVC具有广泛兼容性和泛化能力，适用于少样本细粒度视觉分类任务。


### [55] [IDCNet: Guided Video Diffusion for Metric-Consistent RGBD Scene Generation with Precise Camera Control](https://arxiv.org/abs/2508.04147)
*Lijuan Liu,Wenfa Li,Dongbo Zhang,Shuo Wang,Shaohui Jiao*

Main category: cs.CV

TL;DR: IDC-Net是一个联合生成RGB图像和深度图的统一几何感知扩散模型，通过相机轨迹控制生成RGB-D视频序列。

- Motivation: 现有方法通常将RGB和深度生成分开处理，缺乏几何一致性，IDC-Net旨在解决这一问题。
- Method: 采用联合学习框架，构建相机-图像-深度一致的数据集，并引入几何感知Transformer块实现精细相机控制。
- Result: 实验表明IDC-Net在视觉质量和几何一致性上优于现有方法，生成的RGB-D序列可直接用于3D场景重建。
- Conclusion: IDC-Net通过联合学习框架实现了高质量的RGB-D序列生成，具有实际应用价值。


### [56] [ICM-Fusion: In-Context Meta-Optimized LoRA Fusion for Multi-Task Adaptation](https://arxiv.org/abs/2508.04153)
*Yihua Shao,Xiaofeng Lin,Xinwei Long,Siyu Chen,Minxi Yan,Yang Liu,Ziyang Yan,Ao Ma,Hao Tang,Jingcai Guo*

Main category: cs.CV

TL;DR: ICM-Fusion框架结合元学习和上下文适应，通过任务向量动态平衡优化方向，解决多任务LoRA模型融合中的冲突和遗忘问题。

- Motivation: 现有LoRA融合方法在多任务适应中存在权重冲突和领域遗忘问题，增量学习在少样本场景下泛化能力不足。
- Method: 提出ICM-Fusion框架，利用任务向量算术和F-VAE重建，动态优化任务向量方向并生成多任务LoRA。
- Result: 实验表明ICM-Fusion能显著减少多任务损失，并在少样本场景中实现任务增强。
- Conclusion: ICM-Fusion是一种高效的多任务LoRA融合方法，适用于多种模型和任务。


### [57] [Audio-Assisted Face Video Restoration with Temporal and Identity Complementary Learning](https://arxiv.org/abs/2508.04161)
*Yuqin Cao,Yixuan Gao,Wei Sun,Xiaohong Liu,Yulun Zhang,Xiongkuo Min*

Main category: cs.CV

TL;DR: 论文提出了一种通用音频辅助人脸视频修复网络（GAVN），通过身份和时间互补学习解决多种视频失真问题。

- Motivation: 现有方法忽视了视觉和音频特征的内在关联，尤其是嘴部区域，且仅针对压缩伪影去除。
- Method: GAVN分阶段提取时间特征和身份特征，结合音频信号和面部标志点，最终通过重建模块生成高质量视频。
- Result: 实验表明，GAVN在压缩伪影去除、去模糊和超分辨率任务上优于现有方法。
- Conclusion: GAVN是一种高效的多功能视频修复方法，代码将公开。


### [58] [ToxicTAGS: Decoding Toxic Memes with Rich Tag Annotations](https://arxiv.org/abs/2508.04166)
*Subhankar Swain,Naquee Rizwan,Nayandeep Deb,Vishwajeet Singh Solanki,Vishwa Gangadhar S,Animesh Mukherjee*

Main category: cs.CV

TL;DR: 该论文介绍了首个包含6300个真实世界模因帖子的数据集，用于毒性分类和细粒度标注，并提出标签生成模块以提升检测性能。

- Motivation: 社交媒体模因常传播有害内容，但数据获取和标注成本高阻碍了有效内容审核系统的开发。
- Method: 构建两阶段标注数据集（毒性分类和细粒度标注），并设计标签生成模块以补充缺失的上下文标签。
- Result: 实验表明，引入社会相关标签显著提升了视觉语言模型在检测任务中的性能。
- Conclusion: 该研究为多模态在线环境中的内容审核提供了新颖且可扩展的基础。


### [59] [AD-FM: Multimodal LLMs for Anomaly Detection via Multi-Stage Reasoning and Fine-Grained Reward Optimization](https://arxiv.org/abs/2508.04175)
*Jingyi Liao,Yongyi Su,Rong-Cheng Tu,Zhao Jin,Wenhao Sun,Yiting Li,Dacheng Tao,Xun Xu,Xulei Yang*

Main category: cs.CV

TL;DR: 本文提出了一种改进的多模态大语言模型（MLLM）框架，用于解决专业异常检测中的领域适应问题，通过多阶段推理和细粒度奖励机制显著提升了性能。

- Motivation: 现有基于GRPO的方法在专业异常检测中存在训练数据利用不足和推理过程监督不足的问题，限制了MLLM的应用。
- Method: 引入多阶段审慎推理过程和细粒度奖励机制，结合分类准确性和定位监督，优化模型响应和推理流程。
- Result: 在多个工业数据集上的评估表明，该方法显著提升了模型性能，实现了高效适应和更高准确性。
- Conclusion: 该方法成功弥合了通用MLLM能力与专业异常检测需求之间的差距，适用于细微制造缺陷和结构异常的检测。


### [60] [Uncertainty-Aware Spatial Color Correlation for Low-Light Image Enhancement](https://arxiv.org/abs/2508.04176)
*Jin Kuang,Dong Liu,Yukuang Zhang,Shengsheng Wang*

Main category: cs.CV

TL;DR: U2CLLIE提出了一种结合不确定性感知增强和空间-颜色因果建模的低光图像增强框架，通过双域去噪和层次因果约束显著提升了性能。

- Motivation: 现有方法多关注架构创新，忽略了极端暗光条件下特征表示的不确定性，导致模型可靠性和因果推理能力下降。
- Method: 框架包含不确定性感知双域去噪模块（UaD）和层次因果建模模块（NeCo和AsC），分别优化频域噪声抑制和空间-颜色一致性。
- Result: 在多个基准数据集上达到最先进性能，表现出鲁棒性和强泛化能力。
- Conclusion: U2CLLIE通过不确定性感知和因果建模有效解决了低光图像增强中的关键问题。


### [61] [Deeper Inside Deep ViT](https://arxiv.org/abs/2508.04181)
*Sungrae Hong*

Main category: cs.CV

TL;DR: 研究探讨了ViT-22B模型在本地环境中的训练表现，改进了其不稳定性，并尝试了图像生成任务。

- Motivation: 理解ViT-22B模型的实用性及其在本地环境中的表现。
- Method: 分析ViT-22B的训练行为，改进模型稳定性，并设计图像生成架构。
- Result: ViT-22B在相同参数规模下表现优于ViT，并在图像生成任务中进行了探索。
- Conclusion: ViT-22B在性能和扩展性上具有潜力，但需进一步研究其稳定性。


### [62] [RPCANet++: Deep Interpretable Robust PCA for Sparse Object Segmentation](https://arxiv.org/abs/2508.04190)
*Fengyi Wu,Yimian Dai,Tianfang Zhang,Yixuan Ding,Jian Yang,Ming-Ming Cheng,Zhenming Peng*

Main category: cs.CV

TL;DR: RPCANet++是一个结合RPCA和深度学习的高效稀疏对象分割框架，解决了传统RPCA的计算负担和适应性不足问题。

- Motivation: 传统RPCA模型存在计算负担大、超参数调优依赖性强以及在动态场景中适应性不足的问题，需要一种更高效且灵活的方法。
- Method: RPCANet++通过将松弛的RPCA模型展开为结构化网络（包括BAM、OEM和IRM模块），并引入MAM和DCPM模块以提升背景特征保留和对象提取效率。
- Result: 在多种数据集上的实验表明，RPCANet++在多种成像场景下实现了最先进的性能，并通过可视化和数值测量提升了可解释性。
- Conclusion: RPCANet++结合了RPCA的理论优势和深度网络的效率，为可靠且可解释的稀疏对象分割设定了新的基准。


### [63] [From Learning to Unlearning: Biomedical Security Protection in Multimodal Large Language Models](https://arxiv.org/abs/2508.04192)
*Dunyuan Xu,Xikai Yang,Yaoqian Li,Jinpeng Li,Pheng-Ann Heng*

Main category: cs.CV

TL;DR: 该论文提出了首个生物医学多模态大语言模型遗忘基准（MLLMU-Med），用于评估模型在隐私保护和错误知识移除方面的遗忘效果，并发现现有方法效果有限。

- Motivation: 生物医学多模态大语言模型（MLLMs）的训练数据可能包含隐私信息和错误知识，导致部署后隐私泄露或错误输出。完全重新训练成本高昂，因此需要选择性遗忘有害知识。
- Method: 提出MLLMU-Med基准，通过数据生成流程将合成隐私数据和事实错误整合到训练集中，并设计遗忘效率评分评估性能。
- Result: 评估五种遗忘方法，发现它们在生物医学MLLMs中移除有害知识的效果有限。
- Conclusion: 该研究为生物医学MLLMs的安全研究提供了新方向，表明现有方法仍需改进。


### [64] [Gather and Trace: Rethinking Video TextVQA from an Instance-oriented Perspective](https://arxiv.org/abs/2508.04197)
*Yan Zhang,Gangyan Zeng,Daiqing Wu,Huawen Shen,Binbin Li,Yu Zhou,Can Ma,Xiaojun Bi*

Main category: cs.CV

TL;DR: 论文提出了一种名为GAT的新模型，通过实例导向的方法改进视频文本视觉问答任务，显著提升了准确性和推理速度。

- Motivation: 现有视频文本视觉问答方法存在冗余文本实体和隐式关系建模的问题，限制了准确性和效率。
- Method: 设计了上下文聚合的实例收集模块和实例轨迹追踪模块，分别用于整合文本实体的多模态信息和捕捉文本的动态演变。
- Result: GAT在多个公开数据集上表现优异，准确率提升3.86%，推理速度比现有视频大语言模型快十倍。
- Conclusion: GAT通过实例导向的方法显著提升了视频文本视觉问答的性能，具有高效和泛化能力强的特点。


### [65] [Bootstrap Deep Spectral Clustering with Optimal Transport](https://arxiv.org/abs/2508.04200)
*Wengang Guo,Wei Ye,Chunchun Chen,Xin Sun,Christian Böhm,Claudia Plant,Susanto Rahardja*

Main category: cs.CV

TL;DR: BootSC是一种端到端的深度谱聚类模型，通过联合优化谱聚类的所有阶段，解决了传统方法的优化分离和表示能力有限的问题。

- Motivation: 传统谱聚类方法存在优化过程分离和表示能力有限的问题，限制了其性能。
- Method: 提出BootSC模型，通过单一网络端到端联合学习谱聚类的所有阶段（亲和矩阵构建、谱嵌入和k-means聚类），并利用最优传输监督和正交重参数化技术提升性能。
- Result: 实验表明，BootSC在聚类性能上达到最先进水平，例如在ImageNet-Dogs数据集上比第二名方法提升了16%的NMI。
- Conclusion: BootSC通过端到端联合优化和新技术应用，显著提升了谱聚类的性能。


### [66] [ViFP: A Framework for Visual False Positive Detection to Enhance Reasoning Reliability in VLMs](https://arxiv.org/abs/2508.04201)
*Ben Zhang,LuLu Yu,Lei Gao,Jing Liu,QuanJiang Guo,Hui Gao*

Main category: cs.CV

TL;DR: ViFP是一个提升视觉语言模型推理可靠性的框架，通过检测错误正向推理（FP）来提高答案准确性和推理合理性。它通过构建子问题模板和多轮QA改进推理路径，并引入动态一致性分析和针对性CoT机制减少逻辑错误。实验表明ViFP在多个数据集上显著提升性能。

- Motivation: 现有方法依赖特定多步推理数据集和强化学习策略，训练成本高且泛化能力有限。ViFP旨在解决这些问题，提升视觉推理的可靠性。
- Method: ViFP通过构建子问题模板和多轮QA改进推理路径，动态分析推理路径一致性以识别FP，并引入针对性CoT机制。
- Result: 在A-OKVQA、OKVQA和FVQA数据集上，ViFP显著提升准确率（最高5.4%）并减少FP数量。
- Conclusion: ViFP有效提升视觉语言模型的推理可靠性，同时保持高准确性，为评估模型推理可靠性提供了量化工具VoC。


### [67] [Small Lesions-aware Bidirectional Multimodal Multiscale Fusion Network for Lung Disease Classification](https://arxiv.org/abs/2508.04205)
*Jianxun Yu,Ruiquan Ge,Zhipeng Wang,Cheng Yang,Chenyu Lin,Xianjun Fu,Jikui Liu,Ahmed Elazab,Changmiao Wang*

Main category: cs.CV

TL;DR: 提出了一种多模态多尺度交叉注意力融合网络（MMCAF-Net），用于解决医学影像与电子健康记录数据维度不一致的问题，显著提高了诊断准确性。

- Motivation: 医学疾病诊断中存在小病灶误诊问题，多模态深度学习方法潜力巨大，但数据维度差异导致特征对齐与融合困难。
- Method: MMCAF-Net采用特征金字塔结构和3D多尺度卷积注意力模块提取病灶特征，并通过多尺度交叉注意力模块实现多模态数据融合。
- Result: 在Lung-PET-CT-Dx数据集上评估，诊断准确性显著提升，优于现有方法。
- Conclusion: MMCAF-Net有效解决了多模态数据融合问题，提升了医学诊断的准确性。


### [68] [What Holds Back Open-Vocabulary Segmentation?](https://arxiv.org/abs/2508.04211)
*Josip Šarić,Ivan Martinović,Matej Kristan,Siniša Šegvić*

Main category: cs.CV

TL;DR: 论文提出新方法解决开放词汇模型性能瓶颈问题。

- Motivation: 开放词汇方法在语言-图像预训练中表现停滞，需解决瓶颈问题。
- Method: 利用真实信息设计新组件，识别并解耦性能瓶颈。
- Result: 验证实验揭示了开放词汇模型的失败原因，并提出未来研究方向。
- Conclusion: 新方法为开放词汇模型的性能提升提供了重要见解。


### [69] [SplitGaussian: Reconstructing Dynamic Scenes via Visual Geometry Decomposition](https://arxiv.org/abs/2508.04224)
*Jiahui Li,Shengeng Tang,Jingxuan He,Gang Huang,Zhangye Wang,Yantao Pan,Lechao Cheng*

Main category: cs.CV

TL;DR: SplitGaussian提出了一种新框架，将场景显式分解为静态和动态组件，解决了动态3D场景重建中的运动泄漏和几何失真问题。

- Motivation: 现有基于高斯泼溅的动态场景重建方法将静态和动态元素耦合表示，导致运动泄漏、几何失真和时间闪烁。
- Method: 通过解耦几何和外观建模，SplitGaussian将场景分为静态和动态分支，仅动态分支随时间变形。
- Result: 实验表明，SplitGaussian在渲染质量、几何稳定性和运动分离方面优于现有方法。
- Conclusion: SplitGaussian通过解耦设计提升了时间一致性和重建保真度，同时加速了收敛。


### [70] [Continual Learning for VLMs: A Survey and Taxonomy Beyond Forgetting](https://arxiv.org/abs/2508.04227)
*Yuyang Liu,Qiuhe Hong,Linlan Huang,Alexandra Gomez-Villa,Dipam Goswami,Xialei Liu,Joost van de Weijer,Yonghong Tian*

Main category: cs.CV

TL;DR: 本文综述了视觉语言模型（VLMs）在持续学习（CL）中的挑战与解决方案，提出了分类法并分析了现有评估方法，旨在为终身视觉语言系统研究提供参考。

- Motivation: VLMs在多模态任务中表现出色，但在非平稳数据下的持续学习中面临灾难性遗忘等问题，亟需系统性研究。
- Method: 提出基于三类核心问题的分类法：多模态重放策略、跨模态正则化和参数高效适应，并分析现有评估协议。
- Result: 总结了当前解决方案的优缺点，指出需要更好的基准测试以评估VLM特有的遗忘和组合泛化能力。
- Conclusion: 本文为终身视觉语言系统的研究提供了全面诊断性参考，并提出了未来研究方向。


### [71] [LayerT2V: Interactive Multi-Object Trajectory Layering for Video Generation](https://arxiv.org/abs/2508.04228)
*Kangrui Cen,Baixuan Zhao,Yi Xin,Siqi Luo,Guangtao Zhai,Xiaohong Liu*

Main category: cs.CV

TL;DR: LayerT2V是一种分层生成视频的方法，解决了多物体运动轨迹控制的挑战，显著提升了生成质量。

- Motivation: 现有文本到视频生成模型在多物体运动场景中表现不佳，尤其是物体轨迹交叉时语义冲突严重。
- Method: 通过分层合成背景和前景物体，独立处理每个物体层，避免语义冲突。
- Result: 实验显示，LayerT2V在mIoU和AP50指标上分别比现有方法提升了1.4倍和4.5倍。
- Conclusion: LayerT2V为多物体视频生成提供了更灵活和高效的控制方法。


### [72] [Intention Enhanced Diffusion Model for Multimodal Pedestrian Trajectory Prediction](https://arxiv.org/abs/2508.04229)
*Yu Liu,Zhijie Liu,Xiao Ren,You-Fu Li,He Kong*

Main category: cs.CV

TL;DR: 提出了一种基于扩散模型的多模态行人轨迹预测方法，结合行人运动意图以提高预测的可解释性和准确性。

- Motivation: 行人轨迹预测对自动驾驶路径规划至关重要，但现有扩散模型未充分结合行人运动意图，限制了预测的精确性和可解释性。
- Method: 将行人运动意图分解为横向和纵向分量，引入意图识别模块，并采用高效引导机制生成可解释轨迹。
- Result: 在ETH和UCY基准测试中，与现有最优方法相比表现出竞争力。
- Conclusion: 结合运动意图的扩散模型能有效提升行人轨迹预测的准确性和可解释性。


### [73] [DocVCE: Diffusion-based Visual Counterfactual Explanations for Document Image Classification](https://arxiv.org/abs/2508.04233)
*Saifullah Saifullah,Stefan Agne,Andreas Dengel,Sheraz Ahmed*

Main category: cs.CV

TL;DR: 本文提出了一种名为DocVCE的新方法，利用生成对抗网络和分类器引导生成文档图像的反事实解释，以提升模型决策的透明度和可解释性。

- Motivation: 随着黑盒AI决策系统在文档处理中的广泛应用，提高其透明度和可靠性变得至关重要，尤其是在高风险的场景中。现有的特征重要性图难以解释全局特征，因此需要更直观的解释方法。
- Method: DocVCE结合潜在扩散模型和分类器引导，首先生成合理的视觉反事实解释，然后通过分层块细化找到最接近目标事实图像的细化反事实。
- Result: 在三个文档分类数据集（RVL-CDIP、Tobacco3482、DocLayNet）和三个模型（ResNet、ConvNeXt、DiT）上进行了定性和定量评估，证明了方法的有效性。
- Conclusion: 这是首个在文档图像分析中探索生成反事实解释的工作，为模型决策提供了直观且可操作的解释。


### [74] [A machine learning approach for image classification in synthetic aperture RADAR](https://arxiv.org/abs/2508.04234)
*Romina Gaburro,Patrick Healy,Shraddha Naidu,Clifford Nolan*

Main category: cs.CV

TL;DR: 论文研究了使用卷积神经网络（CNN）在合成孔径雷达（SAR）中识别和分类地面物体的问题，通过模拟和真实SAR数据验证了方法的有效性。

- Motivation: 解决SAR图像中物体形状和冰类型的分类问题，探索CNN在此类任务中的潜力。
- Method: 采用单散射近似方法，结合模拟SAR数据和重建图像进行分类，并比较不同方法的成功率。
- Result: 在模拟和真实SAR数据中均实现了高分类准确率（≥75%），验证了CNN的有效性。
- Conclusion: CNN在SAR数据的几何和环境分类任务中表现优异，同时探讨了天线高度对分类效果的影响。


### [75] [PIS3R: Very Large Parallax Image Stitching via Deep 3D Reconstruction](https://arxiv.org/abs/2508.04236)
*Muhua Zhu,Xinhao Jin,Chengbo Wang,Yongcong Zhang,Yifei Xue,Tie Ji,Yizhen Lao*

Main category: cs.CV

TL;DR: 提出了一种基于深度3D重建的图像拼接方法PIS3R，能够有效处理大视差场景。

- Motivation: 现有方法难以处理大视差场景，导致拼接效果不佳。
- Method: 使用视觉几何基础Transformer获取相机参数和密集3D重建，通过重投影和点条件图像扩散模块优化拼接结果。
- Result: 实验表明，PIS3R在大视差场景下表现优于现有方法。
- Conclusion: PIS3R不仅解决了大视差问题，还保持了3D几何完整性，适用于下游3D视觉任务。


### [76] [From eye to AI: studying rodent social behavior in the era of machine Learning](https://arxiv.org/abs/2508.04255)
*Giuseppe Chindemi,Camilla Bellone,Benoit Girard*

Main category: cs.CV

TL;DR: 论文探讨了AI和机器学习在啮齿动物社会行为研究中的应用，分析了其优势和挑战，并提出了解决方案。

- Motivation: 传统方法存在偏见且难以捕捉复杂行为，现代方法结合计算机视觉、行为学和神经科学，为研究提供了更全面的视角。
- Method: 讨论了分析啮齿动物社会行为的主要步骤和可用工具，并评估其优缺点。
- Result: 现代方法提供了更深入的行为洞察，但也面临集成挑战。
- Conclusion: 论文旨在指导年轻研究者采用这些方法，并促进专家讨论工具的进一步优化。


### [77] [Segment Any Vehicle: Semantic and Visual Context Driven SAM and A Benchmark](https://arxiv.org/abs/2508.04260)
*Xiao Wang,Ziwen Wang,Wentao Wu,Anjie Wang,Jiashu Wu,Yantao Pan,Chenglong Li*

Main category: cs.CV

TL;DR: 提出SAV框架，结合SAM、知识图谱和上下文检索模块，解决车辆部件分割问题，并发布VehicleSeg10K数据集。

- Motivation: SAM无法直接用于车辆部件分割任务，因其缺乏语义标签和公开的文本提示功能。
- Method: SAV框架包含SAM编码器-解码器、车辆部件知识图谱和上下文样本检索编码模块。
- Result: 在VehicleSeg10K等数据集上进行了实验，验证了SAV的有效性。
- Conclusion: SAV框架和VehicleSeg10K数据集为车辆部件分割研究提供了新工具和基准。


### [78] [Revisiting Continual Semantic Segmentation with Pre-trained Vision Models](https://arxiv.org/abs/2508.04267)
*Duzhen Zhang,Yong Ren,Wei Cong,Junhao Zheng,Qiaoyi Su,Shuncheng Jia,Zhong-Zhi Li,Xuanle Zhao,Ye Bai,Feilong Chen,Qi Tian,Tielin Zhang*

Main category: cs.CV

TL;DR: 论文重新评估了直接微调（DFT）在持续语义分割（CSS）中的表现，发现预训练视觉模型（PVMs）本身具有抗遗忘能力，并提出改进方法DFT*。

- Motivation: 现有方法低估了PVMs的抗遗忘能力，认为直接微调会导致严重遗忘，但实际并非如此。
- Method: 通过系统实验分析DFT在两种基准数据集和两种PVM骨干网络上的表现，提出DFT*方法，包括冻结骨干网络和预分配分类器等策略。
- Result: 实验表明DFT*在性能和效率上优于16种现有方法。
- Conclusion: PVMs的抗遗忘能力被低估，DFT*是一种简单高效的CSS解决方案。


### [79] [PKSS-Align: Robust Point Cloud Registration on Pre-Kendall Shape Space](https://arxiv.org/abs/2508.04286)
*Chenlei Lv,Hui Huang*

Main category: cs.CV

TL;DR: PKSS-Align是一种鲁棒的点云配准方法，能够处理相似变换、非均匀密度、噪声点和缺陷部分，无需数据训练或复杂特征编码。

- Motivation: 点云配准对相似变换、噪声和不完整结构敏感，非均匀尺度和缺陷部分容易陷入局部最优。
- Method: 在Pre-Kendall形状空间（PKSS）上测量形状特征相似性，使用流形度量直接生成变换矩阵。
- Result: 实验表明，PKSS-Align在效率和实用性上优于现有方法。
- Conclusion: PKSS-Align是一种高效且鲁棒的点云配准方法，适用于多种复杂场景。


### [80] [MuGS: Multi-Baseline Generalizable Gaussian Splatting Reconstruction](https://arxiv.org/abs/2508.04297)
*Yaopeng Lou,Liao Shen,Tianqi Liu,Jiaqi Li,Zihao Huang,Huiqiang Sun,Zhiguo Cao*

Main category: cs.CV

TL;DR: MuRF是一种用于新视角合成的通用前馈方法，通过结合MVS和MDE特征提升重建能力，采用投影采样机制和参考视图损失优化几何与效率，利用3D高斯表示加速训练与推理，并在多种场景下实现SOTA性能。

- Motivation: 解决稀疏输入视图下（包括小基线和大基线）新视角合成的挑战，提升重建的泛化能力。
- Method: 结合MVS和MDE特征；提出投影采样机制构建概率体积；引入参考视图损失；使用3D高斯表示加速训练与推理。
- Result: 在DTU、RealEstate10K等数据集上实现SOTA性能，并在LLFF和Mip-NeRF 360上展示零样本性能。
- Conclusion: MuRF通过多基线特征融合和高效优化机制，显著提升了新视角合成的质量和效率。


### [81] [Length Matters: Length-Aware Transformer for Temporal Sentence Grounding](https://arxiv.org/abs/2508.04299)
*Yifan Wang,Ziyi Liu,Xiaolong Sun,Jiawei Wang,Hongmin Liu*

Main category: cs.CV

TL;DR: 论文提出了一种基于长度先验的长度感知Transformer（LATR），用于改进时序句子定位（TSG）任务，通过分组查询和长度分类任务减少冗余预测。

- Motivation: 现有的DETR-based模型在TSG任务中取得了进展，但由于缺乏显式监督，查询角色重叠导致冗余预测。
- Method: 提出LATR，将查询分为短、中、长三组，并引入长度分类任务，抑制不匹配长度的预测。
- Result: 在三个公共基准测试中达到最先进性能，消融实验验证了各组件和长度先验的重要性。
- Conclusion: LATR通过明确查询角色和利用长度先验，显著提升了TSG任务的性能。


### [82] [A Foundation Model for DAS Signal Recognition and Visual Prompt Tuning of the Pre-trained Model for Downstream Tasks](https://arxiv.org/abs/2508.04316)
*Kun Gui,Hongliang Ren,Shang Shi,Jin Lu,Changqiu Yu,Quanjun Cao,Guomin Gu,Qi Xuan*

Main category: cs.CV

TL;DR: 该论文提出了一种基于掩码自编码器（MAEPD）的分布式声学传感（DAS）信号识别基础模型，通过自监督预训练和视觉提示调优（VPT）解决数据分布不均和标注数据不足的问题，实验验证了其高效性和泛化能力。

- Motivation: 解决DAS技术中因异构传感环境导致的数据分布不均问题，以及标注数据不足对AI模型跨领域泛化的限制。
- Method: 提出MAEPD模型，基于掩码自编码器进行自监督预训练，使用VPT方法在下游任务中微调少量视觉提示向量。
- Result: 在室内步态识别任务中，VPT-Deep方法分类准确率达96.94%，仅微调0.322%参数，优于传统方法。模型在管道泄漏检测中也表现稳健。
- Conclusion: MAEPD作为一种基础模型，为DAS信号识别提供了高效、泛化性强的新范式。


### [83] [TempFlow-GRPO: When Timing Matters for GRPO in Flow Models](https://arxiv.org/abs/2508.04324)
*Xiaoxuan He,Siming Fu,Yuke Zhao,Wanli Li,Jian Yang,Dacheng Yin,Fengyun Rao,Bo Zhang*

Main category: cs.CV

TL;DR: TempFlow-GRPO提出了一种基于时间结构的GRPO框架，通过轨迹分支机制和噪声感知加权方案，优化了流匹配模型在文本到图像生成中的人类偏好对齐问题。

- Motivation: 现有流匹配模型在人类偏好对齐方面表现不佳，主要原因是时间均匀性假设导致奖励分配不精确，影响了探索效率和收敛效果。
- Method: TempFlow-GRPO引入了轨迹分支机制和噪声感知加权方案，前者通过分支点提供过程奖励，后者根据时间步的探索潜力调整策略优化。
- Result: 该方法在人类偏好对齐和标准文本到图像生成基准测试中实现了最先进的性能。
- Conclusion: TempFlow-GRPO通过时间感知优化显著提升了流匹配模型的生成质量和偏好对齐效果。


### [84] [RiemanLine: Riemannian Manifold Representation of 3D Lines for Factor Graph Optimization](https://arxiv.org/abs/2508.04335)
*Yanyan Li,Ze Yang,Keisuke Tateno,Federico Tombari Liang Zhao,Gim Hee Lee*

Main category: cs.CV

TL;DR: 论文提出了一种名为RiemanLine的3D线最小参数化方法，适用于独立线和并行线组，通过Riemannian流形实现统一表示，显著提升了相机定位和结构映射的精度。

- Motivation: 现有方法主要处理独立线，忽略了人造环境中普遍存在的并行线结构规律，导致参数冗余和优化效率低下。
- Method: 将每条线分解为全局和局部组件：共享的消失方向在单位球面上优化，缩放法向量约束在正交子空间中，从而紧凑编码结构规律。对于n条并行线，参数空间从4n降至2n+2。
- Result: 在ICL-NUIM、TartanAir和合成数据集上的实验表明，该方法显著提高了姿态估计和线重建的精度，同时降低了参数维度和提升了收敛稳定性。
- Conclusion: RiemanLine提供了一种高效且紧凑的3D线表示方法，特别适用于处理并行线结构，为相机定位和结构映射提供了更优的解决方案。


### [85] [RotatedMVPS: Multi-view Photometric Stereo with Rotated Natural Light](https://arxiv.org/abs/2508.04366)
*Songyun Yang,Yufei Han,Jilong Zhang,Kongming Liang,Peng Yu,Zhaowei Qu,Heng Guo*

Main category: cs.CV

TL;DR: RotatedMVPS提出了一种在旋转自然光下恢复形状和反射率的方法，通过旋转平台实现光照一致性，并结合单视图光度立体先验提升精度。

- Motivation: 现有MVPS方法需在暗室控制光照或忽略反射率和光照恢复，限制了自然光照场景和逆渲染任务的适用性。
- Method: 通过旋转平台确保光照一致性，减少环境光复杂性，并结合单视图光度立体先验数据。
- Result: 在合成和真实数据集上验证了方法的有效性。
- Conclusion: RotatedMVPS在自然光照下实现了高精度的形状和反射率恢复。


### [86] [TSPO: Temporal Sampling Policy Optimization for Long-form Video Language Understanding](https://arxiv.org/abs/2508.04369)
*Canhui Tang,Zifan Han,Hongbo Sun,Sanping Zhou,Xuchong Zhang,Xin Wei,Ye Yuan,Jinglin Xu,Hao Sun*

Main category: cs.CV

TL;DR: 提出了一种基于强化学习的时序采样策略优化方法（TSPO），用于提升多模态大语言模型（MLLMs）对长视频的理解能力。

- Motivation: 当前MLLMs在处理长视频时存在上下文限制和训练成本高的问题，现有方法如均匀采样或关键帧搜索可能遗漏重要事件或受限于预训练模型的事件理解能力。
- Method: 提出了一种可训练的事件感知时序代理，通过强化学习将关键帧选择与语言生成建模为联合决策过程，并设计了高效的基于规则的奖励机制。
- Result: TSPO在多个长视频理解基准测试中取得了最先进的性能，并展示了在不同前沿视频MLLMs中的可迁移能力。
- Conclusion: TSPO通过强化学习优化时序采样策略，显著提升了MLLMs对长视频的理解能力，具有广泛的应用潜力。


### [87] [VisionTS++: Cross-Modal Time Series Foundation Model with Continual Pre-trained Visual Backbones](https://arxiv.org/abs/2508.04379)
*Lefei Shen,Mouxiang Chen,Xu Liu,Han Fu,Xiaoxue Ren,Jianling Sun,Zhuo Li,Chenghao Liu*

Main category: cs.CV

TL;DR: VisionTS++通过视觉模型改进时间序列预测，解决了数据模态、多变量和概率预测的三大差距，并在多个基准测试中表现优异。

- Motivation: 探索视觉预训练模型在时间序列预测中的潜力，解决跨模态转移中的关键挑战。
- Method: 提出VisionTS++，包括高质量数据筛选、多变量时间序列彩色图像转换和多分位数预测方法。
- Result: 在多个基准测试中表现优异，MSE降低6%-44%，并在12个概率预测设置中9次排名第一。
- Conclusion: VisionTS++为跨模态知识转移提供了新范式，推动了通用时间序列基础模型的发展。


### [88] [ProtoN: Prototype Node Graph Neural Network for Unconstrained Multi-Impression Ear Recognition](https://arxiv.org/abs/2508.04381)
*Santhoshkumar Peddi,Sadhvik Bathini,Arun Balasubramanian,Monalisa Sarma,Debasis Samanta*

Main category: cs.CV

TL;DR: ProtoN是一个基于图的少样本学习框架，用于耳部生物识别，通过联合处理多个印象并利用原型图神经网络（PGNN）提升识别性能。

- Motivation: 耳部生物识别存在标注数据稀缺和类内差异大的问题，现有方法难以提取一致且具有区分性的特征。
- Method: 提出ProtoN框架，将每个印象表示为类特定图中的节点，并引入可学习的原型节点。通过PGNN层和双路径消息传递机制优化表示，结合跨图原型对齐策略和混合损失函数。
- Result: 在五个基准数据集上，ProtoN达到最高99.60%的Rank-1识别准确率和0.025的EER。
- Conclusion: ProtoN在少样本耳部识别中表现出色，有效解决了数据稀缺和类内差异问题。


### [89] [Deep Learning-based Scalable Image-to-3D Facade Parser for Generating Thermal 3D Building Models](https://arxiv.org/abs/2508.04406)
*Yinan Yu,Alex Gonzalez-Caceres,Samuel Scheidegger,Sanjay Somanath,Alexander Hollberg*

Main category: cs.CV

TL;DR: SI3FP是一种通过计算机视觉和深度学习从图像生成LoD3热模型的管道，解决了现有建筑改造中特征识别的挑战。

- Motivation: 现有建筑改造对气候影响至关重要，但早期改造规划需要基于LoD3热模型的模拟，而现有方法在可扩展性和准确性上存在不足。
- Method: SI3FP直接从正交图像平面建模几何基元，结合计算机视觉和深度学习，支持稀疏和密集数据源。
- Result: 在瑞典典型住宅建筑测试中，SI3FP在窗墙比估计中误差约为5%，适用于早期改造分析。
- Conclusion: SI3FP为大规模能源改造规划提供了高效工具，并在城市发展和规划中有广泛应用。


### [90] [Thinking With Videos: Multimodal Tool-Augmented Reinforcement Learning for Long Video Reasoning](https://arxiv.org/abs/2508.04416)
*Haoji Zhang,Xin Gu,Jiawen Li,Chixiang Ma,Sule Bai,Chubin Zhang,Bowen Zhang,Zhichao Zhou,Dongliang He,Yansong Tang*

Main category: cs.CV

TL;DR: 提出VITAL框架，通过工具增强学习和多任务数据集提升视频推理能力，优于现有方法。

- Motivation: 解决多模态大语言模型在视频推理中的跨模态交互不足和幻觉问题。
- Method: 结合视觉工具箱和多模态CoT，提出DGRPO算法优化多任务强化学习。
- Result: 在11个视频理解基准测试中表现优异，尤其在长视频场景。
- Conclusion: VITAL框架显著提升视频推理能力，代码和数据将公开。


### [91] [Efficient Inter-Task Attention for Multitask Transformer Models](https://arxiv.org/abs/2508.04422)
*Christian Bohn,Thomas Kurbiel,Klaus Friedrichs,Hasan Tercan,Tobias Meisen*

Main category: cs.CV

TL;DR: 提出了一种新型的可变形跨任务自注意力机制，用于多任务学习，显著降低了计算开销并提升了任务性能。

- Motivation: Transformer在多任务学习中因注意力矩阵规模问题面临计算瓶颈，需更高效的跨任务信息聚合方法。
- Method: 提出Deformable Inter-Task Self-Attention，优化跨任务特征图的信息聚合效率。
- Result: 在NYUD-v2和PASCAL-Context数据集上，计算量和推理延迟降低一个数量级，任务性能提升高达7.4%。
- Conclusion: 该方法有效解决了多任务学习中的计算效率问题，同时显著提升了模型性能。


### [92] [Composed Object Retrieval: Object-level Retrieval via Composed Expressions](https://arxiv.org/abs/2508.04424)
*Tong Wang,Guanyu Yang,Nian Liu,Zongyan Han,Jinxing Zhou,Salman Khan,Fahad Shahbaz Khan*

Main category: cs.CV

TL;DR: 论文提出了一种新的任务Composed Object Retrieval (COR)，超越了图像级检索，实现了对象级精确检索和分割，并构建了大规模基准COR127K和统一模型CORE。

- Motivation: 解决多模态系统中基于用户意图的细粒度视觉内容检索的挑战，特别是现有方法无法定位特定对象的问题。
- Method: 提出COR任务，构建COR127K基准，开发CORE模型，整合参考区域编码、自适应视觉-文本交互和区域级对比学习。
- Result: CORE模型在基础和新型类别中显著优于现有模型，为细粒度多模态检索研究提供了简单有效的基线。
- Conclusion: COR任务和CORE模型为细粒度多模态检索开辟了新方向，解决了对象级检索的灵活性挑战。


### [93] [Benchmarking Foundation Models for Mitotic Figure Classification](https://arxiv.org/abs/2508.04441)
*Jonas Ammeling,Jonathan Ganz,Emely Rosbach,Ludwig Lausser,Christof A. Bertram,Katharina Breininger,Marc Aubreville*

Main category: cs.CV

TL;DR: 研究探讨了自监督学习的基础模型在病理学中用于有丝分裂图像分类的表现，发现LoRA适应方法优于线性探测，且能显著减少数据需求。

- Motivation: 病理学中标记数据有限，自监督学习的基础模型能利用未标记数据提升模型性能。
- Method: 比较了线性探测和LoRA适应方法，并评估了模型在未见肿瘤域的表现。
- Result: LoRA适应方法仅需10%数据即可接近100%数据性能，且在未见域表现优异。
- Conclusion: LoRA适应方法高效，但传统架构的完全微调仍具竞争力。


### [94] [Boosting Visual Knowledge-Intensive Training for LVLMs Through Causality-Driven Visual Object Completion](https://arxiv.org/abs/2508.04453)
*Qingguo Hu,Ante Wang,Jia Song,Delai Qiu,Qingsong Liu,Jinsong Su*

Main category: cs.CV

TL;DR: 论文提出了一种基于视觉知识密集型任务CVC的自改进框架，以提升大型视觉语言模型（LVLMs）在深度视觉感知任务中的表现。

- Motivation: 现有LVLMs在需要深度视觉感知的任务中表现不足，可能由于指令调优语料库中视觉知识匮乏。
- Method: 通过自动化实例构建管道生成丰富示例，无需依赖复杂模型或人工干预，LVLMs通过试错学习自我改进。
- Result: 在四个挑战性任务和四个综合基准测试中表现显著提升，特别是在专业任务中平均提升5.4%和4.0%。
- Conclusion: CVC任务框架有效提升了LVLMs的视觉感知和推理能力。


### [95] [4DVD: Cascaded Dense-view Video Diffusion Model for High-quality 4D Content Generation](https://arxiv.org/abs/2508.04467)
*Shuzhou Yang,Xiaodong Cun,Xiaoyu Li,Yaowei Li,Jian Zhang*

Main category: cs.CV

TL;DR: 4DVD是一个级联视频扩散模型，通过解耦方式生成4D内容，分为粗多视图布局生成和结构感知条件生成两个子任务，显著提升了4D生成的质量和效率。

- Motivation: 直接生成高维数据（如4D）复杂度高，现有方法难以同时建模3D空间和时间特征，因此提出解耦方法以简化任务并提高生成质量。
- Method: 4DVD首先预测单目视频的密集视图布局，然后基于布局先验开发结构感知时空生成分支，结合粗结构先验和输入视频的外观内容生成高质量密集视图视频。
- Result: 实验表明，4DVD在新视角合成和4D生成任务上达到了最先进的性能。
- Conclusion: 4DVD通过解耦方法有效生成高质量4D内容，为4D表示优化提供了新思路，具有广泛的实际应用潜力。


### [96] [FrEVL: Leveraging Frozen Pretrained Embeddings for Efficient Vision-Language Understanding](https://arxiv.org/abs/2508.04469)
*Emmanuelle Bourigault,Pauline Bourigault*

Main category: cs.CV

TL;DR: FrEVL框架探索冻结预训练嵌入是否支持有效的视觉语言理解，发现其在判别任务中表现优异，性能接近SOTA，同时显著降低计算和能耗。

- Motivation: 解决视觉语言模型部署中高计算需求的问题，探索冻结嵌入的潜力。
- Method: 利用冻结预训练嵌入进行视觉语言理解，分析其性能与预训练目标的对齐关系。
- Result: 冻结嵌入在标准基准测试中达到SOTA性能的85%至95%，计算速度提升2.3倍，能耗降低52%。
- Conclusion: 冻结嵌入方法在特定场景下是可行的替代方案，为高效多模态理解提供了实践指导。


### [97] [Zero-Residual Concept Erasure via Progressive Alignment in Text-to-Image Model](https://arxiv.org/abs/2508.04472)
*Hongxu Chen,Zhen Wang,Taoran Mei,Lin Li,Bowei Zhu,Runshi Li,Long Chen*

Main category: cs.CV

TL;DR: 论文提出了一种新的概念擦除方法ErasePro，解决了现有方法在复杂文本提示下擦除不彻底和生成质量下降的问题。

- Motivation: 现有概念擦除方法存在擦除不彻底和生成质量下降的局限性，需要更高效的解决方案。
- Method: ErasePro引入零残差约束和渐进式层更新策略，确保目标概念与锚概念完美对齐，并逐步更新模型参数。
- Result: 实验证明ErasePro在不同概念擦除任务中表现优异，实现了更彻底的擦除和更好的生成质量。
- Conclusion: ErasePro通过优化目标设计和更新策略，显著提升了概念擦除的效果和生成质量。


### [98] [QuantVSR: Low-Bit Post-Training Quantization for Real-World Video Super-Resolution](https://arxiv.org/abs/2508.04485)
*Bowen Chai,Zheng Chen,Libo Zhu,Wenbo Li,Yong Guo,Yulun Zhang*

Main category: cs.CV

TL;DR: QuantVSR是一种针对真实世界视频超分辨率（VSR）的低比特量化模型，通过时空复杂度感知机制和可学习偏置对齐模块，显著提升了量化性能。

- Motivation: 扩散模型在视频超分辨率中表现优异，但处理速度慢且资源消耗大，量化是压缩模型的潜在解决方案，但VSR模型的时序特性和高保真要求使其量化具有挑战性。
- Method: 提出QuantVSR，采用时空复杂度感知（STCA）机制为每层分配特定秩，并联合优化全精度和低比特分支；引入可学习偏置对齐（LBA）模块减少量化误差。
- Result: 在合成和真实数据集上的实验表明，QuantVSR性能与全精度模型相当，并显著优于其他低比特量化方法。
- Conclusion: QuantVSR通过STCA和LBA模块有效解决了VSR模型量化问题，为实际应用提供了高效解决方案。


### [99] [Learning Robust Intervention Representations with Delta Embeddings](https://arxiv.org/abs/2508.04492)
*Panagiotis Alimisis,Christos Diou*

Main category: cs.CV

TL;DR: 论文提出了一种通过因果增量嵌入（Causal Delta Embedding）表示干预的方法，以提高模型在分布外（OOD）场景中的鲁棒性。

- Motivation: 现有研究多关注场景变量的因果表示，而较少关注干预本身的表示。本文旨在填补这一空白，通过干预的表示提升模型泛化能力。
- Method: 提出因果增量嵌入框架，该嵌入对视觉场景不变且稀疏影响因果变量，无需额外监督即可从图像对中学习因果表示。
- Result: 在Causal Triplet挑战中，因果增量嵌入在合成和真实世界基准测试中显著优于基线性能。
- Conclusion: 因果增量嵌入是提升OOD鲁棒性的有效策略，尤其在干预表示方面具有潜力。


### [100] [MonoCloth: Reconstruction and Animation of Cloth-Decoupled Human Avatars from Monocular Videos](https://arxiv.org/abs/2508.04505)
*Daisheng Jin,Ying He*

Main category: cs.CV

TL;DR: MonoCloth提出了一种从单目视频重建和动画化穿衣人体化身的创新方法，通过部件分解策略和专用布料模拟模块提升重建质量和动画真实感。

- Motivation: 单目视频中重建3D人体化身因几何信息有限和非刚性运动复杂而具有挑战性，需要更高效的方法。
- Method: 采用部件分解策略（身体、面部、手部、衣物），并针对衣物提出布料模拟模块，结合时间运动线索和几何约束。
- Result: 实验表明MonoCloth在视觉重建质量和动画真实感上优于现有方法，并支持衣物转移等附加任务。
- Conclusion: MonoCloth通过部件化设计和专用模块，显著提升了单目视频中穿衣人体化身的重建与动画效果，具有广泛实用性。


### [101] [Skeleton Motion Words for Unsupervised Skeleton-Based Temporal Action Segmentation](https://arxiv.org/abs/2508.04513)
*Uzay Gökay,Federico Spurio,Dominik R. Bach,Juergen Gall*

Main category: cs.CV

TL;DR: 提出了一种无监督的基于骨架的时间动作分割方法，利用序列到序列的时间自编码器和量化技术，显著优于现有方法。

- Motivation: 现有方法多为监督学习，需要昂贵标注数据，而无监督方法在骨架序列上研究不足，但其在现实应用中更具鲁棒性和隐私保护性。
- Method: 使用序列到序列时间自编码器，将关节信息解耦嵌入，并通过非重叠补丁和量化生成骨架运动词，实现动作聚类。
- Result: 在HuGaDB、LARa和BABEL数据集上评估，性能优于当前最先进的无监督方法。
- Conclusion: 该方法为无监督骨架动作分割提供了有效解决方案，代码已开源。


### [102] [RAIDX: A Retrieval-Augmented Generation and GRPO Reinforcement Learning Framework for Explainable Deepfake Detection](https://arxiv.org/abs/2508.04524)
*Tianxiao Li,Zhenglin Huang,Haiquan Wen,Yiwei He,Shuchang Lyu,Baoyuan Wu,Guangliang Cheng*

Main category: cs.CV

TL;DR: RAIDX是一个结合RAG和GRPO的新型深度伪造检测框架，提高检测准确性和可解释性，无需大量人工标注。

- Motivation: 当前深度伪造检测方法缺乏透明性，且依赖人工标注，RAIDX旨在解决这些问题。
- Method: RAIDX整合RAG引入外部知识提升检测准确性，利用GRPO生成细粒度文本解释和显著性图。
- Result: RAIDX在多个基准测试中表现优异，提供可解释的文本描述和显著性图，达到最先进的检测性能。
- Conclusion: RAIDX首次统一RAG和GRPO，填补了准确性和可解释性方面的关键空白。


### [103] [No Masks Needed: Explainable AI for Deriving Segmentation from Classification](https://arxiv.org/abs/2508.04534)
*Mosong Ma,Tania Stathaki,Michalis Lazarou*

Main category: cs.CV

TL;DR: 提出一种针对医学图像的预训练模型微调方法，结合可解释AI提升分割效果。

- Motivation: 医学图像分割对现代医疗至关重要，但现有无监督方法在医学领域表现不佳。
- Method: 微调预训练模型，结合可解释AI生成相关性分数优化分割。
- Result: 在CBIS-DDSM、NuInsSeg和Kvasir-SEG等数据集上取得更好效果。
- Conclusion: 该方法优于传统方法，适用于医学图像分割。


### [104] [TopKD: Top-scaled Knowledge Distillation](https://arxiv.org/abs/2508.04539)
*Qi Wang,Jinjia Zhou*

Main category: cs.CV

TL;DR: 论文提出TopKD框架，通过放大教师模型中最具信息量的logits（Top-K知识）来改进基于logit的知识蒸馏，无需额外模块或架构改动。

- Motivation: 现有知识蒸馏方法多关注特征级知识转移，忽视了教师模型logit分布中的关键信息。
- Method: TopKD包含Top-K缩放模块（TSM）和Top-K解耦损失（TDL），自适应放大关键logits并提供针对性监督。
- Result: 在多个数据集上，TopKD表现优于现有蒸馏方法，尤其在Vision Transformers中效果显著。
- Conclusion: TopKD展示了logit在知识蒸馏中的潜力，是一种简单高效的通用框架。


### [105] [InceptoFormer: A Multi-Signal Neural Framework for Parkinson's Disease Severity Evaluation from Gait](https://arxiv.org/abs/2508.04540)
*Safwen Naimi,Arij Said,Wassim Bouachir,Guillaume-Alexandre Bilodeau*

Main category: cs.CV

TL;DR: InceptoFormer是一个多信号神经框架，用于通过步态动力学分析评估帕金森病（PD）严重程度，结合了Inception1D和Transformer架构，取得了96.6%的准确率。

- Motivation: 解决PD严重程度评估中的多尺度特征提取和类别不平衡问题。
- Method: 提出Inception1D（多尺度1D卷积）和Transformer框架，结合过采样策略处理数据不平衡。
- Result: 在PD严重程度评估中达到96.6%的准确率，优于现有方法。
- Conclusion: InceptoFormer能有效捕捉步态信号的局部和全局特征，显著提升PD评估性能。


### [106] [Hierarchical Event Memory for Accurate and Low-latency Online Video Temporal Grounding](https://arxiv.org/abs/2508.04546)
*Minghang Zheng,Yuxin Peng,Benyuan Sun,Yi Yang,Yang Liu*

Main category: cs.CV

TL;DR: 本文提出了一种分层事件记忆方法，用于在线视频时间定位任务（OnVTG），通过建模事件级信息并保留长期历史信息，显著提升了性能。

- Motivation: 在线视频时间定位任务（OnVTG）需要模型在不观察未来帧的情况下定位事件，现有方法缺乏有效的事件建模和长期历史信息保留，导致性能低下。
- Method: 提出基于事件的分层事件记忆框架，包括事件提案建模和未来预测分支，以保留历史事件信息并实现实时预测。
- Result: 在TACoS、ActivityNet Captions和MAD数据集上取得了最先进的性能。
- Conclusion: 分层事件记忆方法有效解决了OnVTG任务中的事件建模和长期信息保留问题，显著提升了性能。


### [107] [MSC: A Marine Wildlife Video Dataset with Grounded Segmentation and Clip-Level Captioning](https://arxiv.org/abs/2508.04549)
*Quang-Trung Truong,Yuk-Kwan Wong,Vo Hoang Kim Tuyen Dang,Rinaldi Gotama,Duc Thanh Nguyen,Sai-Kit Yeung*

Main category: cs.CV

TL;DR: 提出了一种两阶段的海洋视频字幕生成方法，通过视频、文本和分割掩码的三元组提升海洋视频理解与分析。

- Motivation: 现有视频字幕数据集难以适应海洋环境的复杂性，无法有效捕捉海洋生物信息。
- Method: 采用两阶段的海洋对象导向视频字幕生成流程，结合视频分割检测显著对象转换。
- Result: 提出的方法显著提升了海洋视频字幕生成的语义丰富性和准确性。
- Conclusion: 该方法为海洋视频理解与分析提供了有效工具，数据集和代码已公开。


### [108] [Two-Way Garment Transfer: Unified Diffusion Framework for Dressing and Undressing Synthesis](https://arxiv.org/abs/2508.04551)
*Angang Zhang,Fang Deng,Hao Chen,Zhongjian Chen,Junyan Li*

Main category: cs.CV

TL;DR: 论文提出了一种双向服装转移模型（TWGTM），首次统一解决了虚拟试穿（VTON）和虚拟脱衣（VTOFF）的双向任务，通过双向特征解耦和分阶段训练实现了互补性。

- Motivation: 现有研究将虚拟试穿和虚拟脱衣视为独立任务，忽略了它们的互补对称性，缺乏系统性研究。
- Method: 提出了双向服装转移模型（TWGTM），结合潜在空间和像素空间的双重条件引导，并采用分阶段训练解决掩模依赖不对称问题。
- Result: 在DressCode和VITON-HD数据集上的实验验证了模型的有效性和竞争力。
- Conclusion: TWGTM首次统一解决了VTON和VTOFF任务，为服装中心图像合成提供了新思路。


### [109] [Augmentation-based Domain Generalization and Joint Training from Multiple Source Domains for Whole Heart Segmentation](https://arxiv.org/abs/2508.04552)
*Franz Thaler,Darko Stern,Gernot Plank,Martin Urschler*

Main category: cs.CV

TL;DR: 论文提出了一种针对心脏医学图像（CT和MR）的语义分割方法，通过平衡联合训练和强化的数据增强技术，解决了领域偏移问题，取得了较高的分割精度。

- Motivation: 心血管疾病是全球主要死因，需要更精确的心脏图像分析方法以支持个性化治疗和心脏数字孪生模型的生成。
- Method: 采用平衡联合训练（同时使用CT和MR数据）和强化的强度与空间数据增强技术，结合5折集成方法。
- Result: 在CT数据上达到93.33% DSC和0.8388 mm ASSD，MR数据上达到89.30% DSC和1.2411 mm ASSD，性能优于或接近单模态训练模型。
- Conclusion: 该方法能高效生成精确的心脏语义分割，为个性化心脏数字孪生模型提供了潜力。


### [110] [One Model For All: Partial Diffusion for Unified Try-On and Try-Off in Any Pose](https://arxiv.org/abs/2508.04559)
*Jinxi Liu,Zijian He,Guangrun Wang,Guanbin Li,Liang Lin*

Main category: cs.CV

TL;DR: OMFA是一个统一的扩散框架，支持无需展示衣物和任意姿势的虚拟试穿和试脱，通过部分扩散策略实现高效的双向衣物-人转换。

- Motivation: 现有方法依赖展示衣物和分割掩码，且难以处理灵活姿势变化，限制了实际应用。
- Method: 采用部分扩散策略，选择性对输入组件（如衣物、人像或面部）应用噪声和去噪，结合SMPL-X姿势条件。
- Result: OMFA在试穿和试脱任务上达到最先进水平，支持多视角和任意姿势的试穿。
- Conclusion: OMFA提供了一个实用且通用的虚拟衣物合成解决方案。


### [111] [Drone Detection with Event Cameras](https://arxiv.org/abs/2508.04564)
*Gabriele Magrini,Lorenzo Berlincioni,Luca Cultrera,Federico Becattini,Pietro Pala*

Main category: cs.CV

TL;DR: 事件相机在无人机检测中表现出色，解决了传统相机的运动模糊和光照问题，支持实时跟踪和识别。

- Motivation: 无人机扩散带来安全挑战，传统相机因目标小、速度快和光照问题难以可靠检测。
- Method: 综述事件相机技术，包括数据表示方法和尖峰神经网络处理流程，扩展至实时跟踪和识别任务。
- Result: 事件相机消除运动模糊，在极端光照下稳定检测，支持低延迟高效反无人机系统。
- Conclusion: 事件相机为下一代可靠、低延迟、高效的反无人机系统提供了强大基础。


### [112] [TAlignDiff: Automatic Tooth Alignment assisted by Diffusion-based Transformation Learning](https://arxiv.org/abs/2508.04565)
*Yunbi Liu,Enqi Tang,Shiyu Li,Lei Ma,Juncheng Li,Shu Lou,Yongchu Pan,Qingshan Liu*

Main category: cs.CV

TL;DR: TAlignDiff是一种基于扩散变换学习的自动牙齿对齐方法，结合点云回归网络和扩散去噪模块，优于传统方法。

- Motivation: 传统深度学习方法通过点对点几何约束预测变换矩阵，但忽略了矩阵与口腔解剖结构的关联及其分布特性。
- Method: TAlignDiff包括点云回归网络（PRN）和扩散去噪模块（DTMD），结合几何约束和扩散建模。
- Result: 实验表明TAlignDiff在牙齿对齐中效果显著，优于现有方法。
- Conclusion: TAlignDiff为牙齿矫正治疗提供了更优的自动对齐方案。


### [113] [CLASP: Cross-modal Salient Anchor-based Semantic Propagation for Weakly-supervised Dense Audio-Visual Event Localization](https://arxiv.org/abs/2508.04566)
*Jinxing Zhou,Ziheng Zhou,Yanghao Zhou,Yuxin Mao,Zhangling Duan,Dan Guo*

Main category: cs.CV

TL;DR: 本文提出了一种弱监督的密集视听事件定位方法（W-DAVEL），通过跨模态显著锚点提升事件定位性能。

- Motivation: 解决在仅有视频级事件标签的弱监督条件下，如何准确定位跨模态事件的挑战。
- Method: 提出跨模态显著锚点识别模块和基于锚点的时间传播模块，利用音频和视觉模态的一致性提升定位效果。
- Result: 在UnAV-100和ActivityNet1.3数据集上实现了最先进的性能。
- Conclusion: 该方法在弱监督条件下显著提升了跨模态事件定位的准确性。


### [114] [Analyzing and Mitigating Object Hallucination: A Training Bias Perspective](https://arxiv.org/abs/2508.04567)
*Yifan Li,Kun Zhou,Wayne Xin Zhao,Lei Fang,Ji-Rong Wen*

Main category: cs.CV

TL;DR: 论文研究了大型视觉语言模型（LVLMs）中的幻觉问题，提出了新基准POPEv2和轻量级去偏方法Obliviate，显著减少了幻觉现象。

- Motivation: LVLMs在多模态能力上表现优异，但仍存在幻觉问题（生成与视觉输入不一致的文本），研究旨在探究训练数据在幻觉中的作用。
- Method: 引入POPEv2基准（包含掩码对象的反事实图像），通过实验发现训练偏差主要存在于语言建模头部，提出Obliviate方法（仅更新LM头部参数）。
- Result: Obliviate显著减少了幻觉现象，仅更新2%参数且复用训练数据，适用于不同模型规模和数据量，并泛化至其他幻觉类型。
- Conclusion: Obliviate是一种高效的去偏方法，为缓解LVLMs的幻觉问题提供了实用解决方案。


### [115] [DDTracking: A Deep Generative Framework for Diffusion MRI Tractography with Streamline Local-Global Spatiotemporal Modeling](https://arxiv.org/abs/2508.04568)
*Yijie Li,Wei Zhang,Xi Zhu,Ye Wu,Yogesh Rathi,Lauren J. O'Donnell,Fan Zhang*

Main category: cs.CV

TL;DR: DDTracking是一种新的深度生成框架，用于扩散MRI纤维束成像，通过条件去噪扩散过程实现流线传播。

- Motivation: 提出一种能够同时捕捉局部空间细节和全局时间依赖性的方法，以提升纤维束成像的准确性和鲁棒性。
- Method: 采用双路径编码网络联合建模局部空间编码和全局时间依赖性，并设计条件扩散模型模块预测流线传播方向。
- Result: 在多个数据集上验证，DDTracking显著优于现有方法，并展现出强大的跨数据集泛化能力。
- Conclusion: DDTracking提供了一种可扩展、适应性强且端到端可学习的解决方案，适用于广泛的扩散MRI应用。


### [116] [Knowledge to Sight: Reasoning over Visual Attributes via Knowledge Decomposition for Abnormality Grounding](https://arxiv.org/abs/2508.04572)
*Jun Li,Che Liu,Wenjia Bai,Mingxuan Liu,Rossella Arcucci,Cosmin I. Bercea,Julia A. Schnabel*

Main category: cs.CV

TL;DR: 提出K2Sight框架，通过结构化语义监督将临床概念分解为视觉属性，实现高效训练的小型模型，性能优于大型医疗VLM。

- Motivation: 解决医疗图像中基于文本描述定位临床异常的问题，现有通用或专用VLM在医疗领域表现不佳或资源消耗大。
- Method: 引入结构化语义监督，将临床概念分解为视觉属性（如形状、密度、解剖位置），并通过指令式提示指导区域-文本对齐。
- Result: 使用仅1.5%的数据训练的小型模型（0.23B和2B参数）性能优于7B+医疗VLM，mAP50提升达9.82%。
- Conclusion: K2Sight框架有效结合领域知识与空间结构，实现数据高效训练，性能优越且资源消耗低。


### [117] [Visual Bias and Interpretability in Deep Learning for Dermatological Image Analysis](https://arxiv.org/abs/2508.04573)
*Enam Ahmed Taufik,Abdullah Khondoker,Antara Firoz Parsa,Seraj Al Mahmud Mostafa*

Main category: cs.CV

TL;DR: 该论文提出了一种深度学习框架，用于多类皮肤病分类，评估了三种图像预处理技术，并比较了多种预训练模型的性能，最终发现DinoV2结合RGB预处理效果最佳。

- Motivation: 皮肤病分类因类间相似性高、类内变异性大以及病变纹理复杂而具有挑战性，需要高效的预处理和模型架构。
- Method: 系统评估了RGB、CMY颜色空间转换和CLAHE三种预处理技术，并比较了DenseNet201、Efficient-NetB5、ViT、Swin Transformer和DinoV2 Large等模型的性能。
- Result: DinoV2结合RGB预处理取得了最高准确率（93%）和F1分数，Grad-CAM可视化进一步验证了其精确的病变定位能力。
- Conclusion: 有效的预处理和模型选择对构建稳健且可解释的皮肤病CAD系统至关重要。


### [118] [Face-voice Association in Multilingual Environments (FAME) 2026 Challenge Evaluation Plan](https://arxiv.org/abs/2508.04592)
*Marta Moscati,Ahmed Abdullah,Muhammad Saad Saeed,Shah Nawaz,Rohan Kumar Das,Muhammad Zaigham Zaheer,Junaid Mir,Muhammad Haroon Yousaf,Khalid Malik,Markus Schedl*

Main category: cs.CV

TL;DR: FAME 2026挑战赛专注于多语言环境下的面部-语音关联研究，使用MAV-Celeb数据集，并提供了挑战赛的细节、基线模型和任务说明。

- Motivation: 全球半数人口为双语者，多语言场景下的面部-语音关联研究具有实际意义。
- Method: 使用MAV-Celeb数据集，探索多语言环境下的面部-语音关联。
- Result: 提供了挑战赛的基线模型和任务细节。
- Conclusion: FAME挑战赛为多语言环境下的面部-语音关联研究提供了平台和资源。


### [119] [Pseudo Depth Meets Gaussian: A Feed-forward RGB SLAM Baseline](https://arxiv.org/abs/2508.04597)
*Linqing Zhao,Xiuwei Xu,Yirui Wang,Hao Wang,Wenzhao Zheng,Yansong Tang,Haibin Yan,Jiwen Lu*

Main category: cs.CV

TL;DR: 提出了一种基于3D高斯SLAM的在线3D重建方法，结合前馈循环预测模块直接从光流推断相机位姿，显著提升了跟踪速度。

- Motivation: 现有方法在长序列处理或深度传感器依赖上存在不足，需解决实时性和几何细节准确性的问题。
- Method: 集成深度估计器和3D高斯映射，结合前馈循环预测模块和局部图渲染技术。
- Result: 在Replica和TUM-RGBD数据集上表现优异，跟踪时间减少90%以上。
- Conclusion: 该方法在保持性能的同时大幅提升效率，适用于实时3D重建。


### [120] [OmniDepth: Bridging Monocular and Stereo Reasoning with Latent Alignment](https://arxiv.org/abs/2508.04611)
*Tongfan Guan,Jiaxin Guo,Chen Wang,Yun-Hui Liu*

Main category: cs.CV

TL;DR: OmniDepth是一个统一框架，通过迭代双向对齐单目和立体深度估计的潜在表示，结合两者的优势。

- Motivation: 单目方法缺乏几何精度，立体方法在反射或无纹理表面存在模糊性，两者在实践中仍分离。OmniDepth旨在通过统一框架解决这些问题。
- Method: 提出跨注意力对齐机制，动态同步单目上下文线索与立体假设表示，在立体推理中相互对齐。
- Result: OmniDepth在Middlebury和ETH3D上零样本泛化误差减少>40%，并解决了透明和反射表面的长期问题。
- Conclusion: OmniDepth通过结合多视图几何与单目上下文，实现了超越单一模态限制的鲁棒3D感知。


### [121] [How Does Bilateral Ear Symmetry Affect Deep Ear Features?](https://arxiv.org/abs/2508.04614)
*Kagan Ozturk,Deeksha Arun,Kevin W. Bowyer,Patrick Flynn*

Main category: cs.CV

TL;DR: 研究探讨了双耳对称性对CNN耳部识别的影响，发现分开处理左右耳能提升性能。

- Motivation: 双耳对称性对CNN学习特征的影响尚未充分研究，本文旨在填补这一空白。
- Method: 开发耳侧分类器区分左右耳，并在训练和测试中利用侧信息，进行跨数据集评估。
- Result: 分开处理左右耳显著提升性能，消融实验提供了训练CNN的实用建议。
- Conclusion: 双耳对称性对CNN耳部识别有重要影响，分开处理左右耳可优化性能。


### [122] [FinMMR: Make Financial Numerical Reasoning More Multimodal, Comprehensive, and Challenging](https://arxiv.org/abs/2508.04625)
*Zichen Tang,Haihong E,Jiacheng Liu,Zhongjun Yang,Rongjin Li,Zihua Rong,Haoyang He,Zhuodi Hao,Xinyang Hu,Kun Ji,Ziyan Ma,Mengyuan Ji,Jun Zhang,Chenghao Ma,Qianhe Zheng,Yang Liu,Yiling Huang,Xinyi Hu,Qing Huang,Zijian Xie,Shiyao Peng*

Main category: cs.CV

TL;DR: FinMMR是一个双语多模态基准，用于评估多模态大语言模型在金融数值推理任务中的能力，包含4.3K问题和8.7K图像，涵盖14个金融子领域。

- Motivation: 现有基准在金融领域的多模态推理能力评估上存在不足，FinMMR填补了这一空白，并推动MLLMs在真实场景中的推理能力提升。
- Method: 通过转换现有金融推理基准和构建新问题，FinMMR整合了多模态数据（文本和图像），涵盖14个金融子领域，并设计了多步精确数值推理任务。
- Result: 最佳MLLM在困难问题上仅达到53.0%准确率，表明任务具有挑战性。
- Conclusion: FinMMR将推动MLLMs在金融领域推理能力的进一步发展。


### [123] [EncQA: Benchmarking Vision-Language Models on Visual Encodings for Charts](https://arxiv.org/abs/2508.04650)
*Kushin Mukherjee,Donghao Ren,Dominik Moritz,Yannick Assogba*

Main category: cs.CV

TL;DR: 论文提出了EncQA基准测试，用于评估视觉语言模型在图表理解中的视觉推理能力，发现模型性能在不同任务和编码中存在显著差异，且模型规模并非总是提升性能的关键。

- Motivation: 当前视觉语言模型在图表理解基准测试中的进步未能全面覆盖视觉推理能力，需要更系统的评估方法。
- Method: 引入EncQA基准测试，包含2,076个合成问题-答案对，覆盖6种视觉编码通道和8种分析任务。
- Result: 评估9种先进模型发现，性能在任务和编码间差异显著，模型规模对许多任务-编码组合无显著提升。
- Conclusion: 提升图表理解需要针对特定视觉推理差距的策略，而非单纯扩大模型或数据集规模。


### [124] [X-SAM: From Segment Anything to Any Segmentation](https://arxiv.org/abs/2508.04655)
*Hao Wang,Limeng Qiao,Zequn Jie,Zhijian Huang,Chengjian Feng,Qingfang Zheng,Lin Ma,Xiangyuan Lan,Xiaodan Liang*

Main category: cs.CV

TL;DR: X-SAM是一个多模态大语言模型框架，扩展了图像分割能力，从“分割任何东西”到“任何分割”，并提出了新的视觉基础分割任务。

- Motivation: 解决大语言模型在像素级感知理解上的不足，以及现有分割模型在多掩码预测和类别特定分割任务中的局限性。
- Method: 提出统一框架和新的视觉基础分割任务（VGD），支持多数据集联合训练。
- Result: X-SAM在多个图像分割基准测试中达到最先进性能。
- Conclusion: X-SAM为多模态像素级视觉理解提供了高效解决方案。


### [125] [YOLOv8-Based Deep Learning Model for Automated Poultry Disease Detection and Health Monitoring paper](https://arxiv.org/abs/2508.04658)
*Akhil Saketh Reddy Sabbella,Ch. Lakshmi Prachothan,Eswar Kumar Panta*

Main category: cs.CV

TL;DR: 使用YOLO v8深度学习模型实时检测鸡群疾病，提升养殖场管理效率。

- Motivation: 传统人工检测鸡群疾病效率低且易出错，需自动化解决方案。
- Method: 开发基于YOLO v8的系统，通过高分辨率图像分析鸡群行为和外观异常。
- Result: 算法通过标注数据集训练，实现高精度实时检测，并向养殖场主发出警报。
- Conclusion: AI技术可早期识别感染，减少人工检查需求，提升大规模养殖场生物安全。


### [126] [PixCuboid: Room Layout Estimation from Multi-view Featuremetric Alignment](https://arxiv.org/abs/2508.04659)
*Gustav Hanning,Kalle Åström,Viktor Larsson*

Main category: cs.CV

TL;DR: PixCuboid提出了一种基于多视图对齐的优化方法，用于估计长方体形状的房间布局，显著优于现有方法。

- Motivation: 粗粒度房间布局估计为下游任务提供重要几何线索，现有方法多基于单视图且假设全景图像。
- Method: 通过端到端训练优化密集深度特征的多视图对齐，学习具有大收敛域和平滑损失景观的特征图。
- Result: 在ScanNet++和2D-3D-Semantics新基准测试中显著优于竞争对手。
- Conclusion: PixCuboid的灵活性使其可轻松扩展到多房间估计，代码和模型权重已开源。


### [127] [HierarchicalPrune: Position-Aware Compression for Large-Scale Diffusion Models](https://arxiv.org/abs/2508.04663)
*Young D. Kwon,Rui Li,Sijia Li,Da Li,Sourav Bhattacharya,Stylianos I. Venieris*

Main category: cs.CV

TL;DR: HierarchicalPrune是一种新的压缩框架，通过分层修剪、权重保护和敏感性引导蒸馏技术，显著减少扩散模型的参数规模，同时保持输出质量。

- Motivation: 现有文本到图像扩散模型参数规模庞大（8-11B），在资源受限设备上推理困难，需要高效压缩方法。
- Method: 结合分层位置修剪、位置权重保护和敏感性引导蒸馏三种技术，优化模型结构。
- Result: 内存占用减少77.5-80.4%，延迟降低27.9-38.0%，生成质量损失极小（GenEval下降2.6%，HPSv2下降7%）。
- Conclusion: HierarchicalPrune在保持感知质量的同时，显著提升了模型的资源效率，优于现有方法。


### [128] [ANPrompt: Anti-noise Prompt Tuning for Vision-Language Models](https://arxiv.org/abs/2508.04677)
*Yansheng Gao,Yufei Zheng,Jinghan Qu,Zixi Zhu,Yukuan Zhang,Shengsheng Wang*

Main category: cs.CV

TL;DR: ANPrompt是一种新的提示调优框架，旨在增强视觉语言模型对弱语义扰动的鲁棒性，通过噪声提示和抗噪声目标提升性能。

- Motivation: 现有提示调优方法对弱语义扰动（如图像或文本噪声）的脆弱性导致模型泛化能力下降，需要一种更鲁棒的解决方案。
- Method: ANPrompt通过融合噪声文本特征生成噪声提示，结合可学习提示令牌生成抗噪声提示，并引入噪声感知视觉提示原型（NRVPP）和弱语义噪声对齐损失（WALoss）。
- Result: 在11个基准测试中，ANPrompt表现优于现有方法，具有更强的抗噪声能力和对新类别的泛化能力。
- Conclusion: ANPrompt通过噪声提示和抗噪声目标显著提升了视觉语言模型在弱语义扰动下的鲁棒性和泛化性能。


### [129] [Perceiving and Acting in First-Person: A Dataset and Benchmark for Egocentric Human-Object-Human Interactions](https://arxiv.org/abs/2508.04681)
*Liang Xu,Chengqun Yang,Zili Lin,Fei Xu,Yifan Liu,Congsheng Xu,Yiyi Zhang,Jie Qin,Xingdong Sheng,Yunhui Liu,Xin Jin,Yichao Yan,Wenjun Zeng,Xiaokang Yang*

Main category: cs.CV

TL;DR: InterVLA是一个大规模的人-物-人交互数据集，结合了第一人称视角和多模态数据，旨在推动AI助手在物理世界中的发展。

- Motivation: 现有数据集多为专业交互类别，缺乏通用交互知识和第一人称视角，而这对AI助手的感知和行动至关重要。
- Method: 通过混合RGB-MoCap系统，结合第一人称视觉和指令，构建了一个包含11.4小时、1.2M帧多模态数据的数据集。
- Result: 建立了新的基准测试，包括第一人称人体运动估计、交互合成和交互预测。
- Conclusion: InterVLA数据集和基准测试将为未来AI代理在物理世界中的应用提供重要支持。


### [130] [TurboTrain: Towards Efficient and Balanced Multi-Task Learning for Multi-Agent Perception and Prediction](https://arxiv.org/abs/2508.04682)
*Zewei Zhou,Seth Z. Zhao,Tianhui Cai,Zhiyu Huang,Bolei Zhou,Jiaqi Ma*

Main category: cs.CV

TL;DR: TurboTrain是一个高效的多智能体训练框架，通过掩码重建学习和梯度冲突抑制简化训练流程，提升性能。

- Motivation: 多智能体系统的端到端训练在多任务性能上有优势，但训练过程复杂且需要大量人工设计。
- Method: 提出TurboTrain框架，包含基于掩码重建学习的多智能体时空预训练和基于梯度冲突抑制的平衡多任务学习策略。
- Result: 在V2XPnP-Seq数据集上验证，TurboTrain提升了多智能体感知和预测模型的性能。
- Conclusion: 预训练能有效捕捉时空特征，平衡多任务学习策略提升了检测和预测性能。


### [131] [BEVCon: Advancing Bird's Eye View Perception with Contrastive Learning](https://arxiv.org/abs/2508.04702)
*Ziyang Leng,Jiawei Yang,Zhicheng Ren,Bolei Zhou*

Main category: cs.CV

TL;DR: BEVCon是一个简单有效的对比学习框架，旨在提升自动驾驶中的鸟瞰图（BEV）感知能力。通过两个对比学习模块，BEVCon显著提升了BEV编码器和图像主干网络的性能，实验结果显示其性能优于现有基线方法。

- Motivation: BEV感知在自动驾驶中至关重要，但现有研究主要关注BEV编码器和任务特定头的优化，而忽略了表示学习的潜力。BEVCon旨在填补这一空白。
- Method: BEVCon引入了两个对比学习模块：实例特征对比模块用于优化BEV特征，视角对比模块用于增强图像主干网络。这些模块结合检测损失进行密集对比学习。
- Result: 在nuScenes数据集上的实验表明，BEVCon实现了最高2.4%的mAP提升，优于现有基线方法。
- Conclusion: BEVCon证明了表示学习在BEV感知中的重要性，为传统任务特定优化提供了补充途径。


### [132] [Occupancy Learning with Spatiotemporal Memory](https://arxiv.org/abs/2508.04705)
*Ziyang Leng,Jiawei Yang,Wenlong Yi,Bolei Zhou*

Main category: cs.CV

TL;DR: ST-Occ是一个用于3D占用预测的时空一致性学习框架，通过历史信息的高效存储和不确定性建模，显著提升了性能。

- Motivation: 解决多帧输入下3D占用预测的高处理成本和时空不一致性问题。
- Method: 提出ST-Occ框架，包含时空记忆模块和注意力机制，用于高效存储历史信息并建模不确定性。
- Result: 在3D占用预测任务中，性能提升3 mIoU，时空不一致性降低29%。
- Conclusion: ST-Occ通过时空特征学习，显著提升了3D占用预测的准确性和一致性。
## physics.med-ph

### [133] [Fast Magnetic Resonance Simulation Using Combined Update with Grouped Isochromats](https://arxiv.org/abs/2508.03960)
*Hidenori Takeshima*

Main category: physics.med-ph

TL;DR: 提出一种基于分组等色体的MR模拟方法，显著减少计算时间。

- Motivation: 克服传统MR模拟器需单独模拟每个等色体的假设，提高计算效率。
- Method: 将多个等色体分组，共享模拟过程中的部分计算，适用于特定梯度类型。
- Result: 新方法比传统方法快3至72倍，例如FSE和EPI序列分别从208.4秒和66.4秒降至38.1秒和7.1秒。
- Conclusion: 分组等色体方法有效提升MR模拟速度，适用于大规模计算场景。
## cs.AI

### [134] [OS Agents: A Survey on MLLM-based Agents for General Computing Devices Use](https://arxiv.org/abs/2508.04482)
*Xueyu Hu,Tao Xiong,Biao Yi,Zishu Wei,Ruixuan Xiao,Yurun Chen,Jiasheng Ye,Meiling Tao,Xiangxin Zhou,Ziyu Zhao,Yuhuai Li,Shengze Xu,Shenzhi Wang,Xinchen Xu,Shuofei Qiao,Zhaokai Wang,Kun Kuang,Tieyong Zeng,Liang Wang,Jiwei Li,Yuchen Eleanor Jiang,Wangchunshu Zhou,Guoyin Wang,Keting Yin,Zhou Zhao,Hongxia Yang,Fan Wu,Shengyu Zhang,Fei Wu*

Main category: cs.AI

TL;DR: 本文综述了基于多模态大语言模型的操作系统代理（OS Agents），探讨了其关键组件、构建方法、评估协议及未来研究方向。

- Motivation: 实现像《钢铁侠》中J.A.R.V.I.S.一样全能且多功能的AI助手，是研究的核心动力。
- Method: 通过分析OS Agents的环境、观察空间、动作空间等关键组件，以及理解、规划和落地等能力，结合领域特定基础模型和代理框架构建方法。
- Result: 综述了OS Agents的研究现状，提出了评估协议和基准测试，并指出了当前挑战。
- Conclusion: OS Agents研究前景广阔，未来需关注安全、隐私、个性化及自我进化等方向。


### [135] [SEAgent: Self-Evolving Computer Use Agent with Autonomous Learning from Experience](https://arxiv.org/abs/2508.04700)
*Zeyi Sun,Ziyu Liu,Yuhang Zang,Yuhang Cao,Xiaoyi Dong,Tong Wu,Dahua Lin,Jiaqi Wang*

Main category: cs.AI

TL;DR: SEAgent是一个自进化框架，通过自主学习和任务生成，提升计算机使用代理在陌生软件环境中的适应能力，显著优于现有方法。

- Motivation: 现有大型视觉语言模型在缺乏人工标注的新颖和专用软件中表现不佳，需要一种自主进化的解决方案。
- Method: SEAgent通过体验学习、世界状态模型和课程生成器，结合对抗模仿和GRPO优化策略，实现代理的自主进化。
- Result: 在OS-World的五个新软件环境中，SEAgent的成功率从11.3%提升至34.5%，显著优于UI-TARS。
- Conclusion: SEAgent通过自主进化和专家到通才的训练策略，显著提升了计算机使用代理的性能和适应性。
## cs.GR

### [136] [RLGS: Reinforcement Learning-Based Adaptive Hyperparameter Tuning for Gaussian Splatting](https://arxiv.org/abs/2508.04078)
*Zhan Li,Huangying Zhan,Changyang Li,Qingan Yan,Yi Xu*

Main category: cs.GR

TL;DR: RLGS是一个基于强化学习的框架，用于自适应调整3D高斯溅射（3DGS）中的超参数，提升渲染质量。

- Motivation: 3DGS中的超参数调优过程繁琐且依赖专家经验，导致结果不一致且不理想。
- Method: RLGS通过轻量级策略模块动态调整学习率和密集化阈值等关键超参数，无需修改现有3DGS架构。
- Result: RLGS在多种3DGS变体（如Taming-3DGS和3DGS-MCMC）上表现优异，例如在TNT数据集上提升Taming-3DGS的PSNR 0.7dB。
- Conclusion: RLGS为3DGS训练中的超参数调优提供了高效通用的自动化解决方案。


### [137] [Surf3R: Rapid Surface Reconstruction from Sparse RGB Views in Seconds](https://arxiv.org/abs/2508.04508)
*Haodong Zhu,Changbai Li,Yangyang Ren,Zichao Feng,Xuhui Liu,Hanlin Chen,Xiantong Zhen,Baochang Zhang*

Main category: cs.GR

TL;DR: Surf3R是一种无需相机标定的端到端多视图3D重建方法，通过多分支解码架构和交叉视图注意力实现高效重建，并在10秒内完成场景重建。

- Motivation: 现有方法依赖复杂的相机标定和姿态估计，限制了实际应用。
- Method: 采用多分支多视图解码架构，结合分支处理、交叉视图注意力和分支间融合，无需相机标定；引入D-Normal正则化器优化3D几何。
- Result: 在ScanNet++和Replica数据集上达到最先进性能，重建速度快且泛化能力强。
- Conclusion: Surf3R通过创新架构和正则化方法，实现了高效、准确的无标定3D重建。


### [138] [MienCap: Realtime Performance-Based Facial Animation with Live Mood Dynamics](https://arxiv.org/abs/2508.04687)
*Ye Pan,Ruisi Zhang,Jingying Wang,Nengfu Chen,Yilin Qiu,Yu Ding,Kenny Mitchell*

Main category: cs.GR

TL;DR: 结合传统混合形状动画技术与机器学习模型，提出非实时和实时系统，提升3D风格化角色的表情动画效果。

- Motivation: 改进基于性能的动画，使3D风格化角色的表情更具真实感和感知有效性。
- Method: 非实时系统使用3D情感转移网络从2D图像生成3D参数；实时系统通过混合形状适应网络生成几何一致且时间稳定的参数。
- Result: 与Faceware相比，系统的表情识别度、强度和吸引力评分显著更高。
- Conclusion: 该系统可集成到动画流程中，帮助动画师更快速准确地创建所需表情。
## cs.NE

### [139] [TDSNNs: Competitive Topographic Deep Spiking Neural Networks for Visual Cortex Modeling](https://arxiv.org/abs/2508.04270)
*Deming Zhou,Yuetong Fang,Zhaorui Wang,Renjing Xu*

Main category: cs.NE

TL;DR: 该论文提出了一种新的时空约束（STC）损失函数，用于地形深度脉冲神经网络（TDSNNs），成功模拟了灵长类视觉皮层的空间功能组织，同时保持了高性能和生物逼真度。

- Motivation: 传统深度人工神经网络（ANNs）在模拟视觉皮层地形组织时忽略了时间动态，导致性能下降和生物逼真度不足。为了解决这一问题，作者利用脉冲神经网络（SNNs）的时序特性，提出了一种更接近生物神经网络的模型。
- Method: 作者设计了一种名为STC的损失函数，用于训练TDSNNs，以模拟视觉皮层的地形组织。该方法结合了空间和时间动态，实现了从低级感官输入到高级抽象表征的层次化组织。
- Result: 实验表明，TDSNNs在模拟视觉皮层地形组织时性能下降极小（ImageNet top-1准确率无下降），且优于现有地形ANNs。此外，地形组织通过脉冲机制促进了高效稳定的时序信息处理，增强了模型鲁棒性。
- Conclusion: TDSNNs在计算性能和生物逼真度之间取得了平衡，为神经科学现象的解释和高效深度学习模型的设计提供了新思路。
## cs.MM

### [140] [Think Before You Segment: An Object-aware Reasoning Agent for Referring Audio-Visual Segmentation](https://arxiv.org/abs/2508.04418)
*Jinxing Zhou,Yanghao Zhou,Mingfei Han,Tong Wang,Xiaojun Chang,Hisham Cholakkal,Rao Muhammad Anwer*

Main category: cs.MM

TL;DR: TGS-Agent通过Think-Ground-Segment流程解决Ref-AVS任务，结合多模态分析和显式推理，无需像素级监督，性能领先。

- Motivation: 现有方法依赖隐式多模态融合和像素级监督，缺乏可解释性。
- Method: 提出TGS-Agent，分解任务为Think-Ground-Segment三步，利用Ref-Thinker进行推理，Grounding-DINO和SAM2完成分割。
- Result: 在Ref-AVSBench和R²-AVSBench上达到SOTA。
- Conclusion: 显式推理方法优于隐式融合，TGS-Agent在性能和可解释性上表现优异。
## cs.CR

### [141] [PLA: Prompt Learning Attack against Text-to-Image Generative Models](https://arxiv.org/abs/2508.03696)
*Xinqi Lyu,Yihao Liu,Yanjie Li,Bin Xiao*

Main category: cs.CR

TL;DR: 本文提出了一种新的对抗攻击框架PLA，用于在黑盒设置下绕过T2I模型的安全机制，通过多模态相似性设计梯度训练方法，实验表明其攻击成功率优于现有方法。

- Motivation: T2I模型可能被滥用于生成NSFW内容，现有对抗攻击方法因搜索空间有限而效果不佳，黑盒设置下梯度训练面临挑战。
- Method: 提出PLA框架，利用多模态相似性设计黑盒T2I模型的梯度训练方法，学习对抗提示。
- Result: PLA能高效攻击黑盒T2I模型的安全机制（如提示过滤器和后验安全检查器），成功率优于现有方法。
- Conclusion: PLA为黑盒T2I模型的对抗攻击提供了有效解决方案，但需警惕其潜在滥用风险。
## cs.HC

### [142] [Tell Me Without Telling Me: Two-Way Prediction of Visualization Literacy and Visual Attention](https://arxiv.org/abs/2508.03713)
*Minsuk Chang,Yao Wang,Huichen Will Wang,Yuanhong Zhou,Andreas Bulling,Cindy Xiong Bearfield*

Main category: cs.HC

TL;DR: 研究提出Lit2Sal和Sal2Lit模型，通过视觉注意力模式预测视觉素养水平，提升可视化设计的个性化效果。

- Motivation: 现有研究忽视了视觉素养水平对视觉注意力行为的影响，导致可视化设计效果不佳。
- Method: 基于235名参与者的用户研究数据，提出Lit2Sal（预测注意力）和Sal2Lit（预测素养）两个计算模型。
- Result: Lit2Sal优于现有显著性模型，Sal2Lit预测素养准确率达86%。
- Conclusion: 考虑个体差异的显著性模型和视觉注意力分析为个性化可视化设计开辟了新方向。


### [143] [DRIVE-T: A Methodology for Discriminative and Representative Data Viz Item Selection for Literacy Construct and Assessment](https://arxiv.org/abs/2508.04160)
*Angela Locoro,Silvia Golia,Davide Falessi*

Main category: cs.HC

TL;DR: DRIVE-T方法通过任务项区分性和代表性，构建数据可视化素养评估工具，分为标注、评分和分析三步，验证其有效性。

- Motivation: 解决数据可视化素养评估中难度级别不明确的问题，提升测试设计的表达性和可重用性。
- Method: DRIVE-T方法包括标注任务项、独立评分、多面Rasch模型分析三步，量化任务项的难度级别。
- Result: 通过DRIVE-T方法，成功构建了数据可视化素养的测量模型，并通过试点研究验证了其有效性。
- Conclusion: DRIVE-T为数据可视化素养评估提供了一种可操作的方法，支持难度级别的量化与验证。
## eess.IV

### [144] [Technical specification of a framework for the collection of clinical images and data](https://arxiv.org/abs/2508.03723)
*Alistair Mackenzie,Mark Halling-Brown,Ruben van Engen,Carlijn Roozemond,Lucy Warren,Dominic Ward,Nadia Smith*

Main category: eess.IV

TL;DR: 报告描述了一个用于收集临床图像和数据的框架，以训练和验证AI工具，涵盖数据收集、伦理、信息治理及共享基础设施。

- Motivation: 确保AI工具训练和验证数据的时效性和代表性，同时兼顾伦理和治理要求。
- Method: 提出自动化持续收集数据的框架，并介绍半自动化方法作为替代方案。
- Result: 框架支持混合新旧数据，确保AI验证结果反映当前实践。
- Conclusion: 自动化框架适合大规模长期数据收集，半自动化方法可作为过渡方案。


### [145] [A Survey of Multimodal Ophthalmic Diagnostics: From Task-Specific Approaches to Foundational Models](https://arxiv.org/abs/2508.03734)
*Xiaoling Luo,Ruli Zheng,Qiaojian Zheng,Zibo Du,Shuo Yang,Meidan Ding,Qihao Xu,Chengliang Liu,Linlin Shen*

Main category: eess.IV

TL;DR: 本文综述了截至2025年眼科多模态深度学习方法的最新进展，包括任务特异性方法和基础模型，并探讨了数据集、评估指标及未来方向。

- Motivation: 视觉障碍是全球重大健康挑战，多模态成像为眼科诊断提供了互补信息，深度学习方法的进步有望提升诊断准确性。
- Method: 分为任务特异性方法和基础模型两类，前者针对特定临床任务，后者结合视觉-语言架构和大语言模型。
- Result: 多模态方法在病灶检测、疾病诊断和图像合成等方面表现优异，但仍面临数据变异性、标注不足等挑战。
- Conclusion: 未来方向包括超广域成像和强化学习框架，以开发智能、可解释且临床适用的眼科AI系统。


### [146] [Improve Retinal Artery/Vein Classification via Channel Couplin](https://arxiv.org/abs/2508.03738)
*Shuang Zeng,Chee Hong Lee,Kaiwen Li,Boxu Xie,Ourui Fu,Hangzhou He,Lei Zhu,Yanye Lu,Fangxiao Cheng*

Main category: eess.IV

TL;DR: 提出了一种新的损失函数和正则化方法，用于视网膜血管分割和动静脉分类，解决了现有方法忽略血管、动脉和静脉之间耦合关系的问题。

- Motivation: 手动分割和分类视网膜血管耗时且不一致，现有自动化方法将动脉、静脉和整体血管分割视为独立任务，忽略了它们的耦合关系。
- Method: 设计了Channel-Coupled Vessel Consistency Loss以增强血管、动脉和静脉预测的一致性，并引入intra-image pixel-level contrastive loss提取更精细的特征表示。
- Result: 在RITE、LES-AV和HRF三个公开数据集上取得了最先进的结果。
- Conclusion: 新方法通过耦合关系和特征优化显著提升了视网膜动静脉分类的准确性。


### [147] [A Modified VGG19-Based Framework for Accurate and Interpretable Real-Time Bone Fracture Detection](https://arxiv.org/abs/2508.03739)
*Md. Ehsanul Haque,Abrar Fahim,Shamik Dey,Syoda Anamika Jahan,S. M. Jahidul Islam,Sakib Rokoni,Md Sakib Morshed*

Main category: eess.IV

TL;DR: 提出了一种基于改进VGG-19模型的自动化骨折检测框架，结合CLAHE等预处理技术和Grad-CAM解释性AI方法，实现高精度和可解释性。

- Motivation: X光图像解读耗时且易出错，现有深度学习模型缺乏可解释性，需提高骨折检测的准确性和临床信任度。
- Method: 使用改进的VGG-19模型，结合CLAHE、Otsu阈值化和Canny边缘检测等预处理技术，并采用Grad-CAM提供可视化解释。
- Result: 模型分类准确率达99.78%，AUC为1.00，实时诊断反馈仅需0.5秒。
- Conclusion: 该框架为骨折检测提供了快速、可靠且可解释的解决方案，有助于提高诊断效率和患者护理质量。


### [148] [Boosting Vision Semantic Density with Anatomy Normality Modeling for Medical Vision-language Pre-training](https://arxiv.org/abs/2508.03742)
*Weiwei Cao,Jianpeng Zhang,Zhongyi Shui,Sinuo Wang,Zeli Chen,Xi Li,Le Lu,Xianghua Ye,Tingbo Liang,Qi Zhang,Ling Zhang*

Main category: eess.IV

TL;DR: 该论文提出了一种提升视觉语义密度的方法，以解决医学图像与报告之间的语义密度差距问题，通过疾病级视觉对比学习和解剖学正态建模，显著提升了诊断性能。

- Motivation: 医学图像与报告之间存在语义密度差距，导致视觉对齐偏差，影响了多功能和通用医学诊断能力的发展。
- Method: 采用疾病级视觉对比学习增强视觉语义，并通过VQ-VAE建模解剖学正态分布，以放大异常信号。
- Result: 在两个胸部CT数据集和一个腹部CT数据集上实现了最先进的零样本性能，平均AUC达到84.9%。
- Conclusion: 该方法有效提升了视觉语义密度和诊断性能，并展示了卓越的迁移学习能力。


### [149] [Do We Need Pre-Processing for Deep Learning Based Ultrasound Shear Wave Elastography?](https://arxiv.org/abs/2508.03744)
*Sarah Grube,Sören Grünhagen,Sarah Latus,Michael Meyling,Alexander Schlaefer*

Main category: eess.IV

TL;DR: 研究探讨了深度学习在超声剪切波弹性成像中是否需要预处理步骤，发现即使使用原始射频数据，深度学习也能可靠区分不同弹性组。

- Motivation: 超声剪切波弹性成像的非侵入性诊断潜力受限于预处理步骤的通用性和标准化问题，研究旨在减少对传统预处理的依赖。
- Method: 使用3D卷积神经网络从时空超声图像预测剪切波速度，比较不同预处理程度（从完全波束形成到原始射频数据）的影响。
- Result: 深度学习方法在所有弹性组中均能显著区分剪切波速度，预处理仅略微提升性能。
- Conclusion: 深度学习可减少对传统预处理的依赖，提供更快、更可靠的临床弹性评估。


### [150] [M$^3$HL: Mutual Mask Mix with High-Low Level Feature Consistency for Semi-Supervised Medical Image Segmentation](https://arxiv.org/abs/2508.03752)
*Yajun Liu,Zenghui Zhang,Jiang Yue,Weiwei Guo,Dongying Li*

Main category: eess.IV

TL;DR: 提出了一种名为M$^3$HL的新方法，结合动态可调掩码和高低层特征一致性，提升了半监督医学图像分割的性能。

- Motivation: 现有CutMix方法在医学图像分割中表现优异，但存在操作僵化和特征一致性不足的问题。
- Method: M$^3$HL包含两部分：动态可调掩码的数据增强（M$^3$）和分层特征一致性框架（HL）。
- Result: 在ACDC和LA数据集上取得了最先进的性能。
- Conclusion: M$^3$HL通过灵活的数据增强和特征一致性约束，显著提升了半监督医学图像分割的效果。


### [151] [Classification non supervis{é}es d'acquisitions hyperspectrales cod{é}es : quelles v{é}rit{é}s terrain ?](https://arxiv.org/abs/2508.03753)
*Trung-tin Dinh,Hervé Carfantan,Antoine Monmayrant,Simon Lacroix*

Main category: eess.IV

TL;DR: 提出了一种基于DD-CASSI高光谱成像仪的无监督分类方法，通过简单的类内光谱变异性模型，实现数据压缩十倍下的类别识别和参考光谱估计。

- Motivation: 解决现有评估方法中存在的问题，如类别定义模糊、类内变异性高和分类错误，强调需要重新思考无监督分类方法的评估标准。
- Method: 利用DD-CASSI高光谱成像仪的有限编码采集数据，基于类内光谱变异性模型进行无监督分类。
- Result: 在Pavia University场景中，通过简单假设成功检测出光谱更一致的区域，验证了方法的有效性。
- Conclusion: 现有评估方法存在不足，需重新设计无监督分类方法的评估标准，尤其是在高数据压缩比下。


### [152] [FUTransUNet-GradCAM: A Hybrid Transformer-U-Net with Self-Attention and Explainable Visualizations for Foot Ulcer Segmentation](https://arxiv.org/abs/2508.03758)
*Akwasi Asare,Mary Sagoe,Justice Williams Asare*

Main category: eess.IV

TL;DR: 提出了一种名为FUTransUNet的混合架构，结合了Vision Transformers的全局注意力机制和U-Net框架，用于糖尿病足溃疡（DFU）的自动分割，显著提升了分割性能。

- Motivation: 糖尿病足溃疡（DFU）的自动分割在临床诊断和治疗规划中至关重要，但由于溃疡区域的异质性外观、不规则形态和复杂背景，传统方法（如U-Net）难以建模长距离空间依赖关系。
- Method: 提出FUTransUNet，将Vision Transformers的全局注意力机制与U-Net结合，通过跳跃连接和解码路径保持细粒度空间分辨率。
- Result: 在FUSeg数据集上，训练集Dice系数为0.8679，IoU为0.7672；验证集Dice系数为0.8751，IoU为0.7780。Grad-CAM可视化验证了模型的透明性。
- Conclusion: FUTransUNet成功整合了全局和局部特征提取，提供了高鲁棒性、准确性和可解释性的解决方案，有望改善临床伤口评估和患者护理。


### [153] [Assessing the Impact of Image Super Resolution on White Blood Cell Classification Accuracy](https://arxiv.org/abs/2508.03759)
*Tatwadarshi P. Nagarhalli,Shruti S. Pawar,Soham A. Dahanukar,Uday Aswalekar,Ashwini M. Save,Sanket D. Patil*

Main category: eess.IV

TL;DR: 研究探讨了通过图像超分辨率技术提升白细胞显微图像分辨率，以提高深度学习分类模型的性能。

- Motivation: 低分辨率的显微图像可能影响白细胞的准确分类，因此需要探索图像增强技术对分类性能的影响。
- Method: 结合图像超分辨率技术提升图像分辨率，并将增强后的图像纳入训练过程，使用深度学习模型进行分类。
- Result: 通过实验评估，发现图像分辨率提升有助于模型捕捉更细微的形态变化，从而提高分类准确性。
- Conclusion: 图像增强技术能显著提升白细胞分类模型的性能，为特定数据集定制更高效的图像识别算法提供了方向。


### [154] [Scaling Artificial Intelligence for Prostate Cancer Detection on MRI towards Population-Based Screening and Primary Diagnosis in a Global, Multiethnic Population (Study Protocol)](https://arxiv.org/abs/2508.03762)
*Anindo Saha,Joeran S. Bosma,Jasper J. Twilt,Alexander B. C. D. Ng,Aqua Asif,Kirti Magudia,Peder Larson,Qinglin Xie,Xiaodong Zhang,Chi Pham Minh,Samuel N. Gitau,Ivo G. Schoots,Martijn F. Boomsma,Renato Cuocolo,Nikolaos Papanikolaou,Daniele Regge,Derya Yakar,Mattijs Elschot,Jeroen Veltman,Baris Turkbey,Nancy A. Obuchowski,Jurgen J. Fütterer,Anwar R. Padhani,Hashim U. Ahmed,Tobias Nordström,Martin Eklund,Veeru Kasivisvanathan,Maarten de Rooij,Henkjan Huisman*

Main category: eess.IV

TL;DR: 该研究通过多中心、多国数据训练和验证了PI-CAI-2B模型，用于检测Gleason分级≥2的前列腺癌，并验证其与标准诊断的一致性。

- Motivation: 开发新一代高效AI系统，用于前列腺癌的MRI检测，以提升诊断准确性和效率。
- Method: 使用22,481例MRI检查数据（来自22国的46个城市）训练和验证PI-CAI-2B模型，分为内部和外部测试集。
- Result: 主要终点是AI评估与标准诊断的一致性，假设在PI-RADS≥3或≥4阈值下可互换。
- Conclusion: 研究旨在验证AI系统在前列腺癌诊断中的可靠性，并探索潜在的诊断偏见。


### [155] [When Deep Learning Fails: Limitations of Recurrent Models on Stroke-Based Handwriting for Alzheimer's Disease Detection](https://arxiv.org/abs/2508.03773)
*Emanuele Nardone,Tiziana D'Alessandro,Francesco Fontanella,Claudio De Stefano*

Main category: eess.IV

TL;DR: 该研究探讨了深度学习是否可以通过笔迹分析实现非侵入性阿尔茨海默病检测，发现传统集成方法在性能上优于循环神经网络架构。

- Motivation: 阿尔茨海默病检测通常依赖昂贵或侵入性方法，限制了可及性，研究旨在探索非侵入性替代方案。
- Method: 使用34种笔迹任务数据集，比较三种循环神经网络（LSTM、GRU、RNN）与传统机器学习模型，循环网络处理离散笔画特征而非原始时序信号。
- Result: 循环网络表现不佳，特异性低且方差高；传统集成方法显著优于深度学习模型，准确性更高且指标平衡。
- Conclusion: 循环网络架构不适合离散笔画特征数据，研究揭示了数据表示与模型兼容性的关键问题，为未来研究指明方向。


### [156] [UNISELF: A Unified Network with Instance Normalization and Self-Ensembled Lesion Fusion for Multiple Sclerosis Lesion Segmentation](https://arxiv.org/abs/2508.03982)
*Jinwei Zhang,Lianrui Zuo,Blake E. Dewey,Samuel W. Remedios,Yihao Liu,Savannah P. Hays,Dzung L. Pham,Ellen M. Mowry,Scott D. Newsome,Peter A. Calabresi,Aaron Carass,Jerry L. Prince*

Main category: eess.IV

TL;DR: UNISELF是一种深度学习方法，用于多发性硬化（MS）病灶的自动分割，通过测试时自集成病灶融合和实例归一化技术，优化了单域训练的准确性和跨域泛化能力。

- Motivation: 现有的深度学习方法在单源有限数据训练下难以同时优化域内准确性和跨域泛化性，UNISELF旨在填补这一空白。
- Method: UNISELF采用测试时自集成病灶融合提升分割准确性，并利用测试时实例归一化（TTIN）处理域偏移和缺失输入对比度。
- Result: 在ISBI 2015测试数据集上表现优异，并在多个跨域测试数据集（如MICCAI 2016和UMCL）上超越基准方法。
- Conclusion: UNISELF在单域训练和跨域泛化方面表现出色，适用于多源数据的分割任务。


### [157] [PET2Rep: Towards Vision-Language Model-Drived Automated Radiology Report Generation for Positron Emission Tomography](https://arxiv.org/abs/2508.04062)
*Yichi Zhang,Wenbo Zhang,Zehui Ling,Gang Feng,Sisi Peng,Deshu Chen,Yuchen Liu,Hongwei Zhang,Shuqi Wang,Lanlan Li,Limei Han,Yuan Cheng,Zixin Hu,Yuan Qi,Le Xue*

Main category: eess.IV

TL;DR: PET2Rep是首个专注于PET图像报告生成的基准数据集，填补了现有基准的空白，并评估了30种前沿视觉语言模型在PET报告生成中的表现。

- Motivation: PET成像在临床决策中至关重要，但手动生成报告耗时且费力。现有视觉语言模型主要关注结构成像，忽视了PET成像的独特代谢特性。
- Method: 引入PET2Rep数据集，包含全身图像-报告对，并设计临床效率指标评估报告质量。比较30种前沿模型的性能。
- Result: 当前最先进的视觉语言模型在PET报告生成任务中表现不佳，未满足实际需求。
- Conclusion: 需解决关键不足以推动视觉语言模型在医学应用中的发展。


### [158] [Unmasking Interstitial Lung Diseases: Leveraging Masked Autoencoders for Diagnosis](https://arxiv.org/abs/2508.04429)
*Ethan Dack,Lorenzo Brigato,Vasilis Dedousis,Janine Gote-Schniering,Cheryl,Hanno Hoppe,Aristomenis Exadaktylos,Manuela Funke-Chambour,Thomas Geiser,Andreas Christe,Lukas Ebner,Stavroula Mougiakakou*

Main category: eess.IV

TL;DR: MAEs用于无标签数据预训练，在稀缺标注的扩散性肺病研究中表现优异，通过5000多张CT扫描训练，显著提升诊断性能。

- Motivation: 扩散性肺病研究中标注数据稀缺，MAEs能有效利用无标签数据学习特征。
- Method: 使用5000多张CT扫描（包括COVID-19和细菌性肺炎）预训练MAE，并在下游分类任务中微调。
- Result: MAEs能提取临床相关特征，提升诊断性能，即使标注数据有限。
- Conclusion: MAEs在扩散性肺病诊断中具有潜力，尤其适用于标注数据不足的场景。


### [159] [TotalRegistrator: Towards a Lightweight Foundation Model for CT Image Registration](https://arxiv.org/abs/2508.04450)
*Xuan Loc Pham,Gwendolyn Vuurberg,Marjan Doppen,Joey Roosen,Tip Stille,Thi Quynh Ha,Thuy Duong Quach,Quoc Vu Dang,Manh Ha Luu,Ewoud J. Smit,Hong Son Mai,Mattias Heinrich,Bram van Ginneken,Mathias Prokop,Alessa Hering*

Main category: eess.IV

TL;DR: TotalRegistrator是一个基于UNet架构和新型场分解策略的轻量级图像配准框架，能够同时对齐多个解剖区域，在多项实验中表现优于基线方法。

- Motivation: 现有图像配准方法多针对单一器官，限制了其通用性，因此需要一种能同时处理多解剖区域的框架。
- Method: 采用标准UNet架构和新型场分解策略，构建轻量级模型（仅需11GB GPU内存），并在大规模纵向CT数据集上训练和评估。
- Result: 在内部数据集上，多器官腹部配准性能优于基线方法，肺部配准略有下降；在外部数据集上表现竞争力强，展示了良好的通用性。
- Conclusion: TotalRegistrator在多器官图像配准中表现出色，具有轻量化和强通用性的特点，代码将开源。


### [160] [OpenDCVCs: A PyTorch Open Source Implementation and Performance Evaluation of the DCVC series Video Codecs](https://arxiv.org/abs/2508.04491)
*Yichi Zhang,Fengqing Zhu*

Main category: eess.IV

TL;DR: OpenDCVCs是一个开源的PyTorch实现，旨在推动学习型视频压缩的可重复研究，提供四种DCVC模型的统一实现。

- Motivation: 解决现有公开代码仅限于评估代码的问题，促进可重复性、基准测试和进一步开发。
- Method: 提供包含四种DCVC模型的统一框架，支持端到端训练和评估，附带详细文档和基准测试结果。
- Result: OpenDCVCs为社区提供了透明、一致的比较和扩展基础，加速研究与合作。
- Conclusion: OpenDCVCs填补了现有公开代码的不足，为学习型视频压缩研究提供了全面支持。


### [161] [Conditional Fetal Brain Atlas Learning for Automatic Tissue Segmentation](https://arxiv.org/abs/2508.04522)
*Johannes Tischer,Patric Kienast,Marlene Stümpflen,Gregor Kasprian,Georg Langs,Roxane Licandro*

Main category: eess.IV

TL;DR: 提出了一种基于深度学习的框架，用于生成连续的、年龄特定的胎儿脑图谱，支持实时脑组织分割。

- Motivation: 胎儿脑MRI评估因脑成熟度、成像协议和孕龄估计的变异性而具有挑战性，需要标准化参考框架。
- Method: 结合直接配准模型和条件判别器，使用219例21至37周孕龄的胎儿MRI数据进行训练。
- Result: 高配准精度，动态解剖变化捕捉清晰，平均DSC为86.3%，揭示了详细的神经典型生长轨迹。
- Conclusion: 该方法支持个体化发育评估，具有实时性能，适用于研究和临床。


### [162] [LA-CaRe-CNN: Cascading Refinement CNN for Left Atrial Scar Segmentation](https://arxiv.org/abs/2508.04553)
*Franz Thaler,Darko Stern,Gernot Plank,Martin Urschler*

Main category: eess.IV

TL;DR: 提出了一种名为LA-CaRe-CNN的两阶段3D CNN模型，用于从LGE MR扫描中准确分割左心房和左心房瘢痕组织，以支持个性化消融治疗。

- Motivation: 心房颤动（AF）治疗需要消融手术，而患者特异性心脏数字孪生模型需要准确的瘢痕组织分割。现有方法需要改进。
- Method: LA-CaRe-CNN是一个两阶段级联CNN，第一阶段分割左心房，第二阶段结合原始图像信息细化瘢痕组织分割。采用强数据增强应对域偏移。
- Result: 模型在5折集成下表现优异：左心房分割DSC为89.21%，瘢痕组织DSC为64.59%。
- Conclusion: LA-CaRe-CNN的分割结果有望用于生成心脏数字孪生模型，支持个性化消融治疗。
## cs.NI

### [163] [CONVERGE: A Multi-Agent Vision-Radio Architecture for xApps](https://arxiv.org/abs/2508.04556)
*Filipe B. Teixeira,Carolina Simões,Paulo Fidalgo,Wagner Pedrosa,André Coelho,Manuel Ricardo,Luis M. Pessoa*

Main category: cs.NI

TL;DR: 论文提出了一种结合电信与计算机视觉的新架构，通过多智能体方法将实时无线电和视频感知信息传递给O-RAN xApps，并展示了实验验证的实时性能。

- Motivation: 高频无线链路主要在视距内运行，视觉数据可帮助预测信道动态并克服障碍，从而提升通信性能。
- Method: 采用多智能体方法，设计新视频功能生成阻塞信息，支持集成感知与通信。
- Result: 实验显示感知信息延迟低于1毫秒，xApp能实时控制5G/6G RAN。
- Conclusion: 该架构成功实现了实时无线电与视频感知的集成，为5G/6G网络提供了新解决方案。
## cs.LG

### [164] [CX-Mind: A Pioneering Multimodal Large Language Model for Interleaved Reasoning in Chest X-ray via Curriculum-Guided Reinforcement Learning](https://arxiv.org/abs/2508.03733)
*Wenjie Li,Yujie Zhang,Haoran Sun,Yueqi Li,Fanrui Zhang,Mengzhe Xu,Victoria Borja Clausich,Sade Mellin,Renhao Yang,Chenrun Wang,Jethro Zih-Shuo Wang,Shiyi Yao,Gen Li,Yidong Xu,Hanyu Wang,Yilin Huang,Angela Lin Wang,Chen Shi,Yin Zhang,Jianan Guo,Luqi Yang,Renxuan Li,Yang Xu,Jiawei Liu,Yao Zhang,Lei Liu,Carlos Gutiérrez SanRomán,Lei Wang*

Main category: cs.LG

TL;DR: CX-Mind是一种基于课程强化学习和可验证过程奖励的生成模型，用于CXR任务的多模态推理，显著提升了诊断效率和准确性。

- Motivation: 现有CXR诊断模型缺乏对推理过程的可验证监督，导致多任务诊断中的推理冗长、奖励稀疏和幻觉频繁。
- Method: 通过构建CX-Set数据集，采用课程强化学习和可验证过程奖励（CuRL-VPR）进行两阶段优化。
- Result: CX-Mind在视觉理解、文本生成和时空对齐方面显著优于现有模型，平均性能提升25.1%。
- Conclusion: CX-Mind在真实临床数据集上表现优异，多中心专家评估证实其临床实用性。


### [165] [GlaBoost: A multimodal Structured Framework for Glaucoma Risk Stratification](https://arxiv.org/abs/2508.03750)
*Cheng Huang,Weizheng Xie,Karanjit Kooner,Tsengdar Lee,Jui-Kai Wang,Jia Zhang*

Main category: cs.LG

TL;DR: GlaBoost是一种多模态梯度提升框架，结合临床特征、眼底图像和专家文本描述，用于青光眼风险预测，显著优于基线模型。

- Motivation: 现有青光眼检测方法依赖单模态数据且缺乏可解释性，限制了临床实用性。
- Method: GlaBoost整合结构化临床特征、眼底图像嵌入和专家文本描述，使用预训练卷积编码器和基于Transformer的语言模型提取特征，通过增强的XGBoost模型进行分类。
- Result: 在真实数据集上验证，GlaBoost准确率达98.71%，特征重要性分析显示临床一致性。
- Conclusion: GlaBoost为青光眼诊断提供了透明且可扩展的解决方案，并可扩展至其他眼科疾病。


### [166] [LRTuckerRep: Low-rank Tucker Representation Model for Multi-dimensional Data Completion](https://arxiv.org/abs/2508.03755)
*Wenwu Gong,Lili Yang*

Main category: cs.LG

TL;DR: 提出了一种新的低秩Tucker表示（LRTuckerRep）模型，结合全局和局部先验建模，通过自适应加权核范数和参数自由的拉普拉斯正则化，实现了高效的多维数据补全。

- Motivation: 多维数据补全在计算科学中至关重要，但现有方法（如低秩近似或局部平滑正则化）存在计算成本高或泛化能力差的问题。
- Method: LRTuckerRep模型通过Tucker分解统一全局和局部先验建模，使用自适应加权核范数和稀疏Tucker核心编码低秩性，并通过拉普拉斯正则化捕获平滑性。
- Result: 在图像修复和交通数据补全实验中，LRTuckerRep在高缺失率下表现出更高的补全准确性和鲁棒性。
- Conclusion: LRTuckerRep模型有效解决了现有方法的局限性，为多维数据补全提供了高效且鲁棒的解决方案。


### [167] [FLAT: Latent-Driven Arbitrary-Target Backdoor Attacks in Federated Learning](https://arxiv.org/abs/2508.04064)
*Tuan Nguyen,Khoa D Doan,Kok-Seng Wong*

Main category: cs.LG

TL;DR: FLAT是一种新型的后门攻击方法，利用条件自编码器生成多样化的目标特定触发器，提高了攻击的灵活性和隐蔽性。

- Motivation: 现有的联邦学习后门攻击方法通常使用固定模式或单一目标触发器，灵活性差且易被检测。FLAT旨在解决这些问题。
- Method: FLAT通过潜在驱动的条件自编码器生成多样化的触发器，支持任意目标选择且无需重新训练。
- Result: 实验表明，FLAT攻击成功率高，并能有效规避现有防御机制。
- Conclusion: FLAT的灵活性凸显了联邦学习中需要新的防御策略以应对潜在驱动的多目标后门威胁。


### [168] [Continual Multiple Instance Learning for Hematologic Disease Diagnosis](https://arxiv.org/abs/2508.04368)
*Zahra Ebrahimi,Raheleh Salehi,Nassir Navab,Carsten Marr,Ario Sadafi*

Main category: cs.LG

TL;DR: 提出了一种针对多实例学习（MIL）的持续学习方法，通过选择实例注意力分数和距离来存储样本，显著优于现有方法。

- Motivation: 实验室和诊所的动态环境需要持续更新机器学习模型，但现有持续学习方法在多实例学习中效果不佳。
- Method: 基于实例注意力分数和距离选择样本存储，保留数据多样性。
- Result: 在白血病实验室数据上，该方法显著优于现有持续学习方法。
- Conclusion: 该方法首次实现了MIL的持续学习，适应数据分布的变化。
## cs.RO

### [169] [RoboTron-Sim: Improving Real-World Driving via Simulated Hard-Case](https://arxiv.org/abs/2508.04642)
*Baihui Xiao,Chengjian Feng,Zhijian Huang,Feng yan,Yujie Zhong,Lin Ma*

Main category: cs.RO

TL;DR: RoboTron-Sim利用模拟数据提升自动驾驶系统在关键场景中的性能，通过HASS数据集和SPE、I2E Encoder技术，实现了50%的性能提升。

- Motivation: 解决真实数据难以覆盖高风险、长尾驾驶事件和复杂交互的问题，提升自动驾驶系统在关键场景的表现。
- Method: 开发HASS模拟数据集，引入SPE和I2E Encoder技术，利用多模态大语言模型学习驾驶技能。
- Result: 在nuScenes上实验表明，性能提升约50%，在真实世界开环规划中达到SOTA。
- Conclusion: RoboTron-Sim能有效管理高风险驾驶场景，提升自动驾驶系统性能。
## astro-ph.IM

### [170] [Super Resolved Imaging with Adaptive Optics](https://arxiv.org/abs/2508.04648)
*Robin Swanson,Esther Y. H. Lin,Masen Lamb,Suresh Sivanandam,Kiriakos N. Kutulakos*

Main category: astro-ph.IM

TL;DR: 提出一种利用自适应光学系统实现天文望远镜视场与分辨率兼得的计算成像方法。

- Motivation: 解决天文望远镜视场与分辨率之间的权衡问题。
- Method: 利用自适应光学系统的变形镜施加学习控制的波前畸变，生成高频子像素位移图像，联合上采样实现超分辨率。
- Result: 实验和模拟显示信噪比提升高达12 dB，且无需硬件改动。
- Conclusion: 该方法可直接应用于现有望远镜系统，显著提升成像性能。
## cs.CL

### [171] [ZARA: Zero-shot Motion Time-Series Analysis via Knowledge and Retrieval Driven LLM Agents](https://arxiv.org/abs/2508.04038)
*Zechen Li,Baiyu Chen,Hao Xue,Flora D. Salim*

Main category: cs.CL

TL;DR: ZARA是一个基于代理的零样本、可解释的人体活动识别框架，直接从原始运动时间序列中提取特征，无需微调或任务特定分类器，性能优于现有基线。

- Motivation: 现有方法需要针对固定活动集训练，且在新行为或传感器设置出现时需要昂贵重新训练；现有基于大语言模型的方法精度有限且缺乏可验证解释性。
- Method: ZARA整合了自动提取的成对特征知识库、多传感器检索模块和分层代理流程，指导大语言模型迭代选择特征并生成预测和解释。
- Result: 在8个HAR基准测试中，ZARA实现了零样本SOTA性能，宏观F1超过最强基线2.53倍。
- Conclusion: ZARA是迈向可信赖、即插即用运动时间序列分析的重要一步，其模块设计经消融研究验证为必要。


### [172] [Beyond the Leaderboard: Rethinking Medical Benchmarks for Large Language Models](https://arxiv.org/abs/2508.04325)
*Zizhan Ma,Wenxuan Wang,Guo Yu,Yiu-Fai Cheung,Meidan Ding,Jie Liu,Wenting Chen,Linlin Shen*

Main category: cs.CL

TL;DR: MedCheck是一个针对医疗基准测试的生命周期评估框架，旨在解决现有基准测试在临床保真度、数据管理和安全性方面的不足。

- Motivation: 现有的大型语言模型（LLM）医疗基准测试存在可靠性问题，缺乏临床实践关联性和安全性评估。
- Method: MedCheck将基准测试开发分为五个阶段，并提供46项医学定制标准，用于评估53个医疗LLM基准测试。
- Result: 分析揭示了普遍问题，如与临床实践脱节、数据完整性危机和安全性评估缺失。
- Conclusion: MedCheck可作为诊断工具和指南，推动医疗AI评估的标准化和可靠性。


### [173] [Chain of Questions: Guiding Multimodal Curiosity in Language Models](https://arxiv.org/abs/2508.04350)
*Nima Iji,Kia Dashtipour*

Main category: cs.CL

TL;DR: 论文提出了一种名为Chain of Questions（CoQ）的好奇心驱动推理框架，用于提升多模态语言模型在复杂环境中的推理能力。

- Motivation: 尽管大型语言模型（LLMs）在推理能力上取得了显著进步，但这些改进尚未完全适用于多模态环境，模型需要主动决定使用哪些感官模态（如视觉、听觉或空间感知）来与环境交互。
- Method: CoQ框架通过动态生成针对环境的问题，引导模型选择性地激活相关模态，从而收集关键信息以支持推理和响应生成。
- Result: 实验结果表明，CoQ方法提升了基础模型在多模态任务中识别和整合感官信息的能力，从而提高了准确性、可解释性和推理过程的适应性。
- Conclusion: CoQ框架为多模态语言模型提供了一种有效的推理方法，能够更好地适应复杂任务的需求。
## cs.IR

### [174] [Audio Does Matter: Importance-Aware Multi-Granularity Fusion for Video Moment Retrieval](https://arxiv.org/abs/2508.04273)
*Junan Lin,Daizong Liu,Xianke Chen,Xiaoye Qu,Xun Yang,Jixiang Zhu,Sanyuan Zhang,Jianfeng Dong*

Main category: cs.IR

TL;DR: 论文提出了一种重要性感知的多粒度融合模型（IMG），用于视频时刻检索（VMR），通过动态选择性地聚合音频-视觉-文本模态，解决了现有方法忽视音频模态或简单嵌入的问题。

- Motivation: 现有VMR方法主要关注视觉和文本模态，忽视了音频模态的重要性，且少数尝试联合模态的方法未能精细处理音频的干扰问题。
- Method: 设计了伪标签监督的音频重要性预测器和多粒度音频融合模块，动态加权音频并融合视觉模态，还提出跨模态知识蒸馏策略处理推理时音频缺失问题。
- Result: 在新建的Charades-AudioMatter数据集上验证了模型有效性，实现了音频-视频融合的VMR方法中的最先进性能。
- Conclusion: IMG模型通过动态选择和融合多模态信息，显著提升了VMR任务的性能，尤其在处理音频干扰和缺失时表现出色。
